{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Spark session\n",
    "\n",
    "The Spark session is the entry point to the Spark interpreter. We need it for running Spark programs.\n",
    "\n",
    "\n",
    "###### Para crear una sesion de Spark\n",
    "\n",
    "###### En intelIJ\n",
    "\n",
    "###### val spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "\n",
    "###### En build.sbt \n",
    "\n",
    "###### libraryDependencies += \"org.apache.spark\" %% \"spark-sql\" % \"3.3.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create a Spark session in standalone mode\n",
    "\n",
    "import $ivy.`org.apache.spark::spark-sql:2.4.5` \n",
    "import $ivy.`sh.almond::almond-spark:0.4.0`\n",
    "\n",
    "import org.apache.spark.sql.{NotebookSparkSession, SparkSession}\n",
    "\n",
    "val spark: SparkSession = \n",
    "    NotebookSparkSession\n",
    "      .builder()\n",
    "      .master(\"local[*]\")\n",
    "      .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging configuration\n",
    "\n",
    "This is convenient to minimize the amount of info displayed in the terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Para minimizar la cantidad de informacion del terminal, solo nos mostrara los errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.slf4j.LoggerFactory\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getRootLogger().setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits._\n",
    "import org.apache.spark.sql.{functions => func, _}\n",
    "import org.apache.spark.sql.types._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Para transformar en un dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List(1,2,3,4).toDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First difference: performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a heavy computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heavyComp(ms: Int = 1000)(x: Int): Int = {\n",
    "  Thread.sleep(ms)\n",
    "  x+1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a way to measure execution time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run[A](code: => A): A = {\n",
    "    val start = System.currentTimeMillis()\n",
    "    val res = code\n",
    "    println(s\"Took ${System.currentTimeMillis() - start}\")\n",
    "    res\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Scala Collection program takes some time to execute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run{\n",
    "    List(1,2,3,4).map(heavyComp(): Int => Int).reduce(_ + _)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the equivalent Dataset program takes half time (or less time depending on the number of cores of your processor)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(\n",
    "    List(1,2,3,4).toDS.map(heavyComp()).reduce(_ + _)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset program run faster because the Spark framework is designed to take advantage of the parallel and distributed architecture of your computing infrastructure. In our case, it simply exploits the number of cores of your processor.\n",
    "\n",
    "\n",
    "###### El programa Dataset se ejecuta más rápido porque el marco Spark está diseñado para aprovechar la arquitectura paralela y distribuida de su infraestructura informática. En nuestro caso, simplemente explota la cantidad de núcleos de su procesador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third difference: the execution framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When an action is applied on a `Dataset` program a _job_ is executed by the distributed platform of Spark through a sequence of *stages*; in each stage, the work to be done is performed concurrently by a number of _tasks_. \n",
    "\n",
    "The so-called [Spark UI](http://localhost:4040/) allows us to debug the execution process of all the jobs that are submitted for execution through a given Spark session. For instace, the following action launches a job that can be inspected through the Spark UI: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ds: Dataset[Int] = List(1,2,3,4).toDS.map(heavyComp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each bar in the notebook execution corresponds to one stage of the job exectuion; the X/Y label in each bar indicates the number of tasks already executed (X) and the total number of tasks of that stage (Y). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the work performed by tasks in each partition through `foreachPartition`:\n",
    "\n",
    "###### Para obtener el trabajo realizado en cada particion `foreachPartition`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.foreachPartition{ it : Iterator[Int] => \n",
    "    println(s\"Task output: \" + it.toList)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "implicit class DatasetOps[T](ds: Dataset[T]){\n",
    "    def collectPartitions: Unit = \n",
    "        ds.foreachPartition{it : Iterator[T] => println(it.toList)}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of tasks scheduled for each stage depends on the number of partitions associated to the dataset. When the dataset is first created from a Scala collection, the number of partitions defaults to the number of cores specified when the Spark context was created. \n",
    "\n",
    "###### Para ver el numero de particiones usamos `.getNumPartitions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List(1,2,3,4).toDS.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of partitions can be set to a specific value using `repartition`: \n",
    "\n",
    "###### Para establecer el numero de particiones `repartition`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List(1,2,3,4,5,6,7,8,9,10,11,12).toDS\n",
    "    .repartition(24)\n",
    "    .map(heavyComp(2000))\n",
    "    .collectPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffling: narrow vs. wide transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a new stage is created when the dataset is repartitioned. More commonly, new stages are created when so-called _wide_ transformations are interpreted. _Narrow_ transformations are those transformations which are not wide: `map`, `filter`, etc. For instance, this program will execute in one stage: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List((\"a\", 1), (\"b\", 2), (\"a\", 3), (\"d\", 3), (\"b\", 4)).toDS\n",
    "    .map{ case (key, value) => (key, value + 1) }\n",
    "    .collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the following one as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List((\"a\", 1), (\"b\", 2), (\"a\", 3), (\"d\", 3), (\"b\", 4)).toDS\n",
    "    .map{ case (key, value) => (key, value + 1) }\n",
    "    .filter{ t => t._1 == \"a\" }\n",
    "    .collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this one introduces a new stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List((\"a\", 1), (\"b\", 2), (\"a\", 3), (\"d\", 3), (\"b\", 4)).toDS\n",
    "    .map{ case (key, value) => (key, value + 1) }\n",
    "    .groupByKey(_._1)\n",
    "    .mapGroups((key, values) => (key, values.toList.map(_._2)))\n",
    "    .collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why? Which difference between `filter` and `groupBy` creates such a need for a new stage? And why the next stage generates a dataset with 200 partitions? Let's answer these questions: \n",
    "* First, a new stage is created when data needs to be moved, or *shuffled*, between partitions. \n",
    "* Indeed, this is the case for `groupBy`.\n",
    "* Last, 200 is the default number of partitions created when a shuffled is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This value can be customised as follows:\n",
    "\n",
    "###### Para establecer el numero de particiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List((\"a\", 1), (\"b\", 2), (\"a\", 3), (\"d\", 3), (\"b\", 4)).toDS\n",
    "    .map{ case (key, value) => (key, value + 1) }\n",
    "    .groupByKey(_._1)\n",
    "    .mapGroups((key, values) => (key, values.toList.map(_._2)))\n",
    "    .collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the contents of the different partitions after each transformation:\n",
    "\n",
    "###### Para observar el contenido de cada particion `.collectPartitions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List((\"a\", 1), (\"e\", 2), (\"f\", 3), (\"d\", 3), \n",
    "     (\"z\", 3), (\"k\", 3), (\"i\", 3), (\"o\", 3), \n",
    "     (\"l\",2), (\"b\", 2), (\"a\", 3), (\"d\", 3), \n",
    "     (\"b\", 4), (\"e\", 4)).toDS\n",
    "    .collectPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List((\"a\", 1), (\"e\", 2), (\"f\", 3), (\"d\", 3), (\"z\", 3), (\"k\", 3), (\"i\", 3), (\"o\", 3), (\"l\",2), (\"b\", 2), (\"a\", 3), (\"d\", 3), (\"b\", 4), (\"e\", 4)).toDS\n",
    "    .map{ case (key, value) => (key, value + 1) }\n",
    "    .collectPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List((\"a\", 1), (\"e\", 2), (\"f\", 3), (\"d\", 3), (\"z\", 3), (\"k\", 3), (\"i\", 3), (\"o\", 3), (\"l\",2), (\"b\", 2), (\"a\", 3), (\"d\", 3), (\"b\", 4), (\"e\", 4)).toDS\n",
    "    .map{ case (key, value) => (key, value + 1) }\n",
    "    .groupByKey(_._1)\n",
    "    .mapGroups((key, values) => (key, values.toList.map(_._2)))\n",
    "    .collectPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Narrow or wide?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformations in the Dataset API can be classified into narrow and wide transformations, systematically. We have already mentioned that `map` and `filter` belong to the former, and `groupByKey` to the latter. What about the following ones?\n",
    "\n",
    "###### Las transformaciones en la API de conjunto de datos se pueden clasificar en transformaciones estrechas y amplias, sistemáticamente. Ya hemos mencionado que map y filter pertenecen a transformaciones estrechas y groupByKey a transformacion amplia. ¿Qué pasa con los siguientes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `coalesce`\n",
    "\n",
    "###### Permite seleccionar el primer valor no nulo de un conjunto de columnas. Para establecer el numero de particiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ds = List(1,2,2,3,4,4,5,6,4,7,3,8).toDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.collectPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.coalesce(2).collectPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dropDuplicates`\n",
    "\n",
    "###### Para eliminar los duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.dropDuplicates.collectPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `flatMap`\n",
    "\n",
    "###### El método flatMap() es idéntico al método map(), pero la única diferencia es que en flatMap se elimina la agrupación interna de un elemento y se genera una secuencia.\n",
    "###### ----------------------------------------------------------------\n",
    "###### val name = Seq(\"Nidhi\", \"Singh\")\n",
    "###### val result1 = name.map(_.toLowerCase)\n",
    "###### Salida\n",
    "###### List(nidhi, singh)\n",
    "###### ----------------------------------------------------------------\n",
    "###### name.flatMap(_.toLowerCase)\n",
    "###### Salida\n",
    "###### List(n, i, d, h, i, s, i, n, g, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.flatMap(i => List(i, -i)).collectPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `limit`\n",
    "\n",
    "###### Maximo numero permitido el resto los quita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.limit(6).collectPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `mapPartitions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.mapPartitions{ it: Iterator[Int] => it.map(_ + 1) }.collectPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `repartition`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.repartition(2).collectPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare it with `coalesce`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.coalesce(2).collectPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differences can be \"explained\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.repartition(2).explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.coalesce(2).explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sort`\n",
    "\n",
    "###### Ordena por valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sort(\"value\").collectPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sort(\"value\").collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about transformations that relate several datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commonly, information is spread across several datasets, and the Spark Dataset API includes transformations to deal with this situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Union`\n",
    "\n",
    "###### Une todos los datos (particiones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ds1: Dataset[Int] = List(1,2,3,4).toDS.repartition(3)\n",
    "val ds2: Dataset[Int] = List(5,6,7,8).toDS.repartition(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1.collectPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2.collectPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1.union(ds2).collectPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No shuffle involved, just a single stage which makes the union of the different partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Join`\n",
    "\n",
    "###### Une los datos a traves de la columna que le indicamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object DS{\n",
    "    case class Person(name: String, age: Int)\n",
    "    case class Student(name: String, degree: String, year: Int)\n",
    "}\n",
    "\n",
    "import DS._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val people: Dataset[Person] = List(\n",
    "    Person(\"Yihui\", 20),\n",
    "    Person(\"Noelia\", 19),\n",
    "    Person(\"Gabriel\", 22),\n",
    "    Person(\"Javier\", 21)).toDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val students: Dataset[Student] = List(\n",
    "    Student(\"Yihui\", \"II\", 2000),\n",
    "    Student(\"Yihui\", \"M\", 2001),\n",
    "    Student(\"Noelia\", \"II\", 2000),\n",
    "    Student(\"Noelia\", \"IS\", 2000),\n",
    "    Student(\"Gabriel\", \"II\", 2004),\n",
    "    Student(\"Javier\", \"II\", 2005),\n",
    "    Student(\"Javier\", \"M\", 2005)).toDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.collectPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students.collectPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.join(students, \"name\").collectPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somewhat unexpectedly, there is no shuffle! This is because Spark performs the join following a \"broadcast\" strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.join(students, \"name\").explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This happens when one of the datasets is small enough to be copied for each partition. We can force Spark to avoid broadcast as follows (just for testing purposes): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\",0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.join(students, \"name\").collectPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching\n",
    "\n",
    "###### Con `.cache` guardamos los resultados/datos en cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most distinctive features of Spark is its ability to cache computations of datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ds5: Dataset[Int] = (0 to 1000).toDS.map(heavyComp(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(ds5.count)\n",
    "run(ds5.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with caching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds5.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(ds5.count)\n",
    "run(ds5.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may instruct the Spark interpreter to not use cached data:\n",
    "\n",
    "###### Para que no usar el dato de la cache utilizamos `.unpersist`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds5.unpersist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(ds5.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `cache` is not a pure transformation but a side-effectful operation. It just instructs the Spark interpreter to cache the dataset as soon as it's executed.  \n",
    "\n",
    "###### `.cache` es una operación secundaria. Simplemente indica al intérprete de Spark que almacene en caché el conjunto de datos tan pronto como se ejecute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ds1: Dataset[Int] = List(1,2,3,4).toDS.map(heavyComp())\n",
    "val ds1_cached: Dataset[Int] = ds1.cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may expect that the only cached dataset is `ds1_cached`, but that's not true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(ds1.count)\n",
    "run(ds1.count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

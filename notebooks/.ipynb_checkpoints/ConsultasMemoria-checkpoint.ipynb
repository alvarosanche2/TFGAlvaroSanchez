{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be1eabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.5` \n",
    "import $ivy.`sh.almond::almond-spark:0.4.0`\n",
    "\n",
    "import org.apache.spark.sql.{NotebookSparkSession, SparkSession}\n",
    "import org.apache.spark.sql.{functions => func, _}\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val spark = NotebookSparkSession\n",
    "      .builder()\n",
    "      .config(\"spark.sql.join.preferSortMergeJoin\", false)\n",
    "      .config(\"spark.sql.shuffle.partitions\", 64)\n",
    "      .master(\"local[8]\")\n",
    "      .getOrCreate()\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "import org.slf4j.LoggerFactory\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "\n",
    "Logger.getRootLogger().setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfd87f1",
   "metadata": {},
   "source": [
    "### Tranformacion de JSON multiline a JSON line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc29ba2c",
   "metadata": {},
   "source": [
    "Para esto lo que he hecho ha sido ejecutar en el CMD el siguiente comando `FOR %a IN (../data/*.json) DO jq . -c \"%a\" > \"../JSONLine/%a\"` con el que vamos recorriendo la carpeta donde se encuentran los ficheros JSON multiline y los vamos guardando con la transformacion en la carpeta JSONLine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8938e79a",
   "metadata": {},
   "source": [
    "### Leer y pasar datos mensuales a Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67747196",
   "metadata": {},
   "outputs": [],
   "source": [
    "val schema = StructType(\n",
    "    Array(\n",
    "        StructField(\"fecha\", StringType, true), \n",
    "        StructField(\"indicativo\", StringType, true),\n",
    "        StructField(\"nombre\", StringType, true),\n",
    "        StructField(\"provincia\", StringType, true),\n",
    "        StructField(\"altitud\", StringType, true),\n",
    "        StructField(\"tm_mes\", StringType, true),\n",
    "        StructField(\"tm_max\", StringType, true),\n",
    "        StructField(\"tm_min\", StringType, true), \n",
    "        StructField(\"ta_max\", StringType, true),\n",
    "        StructField(\"ta_min\", StringType, true),\n",
    "        StructField(\"ts_min\", StringType, true),\n",
    "        StructField(\"ti_max\", StringType, true),\n",
    "        StructField(\"nt_30\", StringType, true),\n",
    "        StructField(\"nt_00\", StringType, true),\n",
    "        StructField(\"p_mes\", StringType, true),\n",
    "        StructField(\"p_max\", StringType, true),\n",
    "        StructField(\"np_001\", StringType, true),\n",
    "        StructField(\"np_010\", StringType, true),\n",
    "        StructField(\"np_100\", StringType, true),\n",
    "        StructField(\"np_300\", StringType, true),\n",
    "        StructField(\"hr\", StringType, true), \n",
    "        StructField(\"e\", StringType, true),\n",
    "        StructField(\"n_llu\", StringType, true),\n",
    "        StructField(\"n_nie\", StringType, true),\n",
    "        StructField(\"n_gra\", StringType, true),\n",
    "        StructField(\"n_tor\", StringType, true),\n",
    "        StructField(\"n_fog\", StringType, true),\n",
    "        StructField(\"n_des\", StringType, true),\n",
    "        StructField(\"n_nub\", StringType, true),\n",
    "        StructField(\"n_cub\", StringType, true),\n",
    "        StructField(\"inso\", StringType, true),\n",
    "        StructField(\"p_sol\", StringType, true),\n",
    "        StructField(\"glo\", StringType, true),\n",
    "        StructField(\"evap\", StringType, true),\n",
    "        StructField(\"w_rec\", StringType, true),\n",
    "        StructField(\"w_racha\", StringType, true),\n",
    "        StructField(\"nw_55\", StringType, true),\n",
    "        StructField(\"nw_91\", StringType, true),\n",
    "        StructField(\"w_med\", StringType, true),\n",
    "        StructField(\"q_med\", StringType, true),\n",
    "        StructField(\"q_max\", StringType, true),\n",
    "        StructField(\"q_min\", StringType, true),\n",
    "        StructField(\"q_mar\", StringType, true),\n",
    "        StructField(\"ts_10\", StringType, true),\n",
    "        StructField(\"ts_20\", StringType, true),\n",
    "        StructField(\"ts_50\", StringType, true),\n",
    "        StructField(\"nv_0050\", StringType, true),\n",
    "        StructField(\"nv_0100\", StringType, true),\n",
    "        StructField(\"nv_1000\", StringType, true)       \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7946c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "val allData = spark.read.schema(schema).json(\"D:/TFGAlvaroSanchez/data/monthJSONLine/*.json\")\n",
    "    .select(\n",
    "            $\"fecha\".cast(DateType), \n",
    "            $\"indicativo\", \n",
    "            $\"p_max\",\n",
    "            $\"glo\".cast(DoubleType), \n",
    "            $\"hr\".cast(DoubleType), \n",
    "            $\"nw_55\".cast(IntegerType), \n",
    "            $\"tm_min\".cast(DoubleType), \n",
    "            $\"ta_max\", \n",
    "            $\"ts_min\".cast(DoubleType), \n",
    "            $\"nt_30\".cast(IntegerType), \n",
    "            $\"n_des\".cast(IntegerType), \n",
    "            $\"w_racha\", \n",
    "            $\"np_100\".cast(IntegerType), \n",
    "            $\"nw_91\".cast(IntegerType), \n",
    "            $\"np_001\".cast(IntegerType), \n",
    "            $\"ta_min\", \n",
    "            $\"w_rec\".cast(IntegerType), \n",
    "            $\"e\".cast(DoubleType), \n",
    "            $\"np_300\".cast(IntegerType), \n",
    "            $\"p_mes\".cast(DoubleType), \n",
    "            $\"w_med\".cast(DoubleType), \n",
    "            $\"nt_00\".cast(IntegerType), \n",
    "            $\"ti_max\".cast(DoubleType), \n",
    "            $\"tm_mes\".cast(DoubleType), \n",
    "            $\"tm_max\".cast(DoubleType), \n",
    "            $\"np_010\".cast(IntegerType)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fe0305",
   "metadata": {},
   "outputs": [],
   "source": [
    "allData.write.format(\"parquet\").partitionBy(\"indicativo\").mode(\"overwrite\").save(\"D:/TFGAlvaroSanchez/data/monthParquet/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4035ff",
   "metadata": {},
   "source": [
    "### Leer y pasar datos diarios a Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd874b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "val schema = StructType(\n",
    "    Array(\n",
    "        StructField(\"fecha\", StringType, true), \n",
    "        StructField(\"indicativo\", StringType, true),\n",
    "        StructField(\"nombre\", StringType, true),\n",
    "        StructField(\"provincia\", StringType, true),\n",
    "        StructField(\"altitud\", StringType, true),\n",
    "        StructField(\"tmed\", StringType, true),\n",
    "        StructField(\"prec\", StringType, true),\n",
    "        StructField(\"tmin\", StringType, true),\n",
    "        StructField(\"horatmin\", StringType, true),\n",
    "        StructField(\"tmax\", StringType, true),\n",
    "        StructField(\"horatmax\", StringType, true),\n",
    "        StructField(\"dir\", StringType, true),\n",
    "        StructField(\"velmedia\", StringType, true),\n",
    "        StructField(\"racha\", StringType, true),\n",
    "        StructField(\"horaracha\", StringType, true),\n",
    "        StructField(\"sol\", StringType, true),\n",
    "        StructField(\"presMax\", StringType, true),\n",
    "        StructField(\"horaPresMax\", StringType, true),\n",
    "        StructField(\"presMin\", StringType, true),\n",
    "        StructField(\"horaPresMin\", StringType, true)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c118e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val allData = spark.read.schema(schema).json(\"D:/TFGAlvaroSanchez/data/dayJSONLine/*.json\")\n",
    "    .withColumn(\"fecha\", $\"fecha\".cast(DateType))\n",
    "    .withColumn(\"altitud\", $\"altitud\".cast(IntegerType))\n",
    "    .withColumn(\"tmed\", func.regexp_replace($\"tmed\", \",\", \".\").cast(DoubleType))\n",
    "    .withColumn(\"prec\", func.regexp_replace($\"prec\", \",\", \".\").cast(DoubleType))\n",
    "    .withColumn(\"tmin\", func.regexp_replace($\"tmin\", \",\", \".\").cast(DoubleType))\n",
    "    .withColumn(\"tmax\", func.regexp_replace($\"tmax\", \",\", \".\").cast(DoubleType))\n",
    "    .withColumn(\"dir\", $\"dir\".cast(IntegerType))\n",
    "    .withColumn(\"velmedia\", func.regexp_replace($\"velmedia\", \",\", \".\").cast(DoubleType))\n",
    "    .withColumn(\"racha\", func.regexp_replace($\"racha\", \",\", \".\").cast(DoubleType))\n",
    "    .withColumn(\"sol\", func.regexp_replace($\"sol\", \",\", \".\").cast(DoubleType))\n",
    "    .withColumn(\"presMax\", func.regexp_replace($\"presMax\", \",\", \".\").cast(DoubleType))\n",
    "    .withColumn(\"horaPresMax\", $\"horaPresMax\".cast(IntegerType))\n",
    "    .withColumn(\"presMin\", func.regexp_replace($\"presMin\", \",\", \".\").cast(DoubleType))\n",
    "    .withColumn(\"horaPresMin\", $\"horaPresmin\".cast(IntegerType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c602d80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "allData.write.format(\"parquet\").partitionBy(\"indicativo\").mode(\"overwrite\").save(\"D:/TFGAlvaroSanchez/data/dayParquet/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565bfa11",
   "metadata": {},
   "source": [
    "### Año de la temperatura máxima promedio en cada mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be0f3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.plotly-scala::plotly-almond:0.7.0`\n",
    "import plotly._, plotly.element._, plotly.layout._, plotly.Almond._\n",
    "\n",
    "// restrict the output height to avoid scrolling in output cells\n",
    "\n",
    "repl.pprinter() = repl.pprinter().copy(defaultHeight = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fadbed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "val data = spark.read.parquet(\"D:/TFGAlvaroSanchez/data/monthParquet/*\").na.drop()\n",
    "\n",
    "val window = Window.partitionBy(\"mes\").orderBy($\"ta_max\".desc)\n",
    "\n",
    "val dataWindow = data\n",
    "    .withColumn(\"ta_max\", func.split($\"ta_max\", \"\\\\(\")(0).cast(DoubleType))\n",
    "    .groupBy($\"fecha\")\n",
    "    .agg(func.avg($\"ta_max\").alias(\"ta_max\"))\n",
    "    .select(func.month($\"fecha\").alias(\"mes\"), func.year($\"fecha\").alias(\"año\"), $\"ta_max\")\n",
    "    .withColumn(\"dense_rank\", func.dense_rank().over(window))\n",
    "    .filter($\"dense_rank\" === 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2971a6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countByYear(year : Int) : Long = {\n",
    "    dataWindow\n",
    "        .filter($\"año\" === year)\n",
    "        .count()\n",
    "}\n",
    " \n",
    "val year2010 = countByYear(2010)\n",
    "val year2011 = countByYear(2011)\n",
    "val year2012 = countByYear(2012)\n",
    "val year2013 = countByYear(2013)\n",
    "val year2014 = countByYear(2014)\n",
    "val year2015 = countByYear(2015)\n",
    "val year2016 = countByYear(2016)\n",
    "val year2017 = countByYear(2017)\n",
    "val year2018 = countByYear(2018)\n",
    "val year2019 = countByYear(2019)\n",
    "val year2020 = countByYear(2020)\n",
    "val year2021 = countByYear(2021)\n",
    "val year2022 = countByYear(2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926f11c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataWindow\n",
    "    .withColumn(\"temperatura maxima\", func.round($\"ta_max\", 2))\n",
    "    .orderBy($\"mes\".asc)\n",
    "    //.select($\"mes\", $\"año\", $\"temperatura maxima\")\n",
    "    .show()\n",
    "\n",
    "val dataToPlot = Seq(\n",
    "  Bar(\n",
    "    Seq(2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022),\n",
    "    Seq(year2010, year2011, year2012, year2013, year2014, year2015, year2016, \n",
    "        year2017, year2018, year2019, year2020, year2021, year2022),\n",
    "    marker = Marker(\n",
    "        color = Color.RGB(255, 174, 0),\n",
    "        opacity = 0.6,\n",
    "        line = Line(\n",
    "            color = Color.RGB(189, 129, 0),\n",
    "              width = 1.5\n",
    "        )\n",
    "    )\n",
    "  )\n",
    ")\n",
    "\n",
    "plot(dataToPlot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f82f55",
   "metadata": {},
   "source": [
    "### Olas de calor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed29262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "val data = spark.read.parquet(\"D:/TFGAlvaroSanchez/data/dayParquet/\")\n",
    "val stations = spark.read.option(\"delimiter\", \";\").csv(\"D:/TFGAlvaroSanchez/data/aemetID.csv\")\n",
    "    .toDF(\"provincia\", \"indicativo\", \"ubicacion\")\n",
    "\n",
    "val window = Window.partitionBy($\"indicativo\", $\"año\").orderBy($\"fecha\")\n",
    "\n",
    "val results = data\n",
    "    .filter(!func.isnull($\"tmax\") && $\"tmax\" >= 40)\n",
    "    .withColumn(\"año\", func.year($\"fecha\"))\n",
    "    .withColumn(\"n_fila\", func.row_number().over(window))\n",
    "    .withColumn(\"id\", func.expr(\"date_sub(fecha, n_fila)\"))\n",
    "    .groupBy($\"indicativo\", $\"año\", $\"id\")\n",
    "    .agg(func.count($\"id\").alias(\"dias\"), func.avg($\"tmax\"), func.max($\"tmax\"), func.min($\"tmax\"))\n",
    "    .filter($\"dias\" > 3)\n",
    "    .join(stations, \"indicativo\")\n",
    "    .select($\"ubicacion\", $\"provincia\", $\"año\", $\"dias\", func.round($\"avg(tmax)\", 2).alias(\"avg(tmax)\"), \n",
    "            $\"max(tmax)\", $\"min(tmax)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdd0fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val resultsSave = results\n",
    "    .groupBy($\"provincia\", $\"año\")\n",
    "    .agg(func.count($\"provincia\"), func.avg($\"dias\"), func.avg($\"avg(tmax)\"), func.max($\"max(tmax)\"), func.min($\"min(tmax)\"))\n",
    "    .select($\"provincia\", $\"año\", $\"count(provincia)\".alias(\"nº de olas de calor\"), \n",
    "            $\"avg(dias)\".alias(\"duracion media\"), $\"avg(avg(tmax))\".alias(\"avg(tmax)\"), \n",
    "            $\"max(max(tmax))\".alias(\"max(tmax)\"), $\"min(min(tmax))\".alias(\"min(tmax)\"))\n",
    "    \n",
    "resultsSave\n",
    "    .withColumnRenamed(\"nº de olas de calor\", \"nOlasCalor\")\n",
    "    .withColumnRenamed(\"duracion media\", \"duracionMedia\")\n",
    "    .withColumnRenamed(\"avg(tmax)\", \"avgTmax\")\n",
    "    .withColumnRenamed(\"max(tmax)\", \"maxTmax\")\n",
    "    .withColumnRenamed(\"min(tmax)\", \"minTmax\")\n",
    "    .write.format(\"parquet\").partitionBy(\"provincia\").mode(\"overwrite\").save(\"D:/TFGAlvaroSanchez/data/resultadoOlasCalor/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cd2e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "results\n",
    "    .withColumnRenamed(\"dias\", \"duracion (dias)\")\n",
    "    .withColumnRenamed(\"avg(tmax)\", \"temperatura media\")\n",
    "    .withColumnRenamed(\"max(tmax)\", \"temperatura maxima\")\n",
    "    .withColumnRenamed(\"min(tmax)\", \"temperatura minima\")\n",
    "    .orderBy($\"ubicacion\", $\"año\")\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbae1f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "results\n",
    "    .groupBy($\"año\")\n",
    "    .agg(func.count($\"año\").alias(\"nº olas de calor\"))\n",
    "    .orderBy($\"año\")\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0a4f7d",
   "metadata": {},
   "source": [
    "Para observar los resultados de forma gráfica en el mapa acceder al notebook `RepresentacionOlasCalor`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

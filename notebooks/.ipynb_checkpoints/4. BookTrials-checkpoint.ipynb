{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e3b5f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spark-stubs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (org.eclipse.jetty.util.log).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkSession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "22/09/18 17:49:49 INFO SparkContext: Running Spark version 2.4.5\n",
      "22/09/18 17:49:50 INFO SparkContext: Submitted application: bbf6fb9e-4bc4-4572-9506-ade1d1252201\n",
      "22/09/18 17:49:50 INFO SecurityManager: Changing view acls to: Alvaro\n",
      "22/09/18 17:49:50 INFO SecurityManager: Changing modify acls to: Alvaro\n",
      "22/09/18 17:49:50 INFO SecurityManager: Changing view acls groups to: \n",
      "22/09/18 17:49:50 INFO SecurityManager: Changing modify acls groups to: \n",
      "22/09/18 17:49:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Alvaro); groups with view permissions: Set(); users  with modify permissions: Set(Alvaro); groups with modify permissions: Set()\n",
      "22/09/18 17:49:51 INFO Utils: Successfully started service 'sparkDriver' on port 64515.\n",
      "22/09/18 17:49:51 INFO SparkEnv: Registering MapOutputTracker\n",
      "22/09/18 17:49:51 INFO SparkEnv: Registering BlockManagerMaster\n",
      "22/09/18 17:49:51 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "22/09/18 17:49:51 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "22/09/18 17:49:51 INFO DiskBlockManager: Created local directory at C:\\Users\\alvar\\AppData\\Local\\Temp\\blockmgr-f3997c0b-c255-4309-918e-a0e02911f078\n",
      "22/09/18 17:49:51 INFO MemoryStore: MemoryStore started with capacity 1646.4 MB\n",
      "22/09/18 17:49:51 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "22/09/18 17:49:51 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "22/09/18 17:49:51 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://LAPTOP-30N4Q54J:4040\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/jitpack.io/com/github/jupyter/jvm-repr/0.4.0/jvm-repr-0.4.0.jar at spark://LAPTOP-30N4Q54J:64515/jars/jvm-repr-0.4.0.jar with timestamp 1663516191853\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/github/javaparser/javaparser-core/3.2.5/javaparser-core-3.2.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/javaparser-core-3.2.5.jar with timestamp 1663516191854\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/github/scopt/scopt_2.12/3.5.0/scopt_2.12-3.5.0.jar at spark://LAPTOP-30N4Q54J:64515/jars/scopt_2.12-3.5.0.jar with timestamp 1663516191855\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/acyclic_2.12/0.1.5/acyclic_2.12-0.1.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/acyclic_2.12-0.1.5.jar with timestamp 1663516191856\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-interp_2.12.8/1.6.5/ammonite-interp_2.12.8-1.6.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/ammonite-interp_2.12.8-1.6.5.jar with timestamp 1663516191857\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-ops_2.12/1.6.5/ammonite-ops_2.12-1.6.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/ammonite-ops_2.12-1.6.5.jar with timestamp 1663516191857\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-repl_2.12.8/1.6.5/ammonite-repl_2.12.8-1.6.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/ammonite-repl_2.12.8-1.6.5.jar with timestamp 1663516191859\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-runtime_2.12/1.6.5/ammonite-runtime_2.12-1.6.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/ammonite-runtime_2.12-1.6.5.jar with timestamp 1663516191860\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-terminal_2.12/1.6.5/ammonite-terminal_2.12-1.6.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/ammonite-terminal_2.12-1.6.5.jar with timestamp 1663516191863\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-util_2.12/1.6.5/ammonite-util_2.12-1.6.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/ammonite-util_2.12-1.6.5.jar with timestamp 1663516191864\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/fansi_2.12/0.2.4/fansi_2.12-0.2.4.jar at spark://LAPTOP-30N4Q54J:64515/jars/fansi_2.12-0.2.4.jar with timestamp 1663516191868\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/fastparse_2.12/2.1.0/fastparse_2.12-2.1.0.jar at spark://LAPTOP-30N4Q54J:64515/jars/fastparse_2.12-2.1.0.jar with timestamp 1663516191870\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/geny_2.12/0.1.5/geny_2.12-0.1.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/geny_2.12-0.1.5.jar with timestamp 1663516191870\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/os-lib_2.12/0.2.6/os-lib_2.12-0.2.6.jar at spark://LAPTOP-30N4Q54J:64515/jars/os-lib_2.12-0.2.6.jar with timestamp 1663516191871\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/pprint_2.12/0.5.2/pprint_2.12-0.5.2.jar at spark://LAPTOP-30N4Q54J:64515/jars/pprint_2.12-0.5.2.jar with timestamp 1663516191872\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/requests_2.12/0.1.7/requests_2.12-0.1.7.jar at spark://LAPTOP-30N4Q54J:64515/jars/requests_2.12-0.1.7.jar with timestamp 1663516191873\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/scalaparse_2.12/2.1.0/scalaparse_2.12-2.1.0.jar at spark://LAPTOP-30N4Q54J:64515/jars/scalaparse_2.12-2.1.0.jar with timestamp 1663516191874\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/sourcecode_2.12/0.1.5/sourcecode_2.12-0.1.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/sourcecode_2.12-0.1.5.jar with timestamp 1663516191875\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/ujson_2.12/0.7.1/ujson_2.12-0.7.1.jar at spark://LAPTOP-30N4Q54J:64515/jars/ujson_2.12-0.7.1.jar with timestamp 1663516191876\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/upack_2.12/0.7.1/upack_2.12-0.7.1.jar at spark://LAPTOP-30N4Q54J:64515/jars/upack_2.12-0.7.1.jar with timestamp 1663516191877\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/upickle-core_2.12/0.7.1/upickle-core_2.12-0.7.1.jar at spark://LAPTOP-30N4Q54J:64515/jars/upickle-core_2.12-0.7.1.jar with timestamp 1663516191878\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/upickle-implicits_2.12/0.7.1/upickle-implicits_2.12-0.7.1.jar at spark://LAPTOP-30N4Q54J:64515/jars/upickle-implicits_2.12-0.7.1.jar with timestamp 1663516191879\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/upickle_2.12/0.7.1/upickle_2.12-0.7.1.jar at spark://LAPTOP-30N4Q54J:64515/jars/upickle_2.12-0.7.1.jar with timestamp 1663516191880\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/utest_2.12/0.6.4/utest_2.12-0.6.4.jar at spark://LAPTOP-30N4Q54J:64515/jars/utest_2.12-0.6.4.jar with timestamp 1663516191882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/io/get-coursier/coursier-cache_2.12/1.1.0-M13-1/coursier-cache_2.12-1.1.0-M13-1.jar at spark://LAPTOP-30N4Q54J:64515/jars/coursier-cache_2.12-1.1.0-M13-1.jar with timestamp 1663516191884\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/io/get-coursier/coursier-core_2.12/1.1.0-M13-1/coursier-core_2.12-1.1.0-M13-1.jar at spark://LAPTOP-30N4Q54J:64515/jars/coursier-core_2.12-1.1.0-M13-1.jar with timestamp 1663516191885\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/io/get-coursier/coursier_2.12/1.1.0-M13-1/coursier_2.12-1.1.0-M13-1.jar at spark://LAPTOP-30N4Q54J:64515/jars/coursier_2.12-1.1.0-M13-1.jar with timestamp 1663516191886\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/net/java/dev/jna/jna/4.2.2/jna-4.2.2.jar at spark://LAPTOP-30N4Q54J:64515/jars/jna-4.2.2.jar with timestamp 1663516191886\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/javassist/javassist/3.21.0-GA/javassist-3.21.0-GA.jar at spark://LAPTOP-30N4Q54J:64515/jars/javassist-3.21.0-GA.jar with timestamp 1663516191887\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/jline/jline-reader/3.6.2/jline-reader-3.6.2.jar at spark://LAPTOP-30N4Q54J:64515/jars/jline-reader-3.6.2.jar with timestamp 1663516191888\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/jline/jline-terminal-jna/3.6.2/jline-terminal-jna-3.6.2.jar at spark://LAPTOP-30N4Q54J:64515/jars/jline-terminal-jna-3.6.2.jar with timestamp 1663516191888\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/jline/jline-terminal/3.6.2/jline-terminal-3.6.2.jar at spark://LAPTOP-30N4Q54J:64515/jars/jline-terminal-3.6.2.jar with timestamp 1663516191889\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.12/1.1.1/scala-xml_2.12-1.1.1.jar at spark://LAPTOP-30N4Q54J:64515/jars/scala-xml_2.12-1.1.1.jar with timestamp 1663516191890\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/scala-sbt/test-interface/1.0/test-interface-1.0.jar at spark://LAPTOP-30N4Q54J:64515/jars/test-interface-1.0.jar with timestamp 1663516191891\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/sh/almond/interpreter-api_2.12/0.4.0/interpreter-api_2.12-0.4.0.jar at spark://LAPTOP-30N4Q54J:64515/jars/interpreter-api_2.12-0.4.0.jar with timestamp 1663516191892\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/sh/almond/jupyter-api_2.12/0.4.0/jupyter-api_2.12-0.4.0.jar at spark://LAPTOP-30N4Q54J:64515/jars/jupyter-api_2.12-0.4.0.jar with timestamp 1663516191893\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/sh/almond/scala-kernel-api_2.12.8/0.4.0/scala-kernel-api_2.12.8-0.4.0.jar at spark://LAPTOP-30N4Q54J:64515/jars/scala-kernel-api_2.12.8-0.4.0.jar with timestamp 1663516191894\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/json4s/json4s-core_2.12/3.5.3/json4s-core_2.12-3.5.3.jar at spark://LAPTOP-30N4Q54J:64515/jars/json4s-core_2.12-3.5.3.jar with timestamp 1663516191895\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar at spark://LAPTOP-30N4Q54J:64515/jars/xml-apis-1.3.04.jar with timestamp 1663516191896\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar at spark://LAPTOP-30N4Q54J:64515/jars/javax.servlet-api-3.1.0.jar with timestamp 1663516191898\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/metrics-core-3.1.5.jar with timestamp 1663516191899\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/google/inject/guice/3.0/guice-3.0.jar at spark://LAPTOP-30N4Q54J:64515/jars/guice-3.0.jar with timestamp 1663516191900\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar at spark://LAPTOP-30N4Q54J:64515/jars/slf4j-log4j12-1.7.16.jar with timestamp 1663516191901\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar at spark://LAPTOP-30N4Q54J:64515/jars/commons-crypto-1.0.0.jar with timestamp 1663516191902\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar at spark://LAPTOP-30N4Q54J:64515/jars/xbean-asm6-shaded-4.8.jar with timestamp 1663516191903\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar at spark://LAPTOP-30N4Q54J:64515/jars/xercesImpl-2.9.1.jar with timestamp 1663516191904\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/github/luben/zstd-jni/1.3.2-2/zstd-jni-1.3.2-2.jar at spark://LAPTOP-30N4Q54J:64515/jars/zstd-jni-1.3.2-2.jar with timestamp 1663516191905\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar at spark://LAPTOP-30N4Q54J:64515/jars/jettison-1.1.jar with timestamp 1663516191906\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/2.4.5/spark-unsafe_2.12-2.4.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/spark-unsafe_2.12-2.4.5.jar with timestamp 1663516191906\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-yarn-server-nodemanager/2.6.5/hadoop-yarn-server-nodemanager-2.6.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/hadoop-yarn-server-nodemanager-2.6.5.jar with timestamp 1663516191907\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/parquet/parquet-hadoop/1.10.1/parquet-hadoop-1.10.1.jar at spark://LAPTOP-30N4Q54J:64515/jars/parquet-hadoop-1.10.1.jar with timestamp 1663516191907\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/joda-time/joda-time/2.9.9/joda-time-2.9.9.jar at spark://LAPTOP-30N4Q54J:64515/jars/joda-time-2.9.9.jar with timestamp 1663516191908\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/commons-io/commons-io/2.4/commons-io-2.4.jar at spark://LAPTOP-30N4Q54J:64515/jars/commons-io-2.4.jar with timestamp 1663516191909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar at spark://LAPTOP-30N4Q54J:64515/jars/commons-configuration-1.6.jar with timestamp 1663516191910\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.7/antlr4-runtime-4.7.jar at spark://LAPTOP-30N4Q54J:64515/jars/antlr4-runtime-4.7.jar with timestamp 1663516191910\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar at spark://LAPTOP-30N4Q54J:64515/jars/jersey-common-2.22.2.jar with timestamp 1663516191911\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/io/airlift/aircompressor/0.10/aircompressor-0.10.jar at spark://LAPTOP-30N4Q54J:64515/jars/aircompressor-0.10.jar with timestamp 1663516191912\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar at spark://LAPTOP-30N4Q54J:64515/jars/commons-compress-1.8.1.jar with timestamp 1663516191913\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar at spark://LAPTOP-30N4Q54J:64515/jars/jcl-over-slf4j-1.7.16.jar with timestamp 1663516191914\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar at spark://LAPTOP-30N4Q54J:64515/jars/javassist-3.18.1-GA.jar with timestamp 1663516191915\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar at spark://LAPTOP-30N4Q54J:64515/jars/aopalliance-repackaged-2.4.0-b34.jar with timestamp 1663516191916\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.jar at spark://LAPTOP-30N4Q54J:64515/jars/lz4-java-1.4.0.jar with timestamp 1663516191917\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar at spark://LAPTOP-30N4Q54J:64515/jars/netty-3.9.9.Final.jar with timestamp 1663516191918\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-network-common_2.12/2.4.5/spark-network-common_2.12-2.4.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/spark-network-common_2.12-2.4.5.jar with timestamp 1663516191919\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/aopalliance/aopalliance/1.0/aopalliance-1.0.jar at spark://LAPTOP-30N4Q54J:64515/jars/aopalliance-1.0.jar with timestamp 1663516191920\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.7.9/jackson-core-2.7.9.jar at spark://LAPTOP-30N4Q54J:64515/jars/jackson-core-2.7.9.jar with timestamp 1663516191924\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-sql_2.12/2.4.5/spark-sql_2.12-2.4.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/spark-sql_2.12-2.4.5.jar with timestamp 1663516191925\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar at spark://LAPTOP-30N4Q54J:64515/jars/gson-2.2.4.jar with timestamp 1663516191926\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar at spark://LAPTOP-30N4Q54J:64515/jars/api-util-1.0.0-M20.jar with timestamp 1663516191927\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/hadoop-yarn-client-2.6.5.jar with timestamp 1663516191928\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar at spark://LAPTOP-30N4Q54J:64515/jars/jaxb-api-2.2.2.jar with timestamp 1663516191930\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-tags_2.12/2.4.5/spark-tags_2.12-2.4.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/spark-tags_2.12-2.4.5.jar with timestamp 1663516191932\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/parquet/parquet-jackson/1.10.1/parquet-jackson-1.10.1.jar at spark://LAPTOP-30N4Q54J:64515/jars/parquet-jackson-1.10.1.jar with timestamp 1663516191934\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar at spark://LAPTOP-30N4Q54J:64515/jars/shims-0.7.45.jar with timestamp 1663516191935\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar at spark://LAPTOP-30N4Q54J:64515/jars/paranamer-2.8.jar with timestamp 1663516191938\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/avro/avro/1.8.2/avro-1.8.2.jar at spark://LAPTOP-30N4Q54J:64515/jars/avro-1.8.2.jar with timestamp 1663516191939\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/httpcomponents/httpcore/4.2.4/httpcore-4.2.4.jar at spark://LAPTOP-30N4Q54J:64515/jars/httpcore-4.2.4.jar with timestamp 1663516191941\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/carrotsearch/hppc/0.7.2/hppc-0.7.2.jar at spark://LAPTOP-30N4Q54J:64515/jars/hppc-0.7.2.jar with timestamp 1663516191942\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.6.7.3/jackson-databind-2.6.7.3.jar at spark://LAPTOP-30N4Q54J:64515/jars/jackson-databind-2.6.7.3.jar with timestamp 1663516191946\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-kvstore_2.12/2.4.5/spark-kvstore_2.12-2.4.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/spark-kvstore_2.12-2.4.5.jar with timestamp 1663516191956\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/commons-codec/commons-codec/1.10/commons-codec-1.10.jar at spark://LAPTOP-30N4Q54J:64515/jars/commons-codec-1.10.jar with timestamp 1663516191957\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar at spark://LAPTOP-30N4Q54J:64515/jars/jetty-util-6.1.26.jar with timestamp 1663516191958\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/hadoop-yarn-common-2.6.5.jar with timestamp 1663516191958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/json4s/json4s-scalap_2.12/3.5.3/json4s-scalap_2.12-3.5.3.jar at spark://LAPTOP-30N4Q54J:64515/jars/json4s-scalap_2.12-3.5.3.jar with timestamp 1663516191959\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar at spark://LAPTOP-30N4Q54J:64515/jars/commons-beanutils-1.7.0.jar with timestamp 1663516191959\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar at spark://LAPTOP-30N4Q54J:64515/jars/avro-ipc-1.8.2.jar with timestamp 1663516191961\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-auth/2.6.5/hadoop-auth-2.6.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/hadoop-auth-2.6.5.jar with timestamp 1663516191962\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/arrow/arrow-vector/0.10.0/arrow-vector-0.10.0.jar at spark://LAPTOP-30N4Q54J:64515/jars/arrow-vector-0.10.0.jar with timestamp 1663516191963\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/parquet/parquet-common/1.10.1/parquet-common-1.10.1.jar at spark://LAPTOP-30N4Q54J:64515/jars/parquet-common-1.10.1.jar with timestamp 1663516191965\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar at spark://LAPTOP-30N4Q54J:64515/jars/compress-lzf-1.0.3.jar with timestamp 1663516191966\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/log4j/log4j/1.2.17/log4j-1.2.17.jar at spark://LAPTOP-30N4Q54J:64515/jars/log4j-1.2.17.jar with timestamp 1663516191967\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar at spark://LAPTOP-30N4Q54J:64515/jars/stream-2.7.0.jar with timestamp 1663516191968\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/vlkan/flatbuffers/1.2.0-3f79e055/flatbuffers-1.2.0-3f79e055.jar at spark://LAPTOP-30N4Q54J:64515/jars/flatbuffers-1.2.0-3f79e055.jar with timestamp 1663516191969\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar at spark://LAPTOP-30N4Q54J:64515/jars/apacheds-i18n-2.0.0-M15.jar with timestamp 1663516191969\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/javax/activation/activation/1.1.1/activation-1.1.1.jar at spark://LAPTOP-30N4Q54J:64515/jars/activation-1.1.1.jar with timestamp 1663516191970\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/javax/inject/javax.inject/1/javax.inject-1.jar at spark://LAPTOP-30N4Q54J:64515/jars/javax.inject-1.jar with timestamp 1663516191971\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar at spark://LAPTOP-30N4Q54J:64515/jars/jul-to-slf4j-1.7.16.jar with timestamp 1663516191972\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar at spark://LAPTOP-30N4Q54J:64515/jars/minlog-1.3.0.jar with timestamp 1663516191973\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/xmlenc/xmlenc/0.52/xmlenc-0.52.jar at spark://LAPTOP-30N4Q54J:64515/jars/xmlenc-0.52.jar with timestamp 1663516191974\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/univocity/univocity-parsers/2.7.3/univocity-parsers-2.7.3.jar at spark://LAPTOP-30N4Q54J:64515/jars/univocity-parsers-2.7.3.jar with timestamp 1663516191975\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/codehaus/janino/janino/3.0.9/janino-3.0.9.jar at spark://LAPTOP-30N4Q54J:64515/jars/janino-3.0.9.jar with timestamp 1663516191975\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-core_2.12/2.4.5/spark-core_2.12-2.4.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/spark-core_2.12-2.4.5.jar with timestamp 1663516191976\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/commons-lang/commons-lang/2.6/commons-lang-2.6.jar at spark://LAPTOP-30N4Q54J:64515/jars/commons-lang-2.6.jar with timestamp 1663516191978\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.25/slf4j-api-1.7.25.jar at spark://LAPTOP-30N4Q54J:64515/jars/slf4j-api-1.7.25.jar with timestamp 1663516191979\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/oro/oro/2.0.8/oro-2.0.8.jar at spark://LAPTOP-30N4Q54J:64515/jars/oro-2.0.8.jar with timestamp 1663516191980\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar at spark://LAPTOP-30N4Q54J:64515/jars/javax.annotation-api-1.2.jar with timestamp 1663516191981\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/commons-cli/commons-cli/1.2/commons-cli-1.2.jar at spark://LAPTOP-30N4Q54J:64515/jars/commons-cli-1.2.jar with timestamp 1663516191982\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar at spark://LAPTOP-30N4Q54J:64515/jars/commons-math3-3.4.1.jar with timestamp 1663516191983\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar at spark://LAPTOP-30N4Q54J:64515/jars/jackson-core-asl-1.9.13.jar with timestamp 1663516191984\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/orc/orc-core/1.5.5/orc-core-1.5.5-nohive.jar at spark://LAPTOP-30N4Q54J:64515/jars/orc-core-1.5.5-nohive.jar with timestamp 1663516191985\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/google/guava/guava/16.0.1/guava-16.0.1.jar at spark://LAPTOP-30N4Q54J:64515/jars/guava-16.0.1.jar with timestamp 1663516191986\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/hadoop-yarn-server-common-2.6.5.jar with timestamp 1663516191987\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar at spark://LAPTOP-30N4Q54J:64515/jars/hk2-api-2.4.0-b34.jar with timestamp 1663516191989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar at spark://LAPTOP-30N4Q54J:64515/jars/leveldbjni-all-1.8.jar with timestamp 1663516191990\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar at spark://LAPTOP-30N4Q54J:64515/jars/osgi-resource-locator-1.0.1.jar with timestamp 1663516191991\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.12/1.1.0/scala-parser-combinators_2.12-1.1.0.jar at spark://LAPTOP-30N4Q54J:64515/jars/scala-parser-combinators_2.12-1.1.0.jar with timestamp 1663516191991\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-catalyst_2.12/2.4.5/spark-catalyst_2.12-2.4.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/spark-catalyst_2.12-2.4.5.jar with timestamp 1663516191992\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/hadoop-yarn-api-2.6.5.jar with timestamp 1663516191993\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/commons-lang3-3.5.jar with timestamp 1663516191994\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar at spark://LAPTOP-30N4Q54J:64515/jars/commons-collections-3.2.2.jar with timestamp 1663516191995\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar at spark://LAPTOP-30N4Q54J:64515/jars/hk2-utils-2.4.0-b34.jar with timestamp 1663516191996\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/tukaani/xz/1.5/xz-1.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/xz-1.5.jar with timestamp 1663516191996\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/2.6.5/hadoop-common-2.6.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/hadoop-common-2.6.5.jar with timestamp 1663516191997\n",
      "22/09/18 17:49:51 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.5/hadoop-mapreduce-client-core-2.6.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/hadoop-mapreduce-client-core-2.6.5.jar with timestamp 1663516191998\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/jline/jline/0.9.94/jline-0.9.94.jar at spark://LAPTOP-30N4Q54J:64515/jars/jline-0.9.94.jar with timestamp 1663516192004\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/orc/orc-mapreduce/1.5.5/orc-mapreduce-1.5.5-nohive.jar at spark://LAPTOP-30N4Q54J:64515/jars/orc-mapreduce-1.5.5-nohive.jar with timestamp 1663516192005\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/httpclient-4.2.5.jar with timestamp 1663516192006\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-annotations/2.6.5/hadoop-annotations-2.6.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/hadoop-annotations-2.6.5.jar with timestamp 1663516192007\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar at spark://LAPTOP-30N4Q54J:64515/jars/unused-1.0.0.jar with timestamp 1663516192028\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar at spark://LAPTOP-30N4Q54J:64515/jars/jersey-container-servlet-core-2.22.2.jar with timestamp 1663516192048\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/codehaus/janino/commons-compiler/3.0.9/commons-compiler-3.0.9.jar at spark://LAPTOP-30N4Q54J:64515/jars/commons-compiler-3.0.9.jar with timestamp 1663516192054\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar at spark://LAPTOP-30N4Q54J:64515/jars/hk2-locator-2.4.0-b34.jar with timestamp 1663516192055\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar at spark://LAPTOP-30N4Q54J:64515/jars/jackson-mapper-asl-1.9.13.jar with timestamp 1663516192056\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar at spark://LAPTOP-30N4Q54J:64515/jars/py4j-0.10.7.jar with timestamp 1663516192057\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar at spark://LAPTOP-30N4Q54J:64515/jars/curator-recipes-2.6.0.jar with timestamp 1663516192057\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.jar at spark://LAPTOP-30N4Q54J:64515/jars/jsr305-3.0.2.jar with timestamp 1663516192058\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/orc/orc-shims/1.5.5/orc-shims-1.5.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/orc-shims-1.5.5.jar with timestamp 1663516192058\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar at spark://LAPTOP-30N4Q54J:64515/jars/curator-client-2.6.0.jar with timestamp 1663516192058\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.jar at spark://LAPTOP-30N4Q54J:64515/jars/snappy-java-1.1.7.3.jar with timestamp 1663516192059\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar at spark://LAPTOP-30N4Q54J:64515/jars/validation-api-1.1.0.Final.jar with timestamp 1663516192059\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-hdfs/2.6.5/hadoop-hdfs-2.6.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/hadoop-hdfs-2.6.5.jar with timestamp 1663516192060\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar at spark://LAPTOP-30N4Q54J:64515/jars/ivy-2.4.0.jar with timestamp 1663516192060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar at spark://LAPTOP-30N4Q54J:64515/jars/stax-api-1.0-2.jar with timestamp 1663516192061\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar at spark://LAPTOP-30N4Q54J:64515/jars/RoaringBitmap-0.7.45.jar with timestamp 1663516192061\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar at spark://LAPTOP-30N4Q54J:64515/jars/jackson-jaxrs-1.9.13.jar with timestamp 1663516192062\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/io/dropwizard/metrics/metrics-json/3.1.5/metrics-json-3.1.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/metrics-json-3.1.5.jar with timestamp 1663516192063\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar at spark://LAPTOP-30N4Q54J:64515/jars/jersey-container-servlet-2.22.2.jar with timestamp 1663516192063\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar at spark://LAPTOP-30N4Q54J:64515/jars/jackson-xc-1.9.13.jar with timestamp 1663516192064\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar at spark://LAPTOP-30N4Q54J:64515/jars/javax.ws.rs-api-2.0.1.jar with timestamp 1663516192064\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/fasterxml/jackson/module/jackson-module-scala_2.12/2.6.7.1/jackson-module-scala_2.12-2.6.7.1.jar at spark://LAPTOP-30N4Q54J:64515/jars/jackson-module-scala_2.12-2.6.7.1.jar with timestamp 1663516192065\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/parquet/parquet-column/1.10.1/parquet-column-1.10.1.jar at spark://LAPTOP-30N4Q54J:64515/jars/parquet-column-1.10.1.jar with timestamp 1663516192066\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/esotericsoftware/kryo-shaded/4.0.2/kryo-shaded-4.0.2.jar at spark://LAPTOP-30N4Q54J:64515/jars/kryo-shaded-4.0.2.jar with timestamp 1663516192067\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar at spark://LAPTOP-30N4Q54J:64515/jars/api-asn1-api-1.0.0-M20.jar with timestamp 1663516192067\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar at spark://LAPTOP-30N4Q54J:64515/jars/pyrolite-4.13.jar with timestamp 1663516192069\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/parquet/parquet-format/2.4.0/parquet-format-2.4.0.jar at spark://LAPTOP-30N4Q54J:64515/jars/parquet-format-2.4.0.jar with timestamp 1663516192071\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.12/1.0.6/scala-xml_2.12-1.0.6.jar at spark://LAPTOP-30N4Q54J:64515/jars/scala-xml_2.12-1.0.6.jar with timestamp 1663516192073\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar at spark://LAPTOP-30N4Q54J:64515/jars/zookeeper-3.4.6.jar with timestamp 1663516192074\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/objenesis/objenesis/2.5.1/objenesis-2.5.1.jar at spark://LAPTOP-30N4Q54J:64515/jars/objenesis-2.5.1.jar with timestamp 1663516192075\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar at spark://LAPTOP-30N4Q54J:64515/jars/jersey-media-jaxb-2.22.2.jar with timestamp 1663516192076\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar at spark://LAPTOP-30N4Q54J:64515/jars/commons-httpclient-3.1.jar with timestamp 1663516192078\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.5/hadoop-mapreduce-client-jobclient-2.6.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/hadoop-mapreduce-client-jobclient-2.6.5.jar with timestamp 1663516192079\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/io/netty/netty-all/4.1.42.Final/netty-all-4.1.42.Final.jar at spark://LAPTOP-30N4Q54J:64515/jars/netty-all-4.1.42.Final.jar with timestamp 1663516192080\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-network-shuffle_2.12/2.4.5/spark-network-shuffle_2.12-2.4.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/spark-network-shuffle_2.12-2.4.5.jar with timestamp 1663516192080\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-sketch_2.12/2.4.5/spark-sketch_2.12-2.4.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/spark-sketch_2.12-2.4.5.jar with timestamp 1663516192081\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar at spark://LAPTOP-30N4Q54J:64515/jars/chill-java-0.9.3.jar with timestamp 1663516192082\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/json4s/json4s-ast_2.12/3.5.3/json4s-ast_2.12-3.5.3.jar at spark://LAPTOP-30N4Q54J:64515/jars/json4s-ast_2.12-3.5.3.jar with timestamp 1663516192083\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar at spark://LAPTOP-30N4Q54J:64515/jars/avro-mapred-1.8.2-hadoop2.jar with timestamp 1663516192084\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/parquet/parquet-encoding/1.10.1/parquet-encoding-1.10.1.jar at spark://LAPTOP-30N4Q54J:64515/jars/parquet-encoding-1.10.1.jar with timestamp 1663516192085\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/commons-net/commons-net/3.1/commons-net-3.1.jar at spark://LAPTOP-30N4Q54J:64515/jars/commons-net-3.1.jar with timestamp 1663516192086\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/io/dropwizard/metrics/metrics-jvm/3.1.5/metrics-jvm-3.1.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/metrics-jvm-3.1.5.jar with timestamp 1663516192087\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar at spark://LAPTOP-30N4Q54J:64515/jars/jersey-client-2.22.2.jar with timestamp 1663516192088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/arrow/arrow-memory/0.10.0/arrow-memory-0.10.0.jar at spark://LAPTOP-30N4Q54J:64515/jars/arrow-memory-0.10.0.jar with timestamp 1663516192092\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.5/hadoop-mapreduce-client-shuffle-2.6.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/hadoop-mapreduce-client-shuffle-2.6.5.jar with timestamp 1663516192093\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/fasterxml/jackson/module/jackson-module-paranamer/2.7.9/jackson-module-paranamer-2.7.9.jar at spark://LAPTOP-30N4Q54J:64515/jars/jackson-module-paranamer-2.7.9.jar with timestamp 1663516192095\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.5/hadoop-mapreduce-client-app-2.6.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/hadoop-mapreduce-client-app-2.6.5.jar with timestamp 1663516192096\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar at spark://LAPTOP-30N4Q54J:64515/jars/htrace-core-3.0.4.jar with timestamp 1663516192096\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar at spark://LAPTOP-30N4Q54J:64515/jars/jersey-guava-2.22.2.jar with timestamp 1663516192097\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/commons-digester/commons-digester/1.8/commons-digester-1.8.jar at spark://LAPTOP-30N4Q54J:64515/jars/commons-digester-1.8.jar with timestamp 1663516192098\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-launcher_2.12/2.4.5/spark-launcher_2.12-2.4.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/spark-launcher_2.12-2.4.5.jar with timestamp 1663516192099\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/twitter/chill_2.12/0.9.3/chill_2.12-0.9.3.jar at spark://LAPTOP-30N4Q54J:64515/jars/chill_2.12-0.9.3.jar with timestamp 1663516192099\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/arrow/arrow-format/0.10.0/arrow-format-0.10.0.jar at spark://LAPTOP-30N4Q54J:64515/jars/arrow-format-0.10.0.jar with timestamp 1663516192100\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar at spark://LAPTOP-30N4Q54J:64515/jars/protobuf-java-2.5.0.jar with timestamp 1663516192101\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar at spark://LAPTOP-30N4Q54J:64515/jars/apacheds-kerberos-codec-2.0.0-M15.jar with timestamp 1663516192102\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/hadoop-client-2.6.5.jar with timestamp 1663516192103\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/metrics-graphite-3.1.5.jar with timestamp 1663516192103\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/json4s/json4s-jackson_2.12/3.5.3/json4s-jackson_2.12-3.5.3.jar at spark://LAPTOP-30N4Q54J:64515/jars/json4s-jackson_2.12-3.5.3.jar with timestamp 1663516192104\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar at spark://LAPTOP-30N4Q54J:64515/jars/javax.inject-2.4.0-b34.jar with timestamp 1663516192105\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar at spark://LAPTOP-30N4Q54J:64515/jars/jersey-server-2.22.2.jar with timestamp 1663516192105\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.6.7/jackson-annotations-2.6.7.jar at spark://LAPTOP-30N4Q54J:64515/jars/jackson-annotations-2.6.7.jar with timestamp 1663516192106\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.5/hadoop-mapreduce-client-common-2.6.5.jar at spark://LAPTOP-30N4Q54J:64515/jars/hadoop-mapreduce-client-common-2.6.5.jar with timestamp 1663516192107\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar at spark://LAPTOP-30N4Q54J:64515/jars/curator-framework-2.6.0.jar with timestamp 1663516192108\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/sh/almond/almond-spark_2.12/0.4.0/almond-spark_2.12-0.4.0.jar at spark://LAPTOP-30N4Q54J:64515/jars/almond-spark_2.12-0.4.0.jar with timestamp 1663516192109\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/io/argonaut/argonaut_2.12/6.2.3/argonaut_2.12-6.2.3.jar at spark://LAPTOP-30N4Q54J:64515/jars/argonaut_2.12-6.2.3.jar with timestamp 1663516192110\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/jetty-server/9.4.15.v20190215/jetty-server-9.4.15.v20190215.jar at spark://LAPTOP-30N4Q54J:64515/jars/jetty-server-9.4.15.v20190215.jar with timestamp 1663516192122\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/sh/almond/ammonite-spark_2.12/0.4.0/ammonite-spark_2.12-0.4.0.jar at spark://LAPTOP-30N4Q54J:64515/jars/ammonite-spark_2.12-0.4.0.jar with timestamp 1663516192123\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/github/alexarchambault/argonaut-shapeless_6.2_2.12/1.2.0-M10/argonaut-shapeless_6.2_2.12-1.2.0-M10.jar at spark://LAPTOP-30N4Q54J:64515/jars/argonaut-shapeless_6.2_2.12-1.2.0-M10.jar with timestamp 1663516192124\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/jetty-io/9.4.15.v20190215/jetty-io-9.4.15.v20190215.jar at spark://LAPTOP-30N4Q54J:64515/jars/jetty-io-9.4.15.v20190215.jar with timestamp 1663516192125\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/typelevel/macro-compat_2.12/1.1.1/macro-compat_2.12-1.1.1.jar at spark://LAPTOP-30N4Q54J:64515/jars/macro-compat_2.12-1.1.1.jar with timestamp 1663516192125\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/jetty-http/9.4.15.v20190215/jetty-http-9.4.15.v20190215.jar at spark://LAPTOP-30N4Q54J:64515/jars/jetty-http-9.4.15.v20190215.jar with timestamp 1663516192127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/jetty-util/9.4.15.v20190215/jetty-util-9.4.15.v20190215.jar at spark://LAPTOP-30N4Q54J:64515/jars/jetty-util-9.4.15.v20190215.jar with timestamp 1663516192128\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/chuusai/shapeless_2.12/2.3.3/shapeless_2.12-2.3.3.jar at spark://LAPTOP-30N4Q54J:64515/jars/shapeless_2.12-2.3.3.jar with timestamp 1663516192129\n",
      "22/09/18 17:49:52 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/sh/almond/spark-stubs_24_2.12/0.4.0/spark-stubs_24_2.12-0.4.0.jar at spark://LAPTOP-30N4Q54J:64515/jars/spark-stubs_24_2.12-0.4.0.jar with timestamp 1663516192131\n",
      "22/09/18 17:49:52 INFO Executor: Starting executor ID driver on host localhost\n",
      "22/09/18 17:49:52 INFO Executor: Using REPL class URI: http://192.168.56.1:64472\n",
      "22/09/18 17:49:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 64558.\n",
      "22/09/18 17:49:52 INFO NettyBlockTransferService: Server created on LAPTOP-30N4Q54J:64558\n",
      "22/09/18 17:49:52 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "22/09/18 17:49:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, LAPTOP-30N4Q54J, 64558, None)\n",
      "22/09/18 17:49:52 INFO BlockManagerMasterEndpoint: Registering block manager LAPTOP-30N4Q54J:64558 with 1646.4 MB RAM, BlockManagerId(driver, LAPTOP-30N4Q54J, 64558, None)\n",
      "22/09/18 17:49:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, LAPTOP-30N4Q54J, 64558, None)\n",
      "22/09/18 17:49:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, LAPTOP-30N4Q54J, 64558, None)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://LAPTOP-30N4Q54J:4040\">Spark UI</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                              \n",
       "\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{NotebookSparkSession, SparkSession}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{functions => func, _}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\n",
       "\u001b[39m\r\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@35ced0e2\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.slf4j.LoggerFactory\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.5` \n",
    "import $ivy.`sh.almond::almond-spark:0.4.0`\n",
    "\n",
    "import org.apache.spark.sql.{NotebookSparkSession, SparkSession}\n",
    "import org.apache.spark.sql.{functions => func, _}\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val spark = NotebookSparkSession\n",
    "      .builder()\n",
    "      .config(\"spark.sql.join.preferSortMergeJoin\", false)\n",
    "      .config(\"spark.sql.shuffle.partitions\", 64)\n",
    "      .master(\"local[*]\")\n",
    "      .getOrCreate()\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "import org.slf4j.LoggerFactory\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "\n",
    "Logger.getRootLogger().setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf06962",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"json\").load(\"D:/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\").schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0920ad8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\n",
    "import org.apache.spark.sql.types.Metadata\n",
    "\n",
    "val myManualSchema = StructType(Array(\n",
    "  StructField(\"DEST_COUNTRY_NAME\", StringType, true),\n",
    "  StructField(\"ORIGIN_COUNTRY_NAME\", StringType, true),\n",
    "  StructField(\"count\", LongType, false,\n",
    "    Metadata.fromJson(\"{\\\"hello\\\":\\\"world\\\"}\"))\n",
    "))\n",
    "\n",
    "val df = spark.read.format(\"json\").schema(myManualSchema)\n",
    "  .load(\"D:/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3d5ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.orderBy($\"count\".asc).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e18606",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.col(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9589ec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.expr\n",
    "expr(\"(((someCol + 5) * 200) - 6) < otherCol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32af3a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41084da",
   "metadata": {},
   "outputs": [],
   "source": [
    "//import org.apache.spark.sql.Row\n",
    "val myRow = Row(\"Hello\", null, 1, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053b3075",
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.read.format(\"json\")\n",
    "  .load(\"D:/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b2e492",
   "metadata": {},
   "outputs": [],
   "source": [
    "val myManualSchema = new StructType(Array(\n",
    "  new StructField(\"some\", StringType, true),\n",
    "  new StructField(\"col\", StringType, true),\n",
    "  new StructField(\"names\", LongType, false)))\n",
    "val myRows = Seq(Row(\"Hello\", null, 1L))\n",
    "val myRDD = spark.sparkContext.parallelize(myRows)\n",
    "val myDf = spark.createDataFrame(myRDD, myManualSchema)\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c23d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"DEST_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d452d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(expr(\"DEST_COUNTRY_NAME as destination\").alias(\"DEST_COUNTRY_NAME\"))\n",
    "  .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337073f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.selectExpr(\n",
    "    \"*\", // include all original columns\n",
    "    \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\")\n",
    "  .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053b8e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.lit\n",
    "df.select(expr(\"*\"), lit(1).as(\"One\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110ff453",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"numberOne\", lit(1)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01135c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\"))\n",
    "  .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7686d754",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"count2\", $\"count\".cast(\"long\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329b3384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{col, column}\n",
    "//df.filter(col(\"count\") < 2).show(2)\n",
    "df.filter($\"count\" < 2).show(2)\n",
    "//df.where(\"count < 2\").show(2)\n",
    "df.where($\"count\" < 2).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813ce38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb736ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "val seed = 5\n",
    "val withReplacement = false\n",
    "val fraction = 0.5\n",
    "df.sample(withReplacement, fraction, seed).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f598f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val schema = df.schema\n",
    "val newRows = Seq(\n",
    "  Row(\"New Country\", \"Other Country\", 5L),\n",
    "  Row(\"New Country 2\", \"Other Country 3\", 1L)\n",
    ")\n",
    "val parallelizedRows = spark.sparkContext.parallelize(newRows)\n",
    "val newDF = spark.createDataFrame(parallelizedRows, schema)\n",
    "newDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c8b8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.union(newDF)\n",
    "  .where(\"count = 1\")\n",
    "  .where($\"ORIGIN_COUNTRY_NAME\" =!= \"United States\")\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761f1c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6e0ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9264a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val collectDF = df.limit(10)\n",
    "collectDF.take(5) // take works with an Integer count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6c0205",
   "metadata": {},
   "outputs": [],
   "source": [
    "collectDF.show() // this prints it out nicely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed23f0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "collectDF.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8725f89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "collectDF.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03360fda",
   "metadata": {},
   "source": [
    "## Capitulo 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa15f192",
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load(\"D:/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2010-12-01.csv\")\n",
    "df.printSchema()\n",
    "df.createOrReplaceTempView(\"dfTable\")    //creará una vista temporal de la tabla en la memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8e8945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.lit\n",
    "df.select(lit(5), lit(\"five\"), lit(5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011e25c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(col(\"InvoiceNo\").equalTo(536365))\n",
    "  .select(\"InvoiceNo\", \"Description\")\n",
    "  .show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fd5d3d",
   "metadata": {},
   "source": [
    "Lo mismo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f637cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.where(\"InvoiceNo = 536365\")\n",
    "  .show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a7f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val priceFilter = col(\"UnitPrice\") > 600\n",
    "val descripFilter = col(\"Description\").contains(\"POSTAGE\")\n",
    "df.where(col(\"StockCode\").isin(\"DOT\")).where(priceFilter.or(descripFilter))\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d705715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val DOTCodeFilter = col(\"StockCode\") === \"DOT\"\n",
    "val priceFilter = col(\"UnitPrice\") > 600\n",
    "val descripFilter = col(\"Description\").contains(\"POSTAGE\")\n",
    "df.withColumn(\"isExpensive\", DOTCodeFilter.and(priceFilter.or(descripFilter)))\n",
    "  .where(\"isExpensive\")\n",
    "  //.filter(\"isExpensive\")\n",
    "  .select(\"unitPrice\", \"isExpensive\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2010ade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{expr, not, col}\n",
    "df.withColumn(\"isExpensive\", not(col(\"UnitPrice\").leq(250)))\n",
    "  .filter(\"isExpensive\")\n",
    "  .select(\"Description\", \"UnitPrice\").show(5)\n",
    "df.withColumn(\"isExpensive\", expr(\"NOT UnitPrice <= 250\"))\n",
    "  .filter(\"isExpensive\")\n",
    "  .select(\"Description\", \"UnitPrice\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e71b0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(col(\"Description\").eqNullSafe(\"CustumerID\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371dc60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val fabricatedQuantity = func.pow(col(\"Quantity\") * col(\"UnitPrice\"), 2) + 5\n",
    "df.select(expr(\"CustomerId\"), fabricatedQuantity.alias(\"realQuantity\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952ce00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "//Como SQL\n",
    "df.selectExpr(\n",
    "  \"CustomerId\",\n",
    "  \"(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f801748",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(func.round(col(\"UnitPrice\"), 1).alias(\"rounded\"), col(\"UnitPrice\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0c61c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(func.round(lit(\"2.5\")), func.bround(lit(\"2.5\"))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fd50eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.stat.corr(\"Quantity\", \"UnitPrice\")\n",
    "df.select(func.corr(\"Quantity\", \"UnitPrice\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc8849c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afdd37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(func.monotonically_increasing_id()).show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bff2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{initcap}\n",
    "df.select(initcap(col(\"Description\"))).show(2, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcff2f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{lit, ltrim, rtrim, rpad, lpad, trim}\n",
    "df.select(\n",
    "    ltrim(lit(\"    HELLO    \")).as(\"ltrim\"),          //Elimina los espacios en blanco de la izquierda\n",
    "    rtrim(lit(\"    HELLO    \")).as(\"rtrim\"),          //Elimina los espacios en blanco de la derecha\n",
    "    trim(lit(\"    HELLO    \")).as(\"trim\"),            //Elimina los espacios en blanco tanto de derecha como izquierda\n",
    "    lpad(lit(\"HELLO\"), 7, \" \").as(\"lp\"),              //Se queda con el numero de posiciones indicadas añadiendo espacios por izq\n",
    "    rpad(lit(\"HELLO\"), 10, \" \").as(\"rp\")).show(2)     //Se queda con el numero de posiciones indicadas añadiendo espacios por der"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe65f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.regexp_replace\n",
    "val simpleColors = Seq(\"black\", \"white\", \"red\", \"green\", \"blue\")\n",
    "val regexString = simpleColors.map(_.toUpperCase).mkString(\"|\")\n",
    "// the | signifies `OR` in regular expression syntax\n",
    "df.select(\n",
    "  regexp_replace(col(\"Description\"), regexString, \"COLOR\").alias(\"color_clean\"),\n",
    "  col(\"Description\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3e3733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.translate\n",
    "df.select(translate(col(\"Description\"), \"LEET\", \"1337\"), col(\"Description\"))\n",
    "  .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bbb134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.regexp_extract\n",
    "val regexString = simpleColors.map(_.toUpperCase).mkString(\"(\", \"|\", \")\")\n",
    "// the | signifies OR in regular expression syntax\n",
    "df.select(\n",
    "     regexp_extract(col(\"Description\"), regexString, 1).alias(\"color_clean\"),\n",
    "     col(\"Description\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3abf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "val simpleColors = Seq(\"black\", \"white\", \"red\", \"green\", \"blue\")\n",
    "val selectedColumns = simpleColors.map(color => {\n",
    "   col(\"Description\").contains(color.toUpperCase).alias(s\"is_$color\")\n",
    "}):+expr(\"*\") // could also append this value\n",
    "df.select(selectedColumns:_*).where(col(\"is_white\").or(col(\"is_red\")))\n",
    "  .select(\"Description\").show(3, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f22bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{current_date, current_timestamp}\n",
    "val dateDF = spark.range(10)\n",
    "  .withColumn(\"today\", current_date())\n",
    "  .withColumn(\"now\", current_timestamp())\n",
    "dateDF.createOrReplaceTempView(\"dateTable\")\n",
    "dateDF.show()\n",
    "dateDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2b83ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{date_add, date_sub}\n",
    "dateDF.select(date_sub(col(\"today\"), 5), date_add(col(\"today\"), 5)).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58d096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{datediff, months_between, to_date}\n",
    "dateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\n",
    "  .select(datediff(col(\"week_ago\"), col(\"today\"))).show(1)\n",
    "dateDF.select(\n",
    "    to_date(lit(\"2016-01-01\")).alias(\"start\"),\n",
    "    to_date(lit(\"2017-05-22\")).alias(\"end\"))\n",
    "  .select(months_between(col(\"start\"), col(\"end\"))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69bb441",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(5).withColumn(\"date\", lit(\"2017-01-01\"))\n",
    "  .select(to_date(col(\"date\"))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc9451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.to_date\n",
    "val dateFormat = \"yyyy-dd-MM\"\n",
    "val cleanDateDF = spark.range(1).select(\n",
    "    to_date(lit(\"2017-12-11\"), dateFormat).alias(\"date\"),\n",
    "    to_date(lit(\"2017-20-12\"), dateFormat).alias(\"date2\"))\n",
    "cleanDateDF.createOrReplaceTempView(\"dateTable2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68364444",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanDateDF.filter(col(\"date2\") > \"'2017-12-12'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af796f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.coalesce\n",
    "df.select(coalesce(col(\"Description\"), col(\"CustomerId\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c66a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.fill(\"All Null values become this string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ac8fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a7b1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.fill(5, Seq(\"StockCode\", \"InvoiceNo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f426be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val fillColValues = Map(\"StockCode\" -> 5, \"Description\" -> \"No Value\")\n",
    "df.na.fill(fillColValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9144b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.selectExpr(\"(Description, InvoiceNo) as complex\", \"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5def6fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.struct\n",
    "val complexDF = df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))\n",
    "complexDF.createOrReplaceTempView(\"complexDF\")\n",
    "complexDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70642137",
   "metadata": {},
   "outputs": [],
   "source": [
    "complexDF.select(\"complex.Description\").show()\n",
    "complexDF.select(col(\"complex\").getField(\"InvoiceNo\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27800dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.split\n",
    "df.select(split(col(\"Description\"), \" \").alias(\"array_col\"))\n",
    "  .selectExpr(\"array_col[0]\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5013652b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.array_contains\n",
    "df.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d30151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.map\n",
    "df.select(map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2f05de",
   "metadata": {},
   "outputs": [],
   "source": [
    "val jsonDF = spark.range(1).selectExpr(\"\"\"\n",
    "  '{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}' as jsonString\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29d62b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca44439a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{get_json_object, json_tuple}\n",
    "jsonDF.select(\n",
    "    get_json_object(col(\"jsonString\"), \"$.myJSONKey.myJSONValue[1]\") as \"column\",\n",
    "    json_tuple(col(\"jsonString\"), \"myJSONKey\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7944f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.to_json\n",
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\")\n",
    "  .select(to_json(col(\"myStruct\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88f960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val udfExampleDF = spark.range(5).toDF(\"num\")\n",
    "def power3(number:Double):Double = number * number * number\n",
    "power3(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957a6d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.udf\n",
    "val power3udf = udf(power3(_:Double):Double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab57fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "udfExampleDF.select(power3udf(col(\"num\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e09a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.udf.register(\"power3\", power3(_:Double):Double)\n",
    "udfExampleDF.selectExpr(\"power3(num)\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f766d53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val df = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load(\"D:/Spark-The-Definitive-Guide-master/data/retail-data/all/*.csv\")\n",
    "  .coalesce(5)\n",
    "df.cache()\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a75508",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bec902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1046bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.countDistinct\n",
    "df.select(countDistinct(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919d0ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{sum, count, avg, expr}\n",
    "\n",
    "df.select(\n",
    "    count(\"Quantity\").alias(\"total_transactions\"),\n",
    "    sum(\"Quantity\").alias(\"total_purchases\"),\n",
    "    avg(\"Quantity\").alias(\"avg_purchases\"),\n",
    "    expr(\"mean(Quantity)\").alias(\"mean_purchases\"))\n",
    "  .selectExpr(\n",
    "    \"total_purchases/total_transactions\",\n",
    "    \"avg_purchases\",\n",
    "    \"mean_purchases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47408188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{var_pop, stddev_pop}\n",
    "import org.apache.spark.sql.functions.{var_samp, stddev_samp}\n",
    "df.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"),\n",
    "  stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f383e0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{skewness, kurtosis}\n",
    "df.select(skewness(\"Quantity\"), kurtosis(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab43c8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{collect_set, collect_list}\n",
    "df.agg(collect_set(\"Country\"), collect_list(\"Country\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec426689",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fba8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.count\n",
    "\n",
    "df.groupBy(\"InvoiceNo\").agg(\n",
    "  count(\"Quantity\").alias(\"quan\"),\n",
    "  expr(\"count(Quantity)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b05eb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "// in Scala\n",
    "df.groupBy(\"InvoiceNo\").agg(\"Quantity\"->\"avg\", \"Quantity\"->\"stddev_pop\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dbaea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{col, to_date}\n",
    "val dfWithDate = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"),\n",
    "  \"MM/d/yyyy H:mm\"))\n",
    "dfWithDate.createOrReplaceTempView(\"dfWithDate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f508b16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWithDate.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50790d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.functions.col\n",
    "val windowSpec = Window\n",
    "  .partitionBy(\"CustomerId\", \"date\")\n",
    "  .orderBy(col(\"Quantity\").desc)\n",
    "  .rowsBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1d5ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.max\n",
    "val maxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86192d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{dense_rank, rank}\n",
    "val purchaseDenseRank = dense_rank().over(windowSpec)\n",
    "val purchaseRank = rank().over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52145ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "dfWithDate.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\")\n",
    "  .select(\n",
    "    col(\"CustomerId\"),\n",
    "    col(\"date\"),\n",
    "    col(\"Quantity\"),\n",
    "    purchaseRank.alias(\"quantityRank\"),\n",
    "    purchaseDenseRank.alias(\"quantityDenseRank\"),\n",
    "    maxPurchaseQuantity.alias(\"maxPurchaseQuantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc917665",
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfNoNull = dfWithDate.drop()\n",
    "dfNoNull.createOrReplaceTempView(\"dfNoNull\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0023cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "val rolledUpDF = dfNoNull.rollup(\"Date\", \"Country\").agg(sum(\"Quantity\"))\n",
    "  .selectExpr(\"Date\", \"Country\", \"`sum(Quantity)` as total_quantity\")\n",
    "  .orderBy(\"Date\")\n",
    "rolledUpDF.show()\n",
    "\n",
    "//donde vea los valores null es donde encontrará los totales generales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94902063",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolledUpDF.where(\"Country IS NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618a46fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNoNull.cube(\"Date\", \"Country\").agg(sum($\"Quantity\"))\n",
    "  .select(\"Date\", \"Country\", \"sum(Quantity)\").orderBy(\"Date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbb393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{grouping_id, sum, expr}\n",
    "\n",
    "dfNoNull.cube(\"customerId\", \"stockCode\").agg(grouping_id(), sum(\"Quantity\"))\n",
    ".orderBy(expr(\"grouping_id()\").desc)\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acd4f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "val pivoted = dfWithDate.groupBy(\"date\").pivot(\"Country\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f395de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.expressions.MutableAggregationBuffer\n",
    "import org.apache.spark.sql.expressions.UserDefinedAggregateFunction\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types._\n",
    "class BoolAnd extends UserDefinedAggregateFunction {\n",
    "  def inputSchema: org.apache.spark.sql.types.StructType =\n",
    "    StructType(StructField(\"value\", BooleanType) :: Nil)\n",
    "  def bufferSchema: StructType = StructType(\n",
    "    StructField(\"result\", BooleanType) :: Nil\n",
    "  )\n",
    "  def dataType: DataType = BooleanType\n",
    "  def deterministic: Boolean = true\n",
    "  def initialize(buffer: MutableAggregationBuffer): Unit = {\n",
    "    buffer(0) = true\n",
    "  }\n",
    "  def update(buffer: MutableAggregationBuffer, input: Row): Unit = {\n",
    "    buffer(0) = buffer.getAs[Boolean](0) && input.getAs[Boolean](0)\n",
    "  }\n",
    "  def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {\n",
    "    buffer1(0) = buffer1.getAs[Boolean](0) && buffer2.getAs[Boolean](0)\n",
    "  }\n",
    "  def evaluate(buffer: Row): Any = {\n",
    "    buffer(0)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28f457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val ba = new BoolAnd\n",
    "spark.udf.register(\"booland\", ba)\n",
    "import org.apache.spark.sql.functions._\n",
    "spark.range(1)\n",
    "  .selectExpr(\"explode(array(TRUE, TRUE, TRUE)) as t\")\n",
    "  .selectExpr(\"explode(array(TRUE, FALSE, TRUE)) as f\", \"t\")\n",
    "  .select(ba(col(\"t\")), expr(\"booland(f)\"))\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d991e7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "val person = Seq(\n",
    "    (0, \"Bill Chambers\", 0, Seq(100)),\n",
    "    (1, \"Matei Zaharia\", 1, Seq(500, 250, 100)),\n",
    "    (2, \"Michael Armbrust\", 1, Seq(250, 100)))\n",
    "  .toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")\n",
    "val graduateProgram = Seq(\n",
    "    (0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n",
    "    (2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n",
    "    (1, \"Ph.D.\", \"EECS\", \"UC Berkeley\"))\n",
    "  .toDF(\"id\", \"degree\", \"department\", \"school\")\n",
    "val sparkStatus = Seq(\n",
    "    (500, \"Vice President\"),\n",
    "    (250, \"PMC Member\"),\n",
    "    (100, \"Contributor\"))\n",
    "  .toDF(\"id\", \"status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e576b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "person.createOrReplaceTempView(\"person\")\n",
    "graduateProgram.createOrReplaceTempView(\"graduateProgram\")\n",
    "sparkStatus.createOrReplaceTempView(\"sparkStatus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ccff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "val joinExpression = person.col(\"graduate_program\") === graduateProgram.col(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850e2215",
   "metadata": {},
   "outputs": [],
   "source": [
    "val wrongJoinExpression = person.col(\"name\") === graduateProgram.col(\"school\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f870edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "person.join(graduateProgram, joinExpression).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c836eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "val joinType = \"outer\"\n",
    "person.join(graduateProgram, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a2ed27",
   "metadata": {},
   "outputs": [],
   "source": [
    "val joinType = \"left_outer\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06295d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val joinType = \"right_outer\"\n",
    "person.join(graduateProgram, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c7d65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val joinType = \"left_semi\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1095d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val joinType = \"cross\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d56b98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "person.crossJoin(graduateProgram).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e5caf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.expr\n",
    "\n",
    "person.withColumnRenamed(\"id\", \"personId\")\n",
    "  .join(sparkStatus, expr(\"array_contains(spark_status, id)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98838f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "val gradProgramDupe = graduateProgram.withColumnRenamed(\"id\", \"graduate_program\")\n",
    "val joinExpr = gradProgramDupe.col(\"graduate_program\") === person.col(\n",
    "  \"graduate_program\")\n",
    "gradProgramDupe.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9d1359",
   "metadata": {},
   "outputs": [],
   "source": [
    "person.join(gradProgramDupe, joinExpr).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430c69b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "person.join(gradProgramDupe,\"graduate_program\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43cc24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "person.join(gradProgramDupe, joinExpr).drop(person.col(\"graduate_program\"))\n",
    "  .select(\"graduate_program\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cdc518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.broadcast\n",
    "val joinExpr = person.col(\"graduate_program\") === graduateProgram.col(\"id\")\n",
    "person.join(broadcast(graduateProgram), joinExpr).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f90a08e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\n",
    "val myManualSchema = new StructType(Array(\n",
    "  new StructField(\"DEST_COUNTRY_NAME\", StringType, true),\n",
    "  new StructField(\"ORIGIN_COUNTRY_NAME\", StringType, true),\n",
    "  new StructField(\"count\", LongType, false)\n",
    "))\n",
    "spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"mode\", \"FAILFAST\")\n",
    "  .schema(myManualSchema)\n",
    "  .load(\"D:/Spark-The-Definitive-Guide-master/data/flight-data/csv/2010-summary.csv\")\n",
    "  .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eec26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val csvFile = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\").option(\"mode\", \"FAILFAST\").schema(myManualSchema)\n",
    "  .load(\"D:/Spark-The-Definitive-Guide-master/data/flight-data/csv/2010-summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4ce449",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \"\\t\")\n",
    "  .save(\"D:/Spark-The-Definitive-Guide-master/tmp/my-tsv-file.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcad77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.write.format(\"json\").mode(\"overwrite\").save(\"D:/Spark-The-Definitive-Guide-master/tmp/my-json-file.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393f5cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"parquet\")\n",
    "  .load(\"D:/Spark-The-Definitive-Guide-master/data/flight-data/parquet/2010-summary.parquet\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65a998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.write.format(\"parquet\").mode(\"overwrite\")\n",
    "  .save(\"D:/Spark-The-Definitive-Guide-master/tmp/my-parquet-file.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c80c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.textFile(\"D:/Spark-The-Definitive-Guide-master/data/flight-data/csv/2010-summary.csv\")\n",
    "  .selectExpr(\"split(value, ',') as rows\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0628676",
   "metadata": {},
   "outputs": [],
   "source": [
    "//csvFile.select(\"DEST_COUNTRY_NAME\").write.text(\"D:/Spark-The-Definitive-Guide-master/tmp/simple-text-file.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521d9a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "//csvFile.repartition(5).write.format(\"csv\").save(\"D:/Spark-The-Definitive-Guide-master/tmp/multiple.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c5227b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csvFile.limit(10).write.mode(\"overwrite\").partitionBy(\"DEST_COUNTRY_NAME\")\n",
    "  .save(\"D:/Spark-The-Definitive-Guide-master/tmp/partitioned-files.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804a3ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val numberBuckets = 10\n",
    "val columnToBucketBy = \"count\"\n",
    "\n",
    "//csvFile.write.format(\"parquet\").mode(\"overwrite\")\n",
    "  //.bucketBy(numberBuckets, columnToBucketBy).saveAsTable(\"bucketedFiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfec363",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT 1 + 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e79a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.json(\"D:/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")\n",
    "  .createOrReplaceTempView(\"some_sql_view\") // DF => SQL\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count)\n",
    "FROM some_sql_view GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\")\n",
    "  .where(\"DEST_COUNTRY_NAME like 'S%'\").where(\"`sum(count)` > 10\")\n",
    "  .count() // SQL => DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bac0f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618a0e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(500).rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe14bdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(10).toDF.rdd.map(rowObject => rowObject.getLong(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89834993",
   "metadata": {},
   "outputs": [],
   "source": [
    "val myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\"\n",
    "  .split(\" \")\n",
    "val words = spark.sparkContext.parallelize(myCollection, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb245749",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.setName(\"myWords\")\n",
    "words.name // myWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5a624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.textFile(\"D:/Spark-The-Definitive-Guide-master/some/path/withTextFiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2c6236",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.wholeTextFiles(\"D:/Spark-The-Definitive-Guide-master/some/path/withTextFiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdaa684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def startsWithS(individual:String) = {\n",
    "  individual.startsWith(\"S\")\n",
    "}\n",
    "words.filter(word => startsWithS(word)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a827db7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val words2 = words.map(word => (word, word(0), word.startsWith(\"S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2feaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "words2.filter(record => record._3).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a002d2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.sortBy(word => word.length() * -1).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560ac81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val fiftyFiftySplit = words.randomSplit(Array[Double](0.5, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcb6c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.parallelize(1 to 20).reduce(_ + _) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde8261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordLengthReducer(leftWord:String, rightWord:String): String = {\n",
    "  if (leftWord.length > rightWord.length)\n",
    "    return leftWord\n",
    "  else\n",
    "    return rightWord\n",
    "}\n",
    "\n",
    "words.reduce(wordLengthReducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161e18a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val confidence = 0.95\n",
    "val timeoutMilliseconds = 400\n",
    "words.countApprox(timeoutMilliseconds, confidence)\n",
    "\n",
    "words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dfc1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.countApproxDistinct(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f0ba1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b807bc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2bda65",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.take(5)\n",
    "words.takeOrdered(5)\n",
    "words.top(5)\n",
    "val withReplacement = true\n",
    "val numberToTake = 6\n",
    "val randomSeed = 100L\n",
    "words.takeSample(withReplacement, numberToTake, randomSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684add61",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.cache()\n",
    "words.getStorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809e1f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexedFunc(partitionIndex:Int, withinPartIterator: Iterator[String]) = {\n",
    "  withinPartIterator.toList.map(\n",
    "    value => s\"Partition: $partitionIndex => $value\").iterator\n",
    "}\n",
    "words.mapPartitionsWithIndex(indexedFunc).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc80ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\"\n",
    "  .split(\" \")\n",
    "val words = spark.sparkContext.parallelize(myCollection, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ab2ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.map(word => (word.toLowerCase, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4771f170",
   "metadata": {},
   "outputs": [],
   "source": [
    "val keyword = words.keyBy(word => word.toLowerCase.toSeq(0).toString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e655acc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword.mapValues(word => word.toUpperCase).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2cb0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword.flatMapValues(word => word.toUpperCase).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59384d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword.keys.collect()\n",
    "keyword.values.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e55dafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword.lookup(\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af0ee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "val distinctChars = words.flatMap(word => word.toLowerCase.toSeq).distinct\n",
    "  .collect()\n",
    "import scala.util.Random\n",
    "val sampleMap = distinctChars.map(c => (c, new Random().nextDouble())).toMap\n",
    "words.map(word => (word.toLowerCase.toSeq(0), word))\n",
    "  .sampleByKey(true, sampleMap, 6L)\n",
    "  .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf72078",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.map(word => (word.toLowerCase.toSeq(0), word))\n",
    "  .sampleByKeyExact(true, sampleMap, 6L).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab439df",
   "metadata": {},
   "outputs": [],
   "source": [
    "val chars = words.flatMap(word => word.toLowerCase.toSeq)\n",
    "val KVcharacters = chars.map(letter => (letter, 1))\n",
    "def maxFunc(left:Int, right:Int) = math.max(left, right)\n",
    "def addFunc(left:Int, right:Int) = left + right\n",
    "//val nums = sc.parallelize(1 to 30, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b86b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "// in Scala\n",
    "val timeout = 1000L //milliseconds\n",
    "val confidence = 0.95\n",
    "KVcharacters.countByKey()\n",
    "KVcharacters.countByKeyApprox(timeout, confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79f061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "KVcharacters.groupByKey().map(row => (row._1, row._2.reduce(addFunc))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8856f0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "KVcharacters.reduceByKey(addFunc).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55604d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "KVcharacters.aggregateByKey(0)(addFunc, maxFunc).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a096ddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val valToCombiner = (value:Int) => List(value)\n",
    "val mergeValuesFunc = (vals:List[Int], valToAppend:Int) => valToAppend :: vals\n",
    "val mergeCombinerFunc = (vals1:List[Int], vals2:List[Int]) => vals1 ::: vals2\n",
    "// now we define these as function variables\n",
    "val outputPartitions = 6\n",
    "KVcharacters\n",
    "  .combineByKey(\n",
    "    valToCombiner,\n",
    "    mergeValuesFunc,\n",
    "    mergeCombinerFunc,\n",
    "    outputPartitions)\n",
    "  .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729d7470",
   "metadata": {},
   "outputs": [],
   "source": [
    "KVcharacters.foldByKey(0)(addFunc).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccf5be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.util.Random\n",
    "val distinctChars = words.flatMap(word => word.toLowerCase.toSeq).distinct\n",
    "val charRDD = distinctChars.map(c => (c, new Random().nextDouble()))\n",
    "val charRDD2 = distinctChars.map(c => (c, new Random().nextDouble()))\n",
    "val charRDD3 = distinctChars.map(c => (c, new Random().nextDouble()))\n",
    "charRDD.cogroup(charRDD2, charRDD3).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a11af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val keyedChars = distinctChars.map(c => (c, new Random().nextDouble()))\n",
    "val outputPartitions = 10\n",
    "KVcharacters.join(keyedChars).count()\n",
    "KVcharacters.join(keyedChars, outputPartitions).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb5dfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d51e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "val myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\"\n",
    "  .split(\" \")\n",
    "val words = spark.sparkContext.parallelize(myCollection, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a44353",
   "metadata": {},
   "outputs": [],
   "source": [
    "val supplementalData = Map(\"Spark\" -> 1000, \"Definitive\" -> 200,\n",
    "                           \"Big\" -> -300, \"Simple\" -> 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7c795c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val suppBroadcast = spark.sparkContext.broadcast(supplementalData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a7add2",
   "metadata": {},
   "outputs": [],
   "source": [
    "suppBroadcast.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a74e8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.map(word => (word, suppBroadcast.value.getOrElse(word, 0)))\n",
    "  .sortBy(wordPair => wordPair._2)\n",
    "  .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee4851da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "var comm = Jupyter.notebook.kernel.comm_manager.new_comm('cancel-stage-a349d9fd-efe1-4cdf-9660-2f29326e6a59', {});\n",
       "\n",
       "function cancelStage(stageId) {\n",
       "  console.log('Cancelling stage ' + stageId);\n",
       "  comm.send({ 'stageId': stageId });\n",
       "}\n",
       "</script>\n",
       "          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">parquet at cmd1.sc:5</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mFlight\u001b[39m\r\n",
       "\u001b[36mflights\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mFlight\u001b[39m] = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Flight(DEST_COUNTRY_NAME: String,\n",
    "                  ORIGIN_COUNTRY_NAME: String, count: BigInt)\n",
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "val flights = spark.read\n",
    "  .parquet(\"D:/Spark-The-Definitive-Guide-master/data/flight-data/parquet/2010-summary.parquet\")\n",
    "  .as[Flight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f7fc807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.util.LongAccumulator\n",
       "\u001b[39m\r\n",
       "\u001b[36maccUnnamed\u001b[39m: \u001b[32mLongAccumulator\u001b[39m = LongAccumulator(id: 25, name: None, value: 0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.util.LongAccumulator\n",
    "val accUnnamed = new LongAccumulator\n",
    "val acc = spark.sparkContext.register(accUnnamed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27126ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36maccChina\u001b[39m: \u001b[32mLongAccumulator\u001b[39m = LongAccumulator(id: 27, name: Some(China), value: 0)\r\n",
       "\u001b[36maccChina2\u001b[39m: \u001b[32mLongAccumulator\u001b[39m = LongAccumulator(id: 26, name: Some(China), value: 0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val accChina = new LongAccumulator\n",
    "val accChina2 = spark.sparkContext.longAccumulator(\"China\")\n",
    "spark.sparkContext.register(accChina, \"China\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcc68d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36maccChinaFunc\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accChinaFunc(flight_row: Flight) = {\n",
    "  val destination = flight_row.DEST_COUNTRY_NAME\n",
    "  val origin = flight_row.ORIGIN_COUNTRY_NAME\n",
    "  if (destination == \"China\") {\n",
    "    accChina.add(flight_row.count.toLong)\n",
    "  }\n",
    "  if (origin == \"China\") {\n",
    "    accChina.add(flight_row.count.toLong)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b81769a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">foreach at cmd5.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "flights.foreach(flight_row => accChinaFunc(flight_row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9049fec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres6\u001b[39m: \u001b[32mjava\u001b[39m.\u001b[32mlang\u001b[39m.\u001b[32mLong\u001b[39m = \u001b[32m953L\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accChina.value // 953"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "009d9885",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.SparkException: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:\norg.apache.spark.sql.almondinternals.NotebookSparkSessionBuilder.getOrCreate(NotebookSparkSessionBuilder.scala:62)\nammonite.$sess.cmd0$Helper.<init>(cmd0.sc:12)\nammonite.$sess.cmd0$.<init>(cmd0.sc:7)\nammonite.$sess.cmd0$.<clinit>(cmd0.sc)\nammonite.$sess.cmd0.$main(cmd0.sc)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:566)\nammonite.runtime.Evaluator$$anon$1.$anonfun$evalMain$1(Evaluator.scala:112)\nammonite.util.Util$.withContextClassloader(Util.scala:16)\nammonite.runtime.Evaluator$$anon$1.evalMain(Evaluator.scala:94)\nammonite.runtime.Evaluator$$anon$1.$anonfun$processLine$2(Evaluator.scala:131)\nammonite.util.Catching.map(Res.scala:116)\nammonite.runtime.Evaluator$$anon$1.$anonfun$processLine$1(Evaluator.scala:125)\nammonite.util.Res$Success.flatMap(Res.scala:61)\nammonite.runtime.Evaluator$$anon$1.processLine(Evaluator.scala:124)\nammonite.interp.Interpreter.$anonfun$evaluateLine$4(Interpreter.scala:287)\nammonite.util.Res$Success.flatMap(Res.scala:61)\nammonite.interp.Interpreter.$anonfun$evaluateLine$2(Interpreter.scala:273)\u001b[39m\r\n  org.apache.spark.SparkContext$.$anonfun$assertNoOtherContextIsRunning$2(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2483\u001b[39m)\r\n  org.apache.spark.SparkContext$.$anonfun$assertNoOtherContextIsRunning$2$adapted(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2479\u001b[39m)\r\n  scala.Option.foreach(\u001b[32mOption.scala\u001b[39m:\u001b[32m274\u001b[39m)\r\n  org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2479\u001b[39m)\r\n  org.apache.spark.SparkContext$.markPartiallyConstructed(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2568\u001b[39m)\r\n  org.apache.spark.SparkContext.<init>(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m85\u001b[39m)\r\n  ammonite.$sess.cmd11$Helper.<init>(\u001b[32mcmd11.sc\u001b[39m:\u001b[32m34\u001b[39m)\r\n  ammonite.$sess.cmd11$.<init>(\u001b[32mcmd11.sc\u001b[39m:\u001b[32m7\u001b[39m)\r\n  ammonite.$sess.cmd11$.<clinit>(\u001b[32mcmd11.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "import scala.collection.mutable.ArrayBuffer\n",
    "import org.apache.spark.util.AccumulatorV2\n",
    "\n",
    "val arr = ArrayBuffer[BigInt]()\n",
    "\n",
    "class EvenAccumulator extends AccumulatorV2[BigInt, BigInt] {\n",
    "  private var num:BigInt = 0\n",
    "  def reset(): Unit = {\n",
    "    this.num = 0\n",
    "  }\n",
    "  def add(intValue: BigInt): Unit = {\n",
    "    if (intValue % 2 == 0) {\n",
    "        this.num += intValue\n",
    "    }\n",
    "  }\n",
    "  def merge(other: AccumulatorV2[BigInt,BigInt]): Unit = {\n",
    "    this.num += other.value\n",
    "  }\n",
    "  def value():BigInt = {\n",
    "    this.num\n",
    "  }\n",
    "  def copy(): AccumulatorV2[BigInt,BigInt] = {\n",
    "    new EvenAccumulator\n",
    "  }\n",
    "  def isZero():Boolean = {\n",
    "    this.num == 0\n",
    "  }\n",
    "}\n",
    "val acc = new EvenAccumulator\n",
    "//val newAcc = sc.register(acc, \"evenAcc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ef8761",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

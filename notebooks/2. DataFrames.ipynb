{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spark-stubs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (org.eclipse.jetty.util.log).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkSession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "22/07/11 11:33:06 INFO SparkContext: Running Spark version 2.4.5\n",
      "22/07/11 11:33:07 INFO SparkContext: Submitted application: 9bd77562-2e28-4361-b27f-9dea01d3ec7e\n",
      "22/07/11 11:33:07 INFO SecurityManager: Changing view acls to: Alvaro\n",
      "22/07/11 11:33:07 INFO SecurityManager: Changing modify acls to: Alvaro\n",
      "22/07/11 11:33:07 INFO SecurityManager: Changing view acls groups to: \n",
      "22/07/11 11:33:07 INFO SecurityManager: Changing modify acls groups to: \n",
      "22/07/11 11:33:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Alvaro); groups with view permissions: Set(); users  with modify permissions: Set(Alvaro); groups with modify permissions: Set()\n",
      "22/07/11 11:33:09 INFO Utils: Successfully started service 'sparkDriver' on port 50855.\n",
      "22/07/11 11:33:09 INFO SparkEnv: Registering MapOutputTracker\n",
      "22/07/11 11:33:09 INFO SparkEnv: Registering BlockManagerMaster\n",
      "22/07/11 11:33:09 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "22/07/11 11:33:09 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "22/07/11 11:33:09 INFO DiskBlockManager: Created local directory at C:\\Users\\alvar\\AppData\\Local\\Temp\\blockmgr-15fdade9-7949-4b64-8141-4da4a469eb7d\n",
      "22/07/11 11:33:09 INFO MemoryStore: MemoryStore started with capacity 1646.4 MB\n",
      "22/07/11 11:33:09 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "22/07/11 11:33:09 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "22/07/11 11:33:09 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://LAPTOP-30N4Q54J.mshome.net:4041\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/jitpack.io/com/github/jupyter/jvm-repr/0.4.0/jvm-repr-0.4.0.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/jvm-repr-0.4.0.jar with timestamp 1657531989649\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/github/javaparser/javaparser-core/3.2.5/javaparser-core-3.2.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/javaparser-core-3.2.5.jar with timestamp 1657531989652\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/github/scopt/scopt_2.12/3.5.0/scopt_2.12-3.5.0.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/scopt_2.12-3.5.0.jar with timestamp 1657531989655\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/acyclic_2.12/0.1.5/acyclic_2.12-0.1.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/acyclic_2.12-0.1.5.jar with timestamp 1657531989657\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-interp_2.12.8/1.6.5/ammonite-interp_2.12.8-1.6.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/ammonite-interp_2.12.8-1.6.5.jar with timestamp 1657531989658\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-ops_2.12/1.6.5/ammonite-ops_2.12-1.6.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/ammonite-ops_2.12-1.6.5.jar with timestamp 1657531989659\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-repl_2.12.8/1.6.5/ammonite-repl_2.12.8-1.6.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/ammonite-repl_2.12.8-1.6.5.jar with timestamp 1657531989661\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-runtime_2.12/1.6.5/ammonite-runtime_2.12-1.6.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/ammonite-runtime_2.12-1.6.5.jar with timestamp 1657531989664\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-terminal_2.12/1.6.5/ammonite-terminal_2.12-1.6.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/ammonite-terminal_2.12-1.6.5.jar with timestamp 1657531989665\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-util_2.12/1.6.5/ammonite-util_2.12-1.6.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/ammonite-util_2.12-1.6.5.jar with timestamp 1657531989667\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/fansi_2.12/0.2.4/fansi_2.12-0.2.4.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/fansi_2.12-0.2.4.jar with timestamp 1657531989668\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/fastparse_2.12/2.1.0/fastparse_2.12-2.1.0.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/fastparse_2.12-2.1.0.jar with timestamp 1657531989671\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/geny_2.12/0.1.5/geny_2.12-0.1.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/geny_2.12-0.1.5.jar with timestamp 1657531989674\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/os-lib_2.12/0.2.6/os-lib_2.12-0.2.6.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/os-lib_2.12-0.2.6.jar with timestamp 1657531989676\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/pprint_2.12/0.5.2/pprint_2.12-0.5.2.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/pprint_2.12-0.5.2.jar with timestamp 1657531989679\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/requests_2.12/0.1.7/requests_2.12-0.1.7.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/requests_2.12-0.1.7.jar with timestamp 1657531989681\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/scalaparse_2.12/2.1.0/scalaparse_2.12-2.1.0.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/scalaparse_2.12-2.1.0.jar with timestamp 1657531989682\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/sourcecode_2.12/0.1.5/sourcecode_2.12-0.1.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/sourcecode_2.12-0.1.5.jar with timestamp 1657531989687\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/ujson_2.12/0.7.1/ujson_2.12-0.7.1.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/ujson_2.12-0.7.1.jar with timestamp 1657531989689\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/upack_2.12/0.7.1/upack_2.12-0.7.1.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/upack_2.12-0.7.1.jar with timestamp 1657531989700\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/upickle-core_2.12/0.7.1/upickle-core_2.12-0.7.1.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/upickle-core_2.12-0.7.1.jar with timestamp 1657531989702\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/upickle-implicits_2.12/0.7.1/upickle-implicits_2.12-0.7.1.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/upickle-implicits_2.12-0.7.1.jar with timestamp 1657531989704\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/upickle_2.12/0.7.1/upickle_2.12-0.7.1.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/upickle_2.12-0.7.1.jar with timestamp 1657531989706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/utest_2.12/0.6.4/utest_2.12-0.6.4.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/utest_2.12-0.6.4.jar with timestamp 1657531989709\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/io/get-coursier/coursier-cache_2.12/1.1.0-M13-1/coursier-cache_2.12-1.1.0-M13-1.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/coursier-cache_2.12-1.1.0-M13-1.jar with timestamp 1657531989713\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/io/get-coursier/coursier-core_2.12/1.1.0-M13-1/coursier-core_2.12-1.1.0-M13-1.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/coursier-core_2.12-1.1.0-M13-1.jar with timestamp 1657531989714\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/io/get-coursier/coursier_2.12/1.1.0-M13-1/coursier_2.12-1.1.0-M13-1.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/coursier_2.12-1.1.0-M13-1.jar with timestamp 1657531989715\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/net/java/dev/jna/jna/4.2.2/jna-4.2.2.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/jna-4.2.2.jar with timestamp 1657531989716\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/javassist/javassist/3.21.0-GA/javassist-3.21.0-GA.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/javassist-3.21.0-GA.jar with timestamp 1657531989717\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/jline/jline-reader/3.6.2/jline-reader-3.6.2.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/jline-reader-3.6.2.jar with timestamp 1657531989718\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/jline/jline-terminal-jna/3.6.2/jline-terminal-jna-3.6.2.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/jline-terminal-jna-3.6.2.jar with timestamp 1657531989720\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/jline/jline-terminal/3.6.2/jline-terminal-3.6.2.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/jline-terminal-3.6.2.jar with timestamp 1657531989721\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.12/1.1.1/scala-xml_2.12-1.1.1.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/scala-xml_2.12-1.1.1.jar with timestamp 1657531989723\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/scala-sbt/test-interface/1.0/test-interface-1.0.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/test-interface-1.0.jar with timestamp 1657531989728\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/sh/almond/interpreter-api_2.12/0.4.0/interpreter-api_2.12-0.4.0.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/interpreter-api_2.12-0.4.0.jar with timestamp 1657531989730\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/sh/almond/jupyter-api_2.12/0.4.0/jupyter-api_2.12-0.4.0.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/jupyter-api_2.12-0.4.0.jar with timestamp 1657531989732\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/sh/almond/scala-kernel-api_2.12.8/0.4.0/scala-kernel-api_2.12.8-0.4.0.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/scala-kernel-api_2.12.8-0.4.0.jar with timestamp 1657531989735\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/json4s/json4s-core_2.12/3.5.3/json4s-core_2.12-3.5.3.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/json4s-core_2.12-3.5.3.jar with timestamp 1657531989736\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/xml-apis-1.3.04.jar with timestamp 1657531989738\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/javax.servlet-api-3.1.0.jar with timestamp 1657531989739\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/metrics-core-3.1.5.jar with timestamp 1657531989742\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/google/inject/guice/3.0/guice-3.0.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/guice-3.0.jar with timestamp 1657531989743\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/slf4j-log4j12-1.7.16.jar with timestamp 1657531989745\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/commons-crypto-1.0.0.jar with timestamp 1657531989746\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/xbean-asm6-shaded-4.8.jar with timestamp 1657531989748\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/xercesImpl-2.9.1.jar with timestamp 1657531989749\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/github/luben/zstd-jni/1.3.2-2/zstd-jni-1.3.2-2.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/zstd-jni-1.3.2-2.jar with timestamp 1657531989751\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/jettison-1.1.jar with timestamp 1657531989752\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/2.4.5/spark-unsafe_2.12-2.4.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/spark-unsafe_2.12-2.4.5.jar with timestamp 1657531989755\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-yarn-server-nodemanager/2.6.5/hadoop-yarn-server-nodemanager-2.6.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/hadoop-yarn-server-nodemanager-2.6.5.jar with timestamp 1657531989757\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/parquet/parquet-hadoop/1.10.1/parquet-hadoop-1.10.1.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/parquet-hadoop-1.10.1.jar with timestamp 1657531989759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/joda-time/joda-time/2.9.9/joda-time-2.9.9.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/joda-time-2.9.9.jar with timestamp 1657531989762\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/commons-io/commons-io/2.4/commons-io-2.4.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/commons-io-2.4.jar with timestamp 1657531989763\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/commons-configuration-1.6.jar with timestamp 1657531989764\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.7/antlr4-runtime-4.7.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/antlr4-runtime-4.7.jar with timestamp 1657531989765\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/jersey-common-2.22.2.jar with timestamp 1657531989766\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/io/airlift/aircompressor/0.10/aircompressor-0.10.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/aircompressor-0.10.jar with timestamp 1657531989767\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/commons-compress-1.8.1.jar with timestamp 1657531989768\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/jcl-over-slf4j-1.7.16.jar with timestamp 1657531989770\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/javassist-3.18.1-GA.jar with timestamp 1657531989771\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/aopalliance-repackaged-2.4.0-b34.jar with timestamp 1657531989773\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/lz4-java-1.4.0.jar with timestamp 1657531989774\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/netty-3.9.9.Final.jar with timestamp 1657531989776\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-network-common_2.12/2.4.5/spark-network-common_2.12-2.4.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/spark-network-common_2.12-2.4.5.jar with timestamp 1657531989777\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/aopalliance/aopalliance/1.0/aopalliance-1.0.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/aopalliance-1.0.jar with timestamp 1657531989779\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.7.9/jackson-core-2.7.9.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/jackson-core-2.7.9.jar with timestamp 1657531989780\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-sql_2.12/2.4.5/spark-sql_2.12-2.4.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/spark-sql_2.12-2.4.5.jar with timestamp 1657531989782\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/gson-2.2.4.jar with timestamp 1657531989783\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/api-util-1.0.0-M20.jar with timestamp 1657531989784\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/hadoop-yarn-client-2.6.5.jar with timestamp 1657531989786\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/jaxb-api-2.2.2.jar with timestamp 1657531989787\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-tags_2.12/2.4.5/spark-tags_2.12-2.4.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/spark-tags_2.12-2.4.5.jar with timestamp 1657531989788\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/parquet/parquet-jackson/1.10.1/parquet-jackson-1.10.1.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/parquet-jackson-1.10.1.jar with timestamp 1657531989789\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/shims-0.7.45.jar with timestamp 1657531989791\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/paranamer-2.8.jar with timestamp 1657531989792\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/avro/avro/1.8.2/avro-1.8.2.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/avro-1.8.2.jar with timestamp 1657531989796\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/httpcomponents/httpcore/4.2.4/httpcore-4.2.4.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/httpcore-4.2.4.jar with timestamp 1657531989798\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/carrotsearch/hppc/0.7.2/hppc-0.7.2.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/hppc-0.7.2.jar with timestamp 1657531989801\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.6.7.3/jackson-databind-2.6.7.3.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/jackson-databind-2.6.7.3.jar with timestamp 1657531989802\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-kvstore_2.12/2.4.5/spark-kvstore_2.12-2.4.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/spark-kvstore_2.12-2.4.5.jar with timestamp 1657531989805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/commons-codec/commons-codec/1.10/commons-codec-1.10.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/commons-codec-1.10.jar with timestamp 1657531989807\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/jetty-util-6.1.26.jar with timestamp 1657531989809\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/hadoop-yarn-common-2.6.5.jar with timestamp 1657531989814\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/json4s/json4s-scalap_2.12/3.5.3/json4s-scalap_2.12-3.5.3.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/json4s-scalap_2.12-3.5.3.jar with timestamp 1657531989818\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/commons-beanutils-1.7.0.jar with timestamp 1657531989830\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/avro-ipc-1.8.2.jar with timestamp 1657531989839\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-auth/2.6.5/hadoop-auth-2.6.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/hadoop-auth-2.6.5.jar with timestamp 1657531989842\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/arrow/arrow-vector/0.10.0/arrow-vector-0.10.0.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/arrow-vector-0.10.0.jar with timestamp 1657531989845\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/parquet/parquet-common/1.10.1/parquet-common-1.10.1.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/parquet-common-1.10.1.jar with timestamp 1657531989848\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/compress-lzf-1.0.3.jar with timestamp 1657531989849\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/log4j/log4j/1.2.17/log4j-1.2.17.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/log4j-1.2.17.jar with timestamp 1657531989850\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/stream-2.7.0.jar with timestamp 1657531989852\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/vlkan/flatbuffers/1.2.0-3f79e055/flatbuffers-1.2.0-3f79e055.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/flatbuffers-1.2.0-3f79e055.jar with timestamp 1657531989854\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/apacheds-i18n-2.0.0-M15.jar with timestamp 1657531989855\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/javax/activation/activation/1.1.1/activation-1.1.1.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/activation-1.1.1.jar with timestamp 1657531989856\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/javax/inject/javax.inject/1/javax.inject-1.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/javax.inject-1.jar with timestamp 1657531989857\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/jul-to-slf4j-1.7.16.jar with timestamp 1657531989858\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/minlog-1.3.0.jar with timestamp 1657531989860\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/xmlenc/xmlenc/0.52/xmlenc-0.52.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/xmlenc-0.52.jar with timestamp 1657531989860\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/univocity/univocity-parsers/2.7.3/univocity-parsers-2.7.3.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/univocity-parsers-2.7.3.jar with timestamp 1657531989861\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/codehaus/janino/janino/3.0.9/janino-3.0.9.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/janino-3.0.9.jar with timestamp 1657531989862\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-core_2.12/2.4.5/spark-core_2.12-2.4.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/spark-core_2.12-2.4.5.jar with timestamp 1657531989863\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/commons-lang/commons-lang/2.6/commons-lang-2.6.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/commons-lang-2.6.jar with timestamp 1657531989865\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.25/slf4j-api-1.7.25.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/slf4j-api-1.7.25.jar with timestamp 1657531989866\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/oro/oro/2.0.8/oro-2.0.8.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/oro-2.0.8.jar with timestamp 1657531989867\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/javax.annotation-api-1.2.jar with timestamp 1657531989868\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/commons-cli/commons-cli/1.2/commons-cli-1.2.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/commons-cli-1.2.jar with timestamp 1657531989869\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/commons-math3-3.4.1.jar with timestamp 1657531989870\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/jackson-core-asl-1.9.13.jar with timestamp 1657531989871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/orc/orc-core/1.5.5/orc-core-1.5.5-nohive.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/orc-core-1.5.5-nohive.jar with timestamp 1657531989872\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/google/guava/guava/16.0.1/guava-16.0.1.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/guava-16.0.1.jar with timestamp 1657531989873\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/hadoop-yarn-server-common-2.6.5.jar with timestamp 1657531989875\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/hk2-api-2.4.0-b34.jar with timestamp 1657531989876\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/leveldbjni-all-1.8.jar with timestamp 1657531989877\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/osgi-resource-locator-1.0.1.jar with timestamp 1657531989879\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.12/1.1.0/scala-parser-combinators_2.12-1.1.0.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/scala-parser-combinators_2.12-1.1.0.jar with timestamp 1657531989880\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-catalyst_2.12/2.4.5/spark-catalyst_2.12-2.4.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/spark-catalyst_2.12-2.4.5.jar with timestamp 1657531989881\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/hadoop-yarn-api-2.6.5.jar with timestamp 1657531989882\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/commons-lang3-3.5.jar with timestamp 1657531989897\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/commons-collections-3.2.2.jar with timestamp 1657531989899\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/hk2-utils-2.4.0-b34.jar with timestamp 1657531989901\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/tukaani/xz/1.5/xz-1.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/xz-1.5.jar with timestamp 1657531989903\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/2.6.5/hadoop-common-2.6.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/hadoop-common-2.6.5.jar with timestamp 1657531989909\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.5/hadoop-mapreduce-client-core-2.6.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/hadoop-mapreduce-client-core-2.6.5.jar with timestamp 1657531989911\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/jline/jline/0.9.94/jline-0.9.94.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/jline-0.9.94.jar with timestamp 1657531989915\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/orc/orc-mapreduce/1.5.5/orc-mapreduce-1.5.5-nohive.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/orc-mapreduce-1.5.5-nohive.jar with timestamp 1657531989917\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/httpclient-4.2.5.jar with timestamp 1657531989919\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-annotations/2.6.5/hadoop-annotations-2.6.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/hadoop-annotations-2.6.5.jar with timestamp 1657531989921\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/unused-1.0.0.jar with timestamp 1657531989922\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/jersey-container-servlet-core-2.22.2.jar with timestamp 1657531989923\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/codehaus/janino/commons-compiler/3.0.9/commons-compiler-3.0.9.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/commons-compiler-3.0.9.jar with timestamp 1657531989925\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/hk2-locator-2.4.0-b34.jar with timestamp 1657531989926\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/jackson-mapper-asl-1.9.13.jar with timestamp 1657531989927\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/py4j-0.10.7.jar with timestamp 1657531989928\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/curator-recipes-2.6.0.jar with timestamp 1657531989928\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/jsr305-3.0.2.jar with timestamp 1657531989929\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/orc/orc-shims/1.5.5/orc-shims-1.5.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/orc-shims-1.5.5.jar with timestamp 1657531989930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/curator-client-2.6.0.jar with timestamp 1657531989931\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/snappy-java-1.1.7.3.jar with timestamp 1657531989932\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/validation-api-1.1.0.Final.jar with timestamp 1657531989933\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-hdfs/2.6.5/hadoop-hdfs-2.6.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/hadoop-hdfs-2.6.5.jar with timestamp 1657531989936\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/ivy-2.4.0.jar with timestamp 1657531989937\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/stax-api-1.0-2.jar with timestamp 1657531989938\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/RoaringBitmap-0.7.45.jar with timestamp 1657531989939\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/jackson-jaxrs-1.9.13.jar with timestamp 1657531989940\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/io/dropwizard/metrics/metrics-json/3.1.5/metrics-json-3.1.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/metrics-json-3.1.5.jar with timestamp 1657531989940\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/jersey-container-servlet-2.22.2.jar with timestamp 1657531989946\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/jackson-xc-1.9.13.jar with timestamp 1657531989947\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/javax.ws.rs-api-2.0.1.jar with timestamp 1657531989948\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/fasterxml/jackson/module/jackson-module-scala_2.12/2.6.7.1/jackson-module-scala_2.12-2.6.7.1.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/jackson-module-scala_2.12-2.6.7.1.jar with timestamp 1657531989950\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/parquet/parquet-column/1.10.1/parquet-column-1.10.1.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/parquet-column-1.10.1.jar with timestamp 1657531989951\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/esotericsoftware/kryo-shaded/4.0.2/kryo-shaded-4.0.2.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/kryo-shaded-4.0.2.jar with timestamp 1657531989956\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/api-asn1-api-1.0.0-M20.jar with timestamp 1657531989958\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/pyrolite-4.13.jar with timestamp 1657531989960\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/parquet/parquet-format/2.4.0/parquet-format-2.4.0.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/parquet-format-2.4.0.jar with timestamp 1657531989967\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.12/1.0.6/scala-xml_2.12-1.0.6.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/scala-xml_2.12-1.0.6.jar with timestamp 1657531989968\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/zookeeper-3.4.6.jar with timestamp 1657531989969\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/objenesis/objenesis/2.5.1/objenesis-2.5.1.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/objenesis-2.5.1.jar with timestamp 1657531989970\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/jersey-media-jaxb-2.22.2.jar with timestamp 1657531989971\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/commons-httpclient-3.1.jar with timestamp 1657531989971\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.5/hadoop-mapreduce-client-jobclient-2.6.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/hadoop-mapreduce-client-jobclient-2.6.5.jar with timestamp 1657531989973\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/io/netty/netty-all/4.1.42.Final/netty-all-4.1.42.Final.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/netty-all-4.1.42.Final.jar with timestamp 1657531989974\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-network-shuffle_2.12/2.4.5/spark-network-shuffle_2.12-2.4.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/spark-network-shuffle_2.12-2.4.5.jar with timestamp 1657531989975\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-sketch_2.12/2.4.5/spark-sketch_2.12-2.4.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/spark-sketch_2.12-2.4.5.jar with timestamp 1657531989976\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/chill-java-0.9.3.jar with timestamp 1657531989978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/json4s/json4s-ast_2.12/3.5.3/json4s-ast_2.12-3.5.3.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/json4s-ast_2.12-3.5.3.jar with timestamp 1657531989979\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/avro-mapred-1.8.2-hadoop2.jar with timestamp 1657531989980\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/parquet/parquet-encoding/1.10.1/parquet-encoding-1.10.1.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/parquet-encoding-1.10.1.jar with timestamp 1657531989981\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/commons-net/commons-net/3.1/commons-net-3.1.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/commons-net-3.1.jar with timestamp 1657531989982\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/io/dropwizard/metrics/metrics-jvm/3.1.5/metrics-jvm-3.1.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/metrics-jvm-3.1.5.jar with timestamp 1657531989982\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/jersey-client-2.22.2.jar with timestamp 1657531989983\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/arrow/arrow-memory/0.10.0/arrow-memory-0.10.0.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/arrow-memory-0.10.0.jar with timestamp 1657531989984\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.5/hadoop-mapreduce-client-shuffle-2.6.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/hadoop-mapreduce-client-shuffle-2.6.5.jar with timestamp 1657531989986\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/fasterxml/jackson/module/jackson-module-paranamer/2.7.9/jackson-module-paranamer-2.7.9.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/jackson-module-paranamer-2.7.9.jar with timestamp 1657531989987\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.5/hadoop-mapreduce-client-app-2.6.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/hadoop-mapreduce-client-app-2.6.5.jar with timestamp 1657531989989\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/htrace-core-3.0.4.jar with timestamp 1657531989991\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/jersey-guava-2.22.2.jar with timestamp 1657531989992\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/commons-digester/commons-digester/1.8/commons-digester-1.8.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/commons-digester-1.8.jar with timestamp 1657531989993\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-launcher_2.12/2.4.5/spark-launcher_2.12-2.4.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/spark-launcher_2.12-2.4.5.jar with timestamp 1657531989997\n",
      "22/07/11 11:33:09 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/twitter/chill_2.12/0.9.3/chill_2.12-0.9.3.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/chill_2.12-0.9.3.jar with timestamp 1657531989998\n",
      "22/07/11 11:33:10 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/arrow/arrow-format/0.10.0/arrow-format-0.10.0.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/arrow-format-0.10.0.jar with timestamp 1657531990000\n",
      "22/07/11 11:33:10 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/protobuf-java-2.5.0.jar with timestamp 1657531990001\n",
      "22/07/11 11:33:10 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/apacheds-kerberos-codec-2.0.0-M15.jar with timestamp 1657531990002\n",
      "22/07/11 11:33:10 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/hadoop-client-2.6.5.jar with timestamp 1657531990004\n",
      "22/07/11 11:33:10 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/metrics-graphite-3.1.5.jar with timestamp 1657531990005\n",
      "22/07/11 11:33:10 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/json4s/json4s-jackson_2.12/3.5.3/json4s-jackson_2.12-3.5.3.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/json4s-jackson_2.12-3.5.3.jar with timestamp 1657531990007\n",
      "22/07/11 11:33:10 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/javax.inject-2.4.0-b34.jar with timestamp 1657531990009\n",
      "22/07/11 11:33:10 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/jersey-server-2.22.2.jar with timestamp 1657531990010\n",
      "22/07/11 11:33:10 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.6.7/jackson-annotations-2.6.7.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/jackson-annotations-2.6.7.jar with timestamp 1657531990012\n",
      "22/07/11 11:33:10 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.5/hadoop-mapreduce-client-common-2.6.5.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/hadoop-mapreduce-client-common-2.6.5.jar with timestamp 1657531990013\n",
      "22/07/11 11:33:10 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/curator-framework-2.6.0.jar with timestamp 1657531990014\n",
      "22/07/11 11:33:10 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/sh/almond/almond-spark_2.12/0.4.0/almond-spark_2.12-0.4.0.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/almond-spark_2.12-0.4.0.jar with timestamp 1657531990016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/11 11:33:10 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/io/argonaut/argonaut_2.12/6.2.3/argonaut_2.12-6.2.3.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/argonaut_2.12-6.2.3.jar with timestamp 1657531990018\n",
      "22/07/11 11:33:10 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/jetty-server/9.4.15.v20190215/jetty-server-9.4.15.v20190215.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/jetty-server-9.4.15.v20190215.jar with timestamp 1657531990019\n",
      "22/07/11 11:33:10 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/sh/almond/ammonite-spark_2.12/0.4.0/ammonite-spark_2.12-0.4.0.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/ammonite-spark_2.12-0.4.0.jar with timestamp 1657531990020\n",
      "22/07/11 11:33:10 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/github/alexarchambault/argonaut-shapeless_6.2_2.12/1.2.0-M10/argonaut-shapeless_6.2_2.12-1.2.0-M10.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/argonaut-shapeless_6.2_2.12-1.2.0-M10.jar with timestamp 1657531990022\n",
      "22/07/11 11:33:10 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/jetty-io/9.4.15.v20190215/jetty-io-9.4.15.v20190215.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/jetty-io-9.4.15.v20190215.jar with timestamp 1657531990024\n",
      "22/07/11 11:33:10 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/typelevel/macro-compat_2.12/1.1.1/macro-compat_2.12-1.1.1.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/macro-compat_2.12-1.1.1.jar with timestamp 1657531990027\n",
      "22/07/11 11:33:10 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/jetty-http/9.4.15.v20190215/jetty-http-9.4.15.v20190215.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/jetty-http-9.4.15.v20190215.jar with timestamp 1657531990029\n",
      "22/07/11 11:33:10 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/jetty-util/9.4.15.v20190215/jetty-util-9.4.15.v20190215.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/jetty-util-9.4.15.v20190215.jar with timestamp 1657531990030\n",
      "22/07/11 11:33:10 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/com/chuusai/shapeless_2.12/2.3.3/shapeless_2.12-2.3.3.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/shapeless_2.12-2.3.3.jar with timestamp 1657531990031\n",
      "22/07/11 11:33:10 INFO SparkContext: Added JAR file:/C:/Users/alvar/AppData/Local/Coursier/cache/v1/https/repo1.maven.org/maven2/sh/almond/spark-stubs_24_2.12/0.4.0/spark-stubs_24_2.12-0.4.0.jar at spark://LAPTOP-30N4Q54J.mshome.net:50855/jars/spark-stubs_24_2.12-0.4.0.jar with timestamp 1657531990033\n",
      "22/07/11 11:33:10 INFO Executor: Starting executor ID driver on host localhost\n",
      "22/07/11 11:33:10 INFO Executor: Using REPL class URI: http://172.30.32.1:50810\n",
      "22/07/11 11:33:10 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50898.\n",
      "22/07/11 11:33:10 INFO NettyBlockTransferService: Server created on LAPTOP-30N4Q54J.mshome.net:50898\n",
      "22/07/11 11:33:10 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "22/07/11 11:33:10 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, LAPTOP-30N4Q54J.mshome.net, 50898, None)\n",
      "22/07/11 11:33:10 INFO BlockManagerMasterEndpoint: Registering block manager LAPTOP-30N4Q54J.mshome.net:50898 with 1646.4 MB RAM, BlockManagerId(driver, LAPTOP-30N4Q54J.mshome.net, 50898, None)\n",
      "22/07/11 11:33:10 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, LAPTOP-30N4Q54J.mshome.net, 50898, None)\n",
      "22/07/11 11:33:10 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, LAPTOP-30N4Q54J.mshome.net, 50898, None)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://LAPTOP-30N4Q54J.mshome.net:4041\">Spark UI</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                              \n",
       "\n",
       "//import org.apache.spark._\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{NotebookSparkSession, SparkSession}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{functions => func, _}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\n",
       "\u001b[39m\r\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@3171e264\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.slf4j.LoggerFactory\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\n",
       "\u001b[39m\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mrun\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.5` \n",
    "import $ivy.`sh.almond::almond-spark:0.4.0`\n",
    "\n",
    "//import org.apache.spark._\n",
    "import org.apache.spark.sql.{NotebookSparkSession, SparkSession}\n",
    "import org.apache.spark.sql.{functions => func, _}\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val spark = NotebookSparkSession\n",
    "      .builder()\n",
    "      .config(\"spark.sql.join.preferSortMergeJoin\", false)\n",
    "      .config(\"spark.sql.shuffle.partitions\", 64)\n",
    "      .master(\"local[*]\")\n",
    "      .getOrCreate()\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "import org.slf4j.LoggerFactory\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "\n",
    "Logger.getRootLogger().setLevel(Level.ERROR)\n",
    "\n",
    "def run[A](code: => A): A = {\n",
    "    val start = System.currentTimeMillis()\n",
    "    val res = code\n",
    "    println(s\"Took ${System.currentTimeMillis() - start}\")\n",
    "    res\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On `DataFrame`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create datasets from external data sources using different formats, e.g. Json, parquet, CSV, etc. \n",
    "\n",
    "###### Para leer datos en otros formatos usamos `.read.formato(\"fichero.formato\")`\n",
    "###### Con multiline permitimos que los datos JSON se encuentren en diferentes lineas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "var comm = Jupyter.notebook.kernel.comm_manager.new_comm('cancel-stage-cd4e923f-d212-4fdb-8651-e0ec2835d174', {});\n",
       "\n",
       "function cancelStage(stageId) {\n",
       "  console.log('Cancelling stage ' + stageId);\n",
       "  comm.send({ 'stageId': stageId });\n",
       "}\n",
       "</script>\n",
       "          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">json at cmd1.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata\u001b[39m: \u001b[32mDataFrame\u001b[39m = [e: string, fecha: string ... 28 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data: DataFrame = spark.read.option(\"multiline\", \"true\").json(\"D:/TFGAlvaroSanchez/data/2916A(Vitigudino)-2018.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we created a `DataFrame`, not a `Dataset`. Dataframes are like datasets, i.e. programs to generate distributed data sets, but *dynamically typed*. This means that, at compile time, Scala only knows that a dataframe consists of `Row`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">collect at cmd2.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres2\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mRow\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  [77.0,2018-1,82.0,2916A,4.0,14.0,8.0,2.0,0.0,13.0,0.0,2.0,0.0,12.4(13),53.0,41.0,1024.9,944.8(28),933.8,911.9(07),16.2(03),-2.8(15),4.2,10.3,5.7,1.0,10.1,12.0,23/18.3(05),281.0],\n",
       "  [59.0,2018-2,68.0,2916A,5.7,8.0,6.0,2.0,1.0,21.0,0.0,4.0,0.0,30.2(27),52.0,53.0,1016.7,936.4(17),926.0,899.7(28),17.5(26),-5.2(24),3.1,10.3,4.6,-1.1,7.8,13.0,26/19.2(13),293.0],\n",
       "  [77.0,2018-3,77.0,2916A,4.1,22.0,20.0,4.0,1.0,6.0,0.0,16.0,0.0,32.2(09),172.6,34.0,1008.7,938.4(27),919.4,891.4(01),19.5(27),-3.0(22),7.8,11.4,6.7,1.9,8.9,20.0,26/23.9(10),452.0],\n",
       "  [95.0,2018-4,71.0,2916A,6.0,17.0,16.0,4.0,0.0,1.0,0.0,null,null,15.0(12),102.4,45.0,1011.9,935.2(17),923.8,907.1(10),26.9(25),-0.3(11),8.6,16.9,11.2,5.4,11.6,null,null,null],\n",
       "  [107.0,2018-5,61.0,2916A,7.8,12.0,8.0,3.0,0.0,1.0,0.0,null,null,26.0(24),76.8,54.0,1014.4,934.9(13),927.3,922.4(21),26.9(08),0.0(13),15.9,21.0,14.2,7.4,12.2,null,null,null],\n",
       "  [139.0,2018-6,62.0,2916A,8.4,11.0,9.0,0.0,0.0,0.0,8.0,null,null,9.6(08),49.0,56.0,1013.9,935.9(18),928.2,919.8(30),34.1(24),7.1(02),15.3,25.2,18.6,11.9,17.3,11.0,null,null],\n",
       "  [129.0,2018-7,50.0,2916A,11.0,5.0,2.0,1.0,0.0,0.0,16.0,null,null,26.0(15),29.0,74.0,1013.7,934.6(05),928.9,922.6(01),33.8(10),10.1(05),20.9,29.5,21.4,13.2,16.3,null,null,null],\n",
       "  [104.0,2018-8,36.0,2916A,11.9,0.0,0.0,0.0,0.0,0.0,28.0,null,null,0.0(--),0.0,86.0,1014.2,934.7(10),930.0,924.2(08),40.7(06),9.8(09),27.8,33.9,24.2,14.4,21.3,9.0,null,null],\n",
       "  [109.0,2018-9,43.0,2916A,9.9,4.0,2.0,0.0,0.0,0.0,22.0,null,null,3.4(02),7.0,80.0,1016.6,940.0(27),931.5,923.4(03),36.9(02),8.5(25),25.7,31.4,22.5,13.6,18.0,null,null,null],\n",
       "  [88.0,2018-10,59.0,2916A,6.7,14.0,6.0,0.0,0.0,2.0,2.0,2.0,0.0,5.6(15),26.6,60.0,1014.7,938.7(24),926.9,908.9(30),30.3(05),-2.6(28),9.1,20.6,13.7,6.9,12.8,13.0,18/16.7(13),321.0],\n",
       "  [91.0,2018-11,82.0,2916A,2.7,18.0,16.0,5.0,1.0,0.0,0.0,5.0,0.0,30.4(11),147.8,27.0,1014.9,936.5(02),925.6,911.8(20),19.6(02),0.7(01),7.5,12.6,8.6,4.6,8.5,13.0,29/17.5(25),306.0],\n",
       "  [84.0,2018-12,82.0,2916A,4.3,10.0,5.0,1.0,0.0,6.0,0.0,2.0,0.0,13.8(15),31.4,47.0,1027.4,942.9(22),936.6,918.8(13),18.3(09),-1.5(31),7.7,12.6,7.4,2.2,7.2,11.0,25/18.9(13),242.0],\n",
       "  [97.0,2018-13,64.0,2916A,6.9,135.0,98.0,22.0,3.0,50.0,76.0,null,null,32.2(09/mar),747.6,55.0,1016.0,944.8(28/ene),928.2,891.4(01/mar),40.7(06/ago),-5.2(24/feb..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, a `DataFrame` is defined as an alias of `Dataset`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdataDS\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [e: string, fecha: string ... 28 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataDS: Dataset[Row] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the type of the information to be processed is there! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- e: string (nullable = true)\n",
      " |-- fecha: string (nullable = true)\n",
      " |-- hr: string (nullable = true)\n",
      " |-- indicativo: string (nullable = true)\n",
      " |-- inso: string (nullable = true)\n",
      " |-- np_001: string (nullable = true)\n",
      " |-- np_010: string (nullable = true)\n",
      " |-- np_100: string (nullable = true)\n",
      " |-- np_300: string (nullable = true)\n",
      " |-- nt_00: string (nullable = true)\n",
      " |-- nt_30: string (nullable = true)\n",
      " |-- nw_55: string (nullable = true)\n",
      " |-- nw_91: string (nullable = true)\n",
      " |-- p_max: string (nullable = true)\n",
      " |-- p_mes: string (nullable = true)\n",
      " |-- p_sol: string (nullable = true)\n",
      " |-- q_mar: string (nullable = true)\n",
      " |-- q_max: string (nullable = true)\n",
      " |-- q_med: string (nullable = true)\n",
      " |-- q_min: string (nullable = true)\n",
      " |-- ta_max: string (nullable = true)\n",
      " |-- ta_min: string (nullable = true)\n",
      " |-- ti_max: string (nullable = true)\n",
      " |-- tm_max: string (nullable = true)\n",
      " |-- tm_mes: string (nullable = true)\n",
      " |-- tm_min: string (nullable = true)\n",
      " |-- ts_min: string (nullable = true)\n",
      " |-- w_med: string (nullable = true)\n",
      " |-- w_racha: string (nullable = true)\n",
      " |-- w_rec: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres4_0\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"e\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"fecha\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"hr\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"indicativo\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"inso\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"np_001\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"np_010\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"np_100\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"np_300\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"nt_00\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"nt_30\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"nw_55\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"nw_91\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"p_max\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"p_mes\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"p_sol\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"q_mar\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"q_max\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"q_med\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"q_min\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"ta_max\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"ta_min\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"ti_max\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"tm_max\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"tm_mes\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"tm_min\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"ts_min\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"w_med\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"w_racha\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"w_rec\"\u001b[39m, StringType, true, {})\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.schema\n",
    "data.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can convert a dataframe into a dataset: \n",
    "\n",
    "###### Con `.as[class]` transformamos el DataFrame en DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mData\u001b[39m\r\n",
       "\u001b[36mdataDs\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mData\u001b[39m] = [e: string, fecha: string ... 28 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "\n",
    "case class Data(fecha: String, indicativo: String, p_max: String, hr: String, inso: String, q_max: String,\n",
    "                nw_55: String, q_mar: String, q_med: String, tm_min: String, ta_max : String, \n",
    "               ts_min : String ,nt_30: String, w_racha: String, np_100: String, p_sol: String, nw_91: String, np_001: String,\n",
    "               ta_min: String, w_rec: String, e: String, np_300: String, p_mes: String, w_med: String, \n",
    "               nt_00: String, ti_max: String, tm_mes: String, tm_max: String, q_min: String, np_010: String)\n",
    "\n",
    "val dataDs: Dataset[Data] = data.as[Data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd6.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+----------+----+------+------+------+------+-----+-----+-----+-----+------------+-----+-----+------+-------------+-----+-------------+------------+------------+------+------+------+------+------+-----+-----------+-----+\n",
      "|    e|  fecha|  hr|indicativo|inso|np_001|np_010|np_100|np_300|nt_00|nt_30|nw_55|nw_91|       p_max|p_mes|p_sol| q_mar|        q_max|q_med|        q_min|      ta_max|      ta_min|ti_max|tm_max|tm_mes|tm_min|ts_min|w_med|    w_racha|w_rec|\n",
      "+-----+-------+----+----------+----+------+------+------+------+-----+-----+-----+-----+------------+-----+-----+------+-------------+-----+-------------+------------+------------+------+------+------+------+------+-----+-----------+-----+\n",
      "| 77.0| 2018-1|82.0|     2916A| 4.0|  14.0|   8.0|   2.0|   0.0| 13.0|  0.0|  2.0|  0.0|    12.4(13)| 53.0| 41.0|1024.9|    944.8(28)|933.8|    911.9(07)|    16.2(03)|    -2.8(15)|   4.2|  10.3|   5.7|   1.0|  10.1| 12.0|23/18.3(05)|281.0|\n",
      "| 59.0| 2018-2|68.0|     2916A| 5.7|   8.0|   6.0|   2.0|   1.0| 21.0|  0.0|  4.0|  0.0|    30.2(27)| 52.0| 53.0|1016.7|    936.4(17)|926.0|    899.7(28)|    17.5(26)|    -5.2(24)|   3.1|  10.3|   4.6|  -1.1|   7.8| 13.0|26/19.2(13)|293.0|\n",
      "| 77.0| 2018-3|77.0|     2916A| 4.1|  22.0|  20.0|   4.0|   1.0|  6.0|  0.0| 16.0|  0.0|    32.2(09)|172.6| 34.0|1008.7|    938.4(27)|919.4|    891.4(01)|    19.5(27)|    -3.0(22)|   7.8|  11.4|   6.7|   1.9|   8.9| 20.0|26/23.9(10)|452.0|\n",
      "| 95.0| 2018-4|71.0|     2916A| 6.0|  17.0|  16.0|   4.0|   0.0|  1.0|  0.0| null| null|    15.0(12)|102.4| 45.0|1011.9|    935.2(17)|923.8|    907.1(10)|    26.9(25)|    -0.3(11)|   8.6|  16.9|  11.2|   5.4|  11.6| null|       null| null|\n",
      "|107.0| 2018-5|61.0|     2916A| 7.8|  12.0|   8.0|   3.0|   0.0|  1.0|  0.0| null| null|    26.0(24)| 76.8| 54.0|1014.4|    934.9(13)|927.3|    922.4(21)|    26.9(08)|     0.0(13)|  15.9|  21.0|  14.2|   7.4|  12.2| null|       null| null|\n",
      "|139.0| 2018-6|62.0|     2916A| 8.4|  11.0|   9.0|   0.0|   0.0|  0.0|  8.0| null| null|     9.6(08)| 49.0| 56.0|1013.9|    935.9(18)|928.2|    919.8(30)|    34.1(24)|     7.1(02)|  15.3|  25.2|  18.6|  11.9|  17.3| 11.0|       null| null|\n",
      "|129.0| 2018-7|50.0|     2916A|11.0|   5.0|   2.0|   1.0|   0.0|  0.0| 16.0| null| null|    26.0(15)| 29.0| 74.0|1013.7|    934.6(05)|928.9|    922.6(01)|    33.8(10)|    10.1(05)|  20.9|  29.5|  21.4|  13.2|  16.3| null|       null| null|\n",
      "|104.0| 2018-8|36.0|     2916A|11.9|   0.0|   0.0|   0.0|   0.0|  0.0| 28.0| null| null|     0.0(--)|  0.0| 86.0|1014.2|    934.7(10)|930.0|    924.2(08)|    40.7(06)|     9.8(09)|  27.8|  33.9|  24.2|  14.4|  21.3|  9.0|       null| null|\n",
      "|109.0| 2018-9|43.0|     2916A| 9.9|   4.0|   2.0|   0.0|   0.0|  0.0| 22.0| null| null|     3.4(02)|  7.0| 80.0|1016.6|    940.0(27)|931.5|    923.4(03)|    36.9(02)|     8.5(25)|  25.7|  31.4|  22.5|  13.6|  18.0| null|       null| null|\n",
      "| 88.0|2018-10|59.0|     2916A| 6.7|  14.0|   6.0|   0.0|   0.0|  2.0|  2.0|  2.0|  0.0|     5.6(15)| 26.6| 60.0|1014.7|    938.7(24)|926.9|    908.9(30)|    30.3(05)|    -2.6(28)|   9.1|  20.6|  13.7|   6.9|  12.8| 13.0|18/16.7(13)|321.0|\n",
      "| 91.0|2018-11|82.0|     2916A| 2.7|  18.0|  16.0|   5.0|   1.0|  0.0|  0.0|  5.0|  0.0|    30.4(11)|147.8| 27.0|1014.9|    936.5(02)|925.6|    911.8(20)|    19.6(02)|     0.7(01)|   7.5|  12.6|   8.6|   4.6|   8.5| 13.0|29/17.5(25)|306.0|\n",
      "| 84.0|2018-12|82.0|     2916A| 4.3|  10.0|   5.0|   1.0|   0.0|  6.0|  0.0|  2.0|  0.0|    13.8(15)| 31.4| 47.0|1027.4|    942.9(22)|936.6|    918.8(13)|    18.3(09)|    -1.5(31)|   7.7|  12.6|   7.4|   2.2|   7.2| 11.0|25/18.9(13)|242.0|\n",
      "| 97.0|2018-13|64.0|     2916A| 6.9| 135.0|  98.0|  22.0|   3.0| 50.0| 76.0| null| null|32.2(09/mar)|747.6| 55.0|1016.0|944.8(28/ene)|928.2|891.4(01/mar)|40.7(06/ago)|-5.2(24/feb)|   3.1|  19.6|  13.2|   6.8|  21.3| null|       null| null|\n",
      "+-----+-------+----+----------+----+------+------+------+------+-----+-----+-----+-----+------------+-----+-----+------+-------------+-----+-------------+------------+------------+------+------+------+------+------+-----+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd6.sc:2</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+----------+----+------+------+------+------+-----+-----+-----+-----+------------+-----+-----+------+-------------+-----+-------------+------------+------------+------+------+------+------+------+-----+-----------+-----+\n",
      "|    e|  fecha|  hr|indicativo|inso|np_001|np_010|np_100|np_300|nt_00|nt_30|nw_55|nw_91|       p_max|p_mes|p_sol| q_mar|        q_max|q_med|        q_min|      ta_max|      ta_min|ti_max|tm_max|tm_mes|tm_min|ts_min|w_med|    w_racha|w_rec|\n",
      "+-----+-------+----+----------+----+------+------+------+------+-----+-----+-----+-----+------------+-----+-----+------+-------------+-----+-------------+------------+------------+------+------+------+------+------+-----+-----------+-----+\n",
      "| 77.0| 2018-1|82.0|     2916A| 4.0|  14.0|   8.0|   2.0|   0.0| 13.0|  0.0|  2.0|  0.0|    12.4(13)| 53.0| 41.0|1024.9|    944.8(28)|933.8|    911.9(07)|    16.2(03)|    -2.8(15)|   4.2|  10.3|   5.7|   1.0|  10.1| 12.0|23/18.3(05)|281.0|\n",
      "| 59.0| 2018-2|68.0|     2916A| 5.7|   8.0|   6.0|   2.0|   1.0| 21.0|  0.0|  4.0|  0.0|    30.2(27)| 52.0| 53.0|1016.7|    936.4(17)|926.0|    899.7(28)|    17.5(26)|    -5.2(24)|   3.1|  10.3|   4.6|  -1.1|   7.8| 13.0|26/19.2(13)|293.0|\n",
      "| 77.0| 2018-3|77.0|     2916A| 4.1|  22.0|  20.0|   4.0|   1.0|  6.0|  0.0| 16.0|  0.0|    32.2(09)|172.6| 34.0|1008.7|    938.4(27)|919.4|    891.4(01)|    19.5(27)|    -3.0(22)|   7.8|  11.4|   6.7|   1.9|   8.9| 20.0|26/23.9(10)|452.0|\n",
      "| 95.0| 2018-4|71.0|     2916A| 6.0|  17.0|  16.0|   4.0|   0.0|  1.0|  0.0| null| null|    15.0(12)|102.4| 45.0|1011.9|    935.2(17)|923.8|    907.1(10)|    26.9(25)|    -0.3(11)|   8.6|  16.9|  11.2|   5.4|  11.6| null|       null| null|\n",
      "|107.0| 2018-5|61.0|     2916A| 7.8|  12.0|   8.0|   3.0|   0.0|  1.0|  0.0| null| null|    26.0(24)| 76.8| 54.0|1014.4|    934.9(13)|927.3|    922.4(21)|    26.9(08)|     0.0(13)|  15.9|  21.0|  14.2|   7.4|  12.2| null|       null| null|\n",
      "|139.0| 2018-6|62.0|     2916A| 8.4|  11.0|   9.0|   0.0|   0.0|  0.0|  8.0| null| null|     9.6(08)| 49.0| 56.0|1013.9|    935.9(18)|928.2|    919.8(30)|    34.1(24)|     7.1(02)|  15.3|  25.2|  18.6|  11.9|  17.3| 11.0|       null| null|\n",
      "|129.0| 2018-7|50.0|     2916A|11.0|   5.0|   2.0|   1.0|   0.0|  0.0| 16.0| null| null|    26.0(15)| 29.0| 74.0|1013.7|    934.6(05)|928.9|    922.6(01)|    33.8(10)|    10.1(05)|  20.9|  29.5|  21.4|  13.2|  16.3| null|       null| null|\n",
      "|104.0| 2018-8|36.0|     2916A|11.9|   0.0|   0.0|   0.0|   0.0|  0.0| 28.0| null| null|     0.0(--)|  0.0| 86.0|1014.2|    934.7(10)|930.0|    924.2(08)|    40.7(06)|     9.8(09)|  27.8|  33.9|  24.2|  14.4|  21.3|  9.0|       null| null|\n",
      "|109.0| 2018-9|43.0|     2916A| 9.9|   4.0|   2.0|   0.0|   0.0|  0.0| 22.0| null| null|     3.4(02)|  7.0| 80.0|1016.6|    940.0(27)|931.5|    923.4(03)|    36.9(02)|     8.5(25)|  25.7|  31.4|  22.5|  13.6|  18.0| null|       null| null|\n",
      "| 88.0|2018-10|59.0|     2916A| 6.7|  14.0|   6.0|   0.0|   0.0|  2.0|  2.0|  2.0|  0.0|     5.6(15)| 26.6| 60.0|1014.7|    938.7(24)|926.9|    908.9(30)|    30.3(05)|    -2.6(28)|   9.1|  20.6|  13.7|   6.9|  12.8| 13.0|18/16.7(13)|321.0|\n",
      "| 91.0|2018-11|82.0|     2916A| 2.7|  18.0|  16.0|   5.0|   1.0|  0.0|  0.0|  5.0|  0.0|    30.4(11)|147.8| 27.0|1014.9|    936.5(02)|925.6|    911.8(20)|    19.6(02)|     0.7(01)|   7.5|  12.6|   8.6|   4.6|   8.5| 13.0|29/17.5(25)|306.0|\n",
      "| 84.0|2018-12|82.0|     2916A| 4.3|  10.0|   5.0|   1.0|   0.0|  6.0|  0.0|  2.0|  0.0|    13.8(15)| 31.4| 47.0|1027.4|    942.9(22)|936.6|    918.8(13)|    18.3(09)|    -1.5(31)|   7.7|  12.6|   7.4|   2.2|   7.2| 11.0|25/18.9(13)|242.0|\n",
      "| 97.0|2018-13|64.0|     2916A| 6.9| 135.0|  98.0|  22.0|   3.0| 50.0| 76.0| null| null|32.2(09/mar)|747.6| 55.0|1016.0|944.8(28/ene)|928.2|891.4(01/mar)|40.7(06/ago)|-5.2(24/feb)|   3.1|  19.6|  13.2|   6.8|  21.3| null|       null| null|\n",
      "+-----+-------+----+----------+----+------+------+------+------+-----+-----+-----+-----+------------+-----+-----+------+-------------+-----+-------------+------------+------------+------+------+------+------+------+-----+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDs.show\n",
    "data.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Untyped transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Dataset` API includes a section on _untyped transformations_. These are transformations that are not defined over the Scala types but over the inner Spark SQL types (i.e. `StructType`s). More exactly, these could be named *dynamically typed transformations*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These transformations are in close corresponde with their SQL counterparts: `SELECT`, `WHERE`, `GROUP BY`, `FROM`, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `select` transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, the equivalent to the `map` typed transformation is `select`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">collect at cmd7.sc:2</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd7.sc:3</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|       value|\n",
      "+------------+\n",
      "|    16.2(03)|\n",
      "|    17.5(26)|\n",
      "|    19.5(27)|\n",
      "|    26.9(25)|\n",
      "|    26.9(08)|\n",
      "|    34.1(24)|\n",
      "|    33.8(10)|\n",
      "|    40.7(06)|\n",
      "|    36.9(02)|\n",
      "|    30.3(05)|\n",
      "|    19.6(02)|\n",
      "|    18.3(09)|\n",
      "|40.7(06/ago)|\n",
      "+------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String, true], true, false) AS value#367]\n",
      "+- *(1) MapElements ammonite.$sess.cmd7$Helper$$Lambda$4582/0x00000008017ca040@502aa312, obj#366: java.lang.String\n",
      "   +- *(1) DeserializeToObject newInstance(class ammonite.$sess.cmd5$Helper$Data), obj#365: ammonite.$sess.cmd5$Helper$Data\n",
      "      +- *(1) FileScan json [e#0,fecha#1,hr#2,indicativo#3,inso#4,np_001#5,np_010#6,np_100#7,np_300#8,nt_00#9,nt_30#10,nw_55#11,nw_91#12,p_max#13,p_mes#14,p_sol#15,q_mar#16,q_max#17,q_med#18,q_min#19,ta_max#20,ta_min#21,ti_max#22,tm_max#23,... 6 more fields] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/D:/TFGAlvaroSanchez/data/2916A(Vitigudino)-2018.json], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<e:string,fecha:string,hr:string,indicativo:string,inso:string,np_001:string,np_010:string,...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mds\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mString\u001b[39m] = [value: string]\r\n",
       "\u001b[36mres7_1\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m\"16.2(03)\"\u001b[39m,\n",
       "  \u001b[32m\"17.5(26)\"\u001b[39m,\n",
       "  \u001b[32m\"19.5(27)\"\u001b[39m,\n",
       "  \u001b[32m\"26.9(25)\"\u001b[39m,\n",
       "  \u001b[32m\"26.9(08)\"\u001b[39m,\n",
       "  \u001b[32m\"34.1(24)\"\u001b[39m,\n",
       "  \u001b[32m\"33.8(10)\"\u001b[39m,\n",
       "  \u001b[32m\"40.7(06)\"\u001b[39m,\n",
       "  \u001b[32m\"36.9(02)\"\u001b[39m,\n",
       "  \u001b[32m\"30.3(05)\"\u001b[39m,\n",
       "  \u001b[32m\"19.6(02)\"\u001b[39m,\n",
       "  \u001b[32m\"18.3(09)\"\u001b[39m,\n",
       "  \u001b[32m\"40.7(06/ago)\"\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ds: Dataset[String] = dataDs.map(_.ta_max)\n",
    "ds.collect\n",
    "ds.show\n",
    "ds.explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.sql.AnalysisException: Path does not exist: file:/D:/TFGAlvaroSanchez/json_data/2916A(Vitigudino)-2018.json;\u001b[39m\r\n  org.apache.spark.sql.execution.datasources.DataSource.$anonfun$checkAndGlobPathIfNecessary$1(\u001b[32mDataSource.scala\u001b[39m:\u001b[32m558\u001b[39m)\r\n  scala.collection.TraversableLike.$anonfun$flatMap$1(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m244\u001b[39m)\r\n  scala.collection.immutable.List.foreach(\u001b[32mList.scala\u001b[39m:\u001b[32m392\u001b[39m)\r\n  scala.collection.TraversableLike.flatMap(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m244\u001b[39m)\r\n  scala.collection.TraversableLike.flatMap$(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m241\u001b[39m)\r\n  scala.collection.immutable.List.flatMap(\u001b[32mList.scala\u001b[39m:\u001b[32m355\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(\u001b[32mDataSource.scala\u001b[39m:\u001b[32m545\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(\u001b[32mDataSource.scala\u001b[39m:\u001b[32m359\u001b[39m)\r\n  org.apache.spark.sql.DataFrameReader.loadV1Source(\u001b[32mDataFrameReader.scala\u001b[39m:\u001b[32m223\u001b[39m)\r\n  org.apache.spark.sql.DataFrameReader.load(\u001b[32mDataFrameReader.scala\u001b[39m:\u001b[32m211\u001b[39m)\r\n  org.apache.spark.sql.DataFrameReader.json(\u001b[32mDataFrameReader.scala\u001b[39m:\u001b[32m392\u001b[39m)\r\n  org.apache.spark.sql.DataFrameReader.json(\u001b[32mDataFrameReader.scala\u001b[39m:\u001b[32m326\u001b[39m)\r\n  ammonite.$sess.cmd8$Helper.<init>(\u001b[32mcmd8.sc\u001b[39m:\u001b[32m2\u001b[39m)\r\n  ammonite.$sess.cmd8$.<init>(\u001b[32mcmd8.sc\u001b[39m:\u001b[32m7\u001b[39m)\r\n  ammonite.$sess.cmd8$.<clinit>(\u001b[32mcmd8.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "val df: DataFrame = \n",
    "    spark.read.option(\"multiline\", \"true\").json(\"D:/TFGAlvaroSanchez/json_data/2916A(Vitigudino)-2018.json\").select($\"ta_max\")\n",
    "df.collect\n",
    "df.show\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Tenga en cuenta que perdimos la etiqueta de la columna (ta_max) en el caso de la transformación del conjunto de datos (DataSet). Esto no está sucediendo con select (DataFrame). Además, tenemos más control sobre el esquema resultante:\n",
    "\n",
    "\n",
    "###### `substring()` -> `substring(inicio, fin)`\n",
    "###### `substr()` -> `substr(inicio, longitud)`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDs.schema\n",
    "dataDS.map(t => (t(1).toString, t(20).toString.substring(0,4), t(1).toString.substring(5,6)))\n",
    "    .show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select($\"fecha\", $\"ta_max\".substr(0,4) as \"temperatura max\", $\"fecha\".substr(6,1) as \"mes\")\n",
    "    .show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [org.apache.spark.sql.functions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$) contains dozens of column operators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that _untyped_, or more properly, _dynamically typed_, character means that the Scala compiler won't complain if we choose a non-existent column:\n",
    "\n",
    "###### El compilador de Scala no se quejará si elegimos una columna inexistente, señala el error en tiempo de ejecucion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy val df: DataFrame = spark.read.option(\"multiline\", \"true\").json(\"D:/TFGAlvaroSanchez/json_data/2916A(Vitigudino)-2018.json\").select($\"nam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error will be shown at runtime: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the contrary, the error in the dataset transformation manifests at compile-time:\n",
    "\n",
    "###### En el DataSet nos muestra el error en tiempo de ejecucion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDs.map(_.nam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `filter` transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the equivalent to the typed `filter` transformation:\n",
    "\n",
    "###### En este caso primero hemos tenido que transformar la columna ta_max en tipo integer. Lo realizamos mediante `.withColumn(\"nuevoNombreColumna\", \"nombreColumna\".cast(tipo))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val data1 = data.withColumn(\"ta_max\", $\"ta_max\".substr(0,4).cast(IntegerType))\n",
    "data1.filter($\"ta_max\" > 30)\n",
    "    .show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we pass a column function not denoting a boolean value, we won't even get a run-time exception:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df: DataFrame = \n",
    "    data.filter($\"fecha\" > 2001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `groupBy` transformation\n",
    "\n",
    "###### Agrupamos los datos mediante la columna que le indiquemos. Si tiene el mismo valor en esta columna se agrupan, mediante `count` realizamos un recuento de cuantos se han agrupado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val students: DataFrame = spark.read.json(\"D:/GitHub/spark-intro/data/students.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students.groupBy($\"degree\").count.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Join` transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already discussed joins, but we didn't mention that the resulting type of a join is a dataframe, not a dataset: \n",
    "\n",
    "###### El tipo resultante de una unión es un DataFrame, no un DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "\n",
    "case class Student(name: String, degree: String)\n",
    "case class Person(name: String, age: Long)\n",
    "\n",
    "val people : DataFrame = spark.read.json(\"D:/GitHub/spark-intro/data/people.json\") \n",
    "val peopleDs: Dataset[Person] = people.as[Person]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDs.join(students.as[Student], \"name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Podemos observar que cambia el tipo de dato de la variable peopleDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The problems of `Dataset`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets are nice because they are type safe, but, unfortunately, they are less efficient than data frames in several respects. This can be best shown by reading from parquet source files. \n",
    "\n",
    "###### Los DataSets son buenos porque son de tipo seguro, pero, desafortunadamente, son menos eficientes que los DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet is a _columnar_ format, which means that it stores physically data around columns, allowing us to read only data from a particular column without reading the entire row.\n",
    "\n",
    "###### Parquet es un formato de columnas, lo que significa que almacena datos físicos alrededor de las columnas, lo que nos permite leer solo los datos de una columna en particular sin leer toda la fila."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.write.mode(\"overwrite\").parquet(\"D:/GitHub/spark-intro/data/people.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(\"D:/GitHub/spark-intro/data/people.parquet\").schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `ReadSchema` optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a program that simply read the _name_ column of the people dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ds: Dataset[String] = \n",
    "    spark.read.parquet(\"D:/GitHub/spark-intro/data/people.parquet\").as[Person]\n",
    "        .map(_.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which works as intended: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a problem, however: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the plan includes the directive `ReadSchema: struct<age:bigint,name:string>`, which generates a query to scan the full schema of the parquet file. But we just want to read the names! We can create an optimun program using dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df: DataFrame = \n",
    "    spark.read.parquet(\"D:/GitHub/spark-intro/data/people.parquet\").select($\"name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which works similarly: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but more efficiently (note the the value of the `ReadSchema` directive):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can empirically check that it actually works using the Spark UI. First, we create a parquet file with enough rows and several columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{lit, rand, round}\n",
    "spark.range(0, 1000000)\n",
    "    .select($\"id\" as \"_1\", lit(1) as \"_2\")\n",
    "    .write.mode(\"overwrite\").parquet(\"D:/GitHub/spark-intro/data/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we read the second column using both datasets and dataframes, and check the Spark UI for the _Input Size_ field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val test = spark.read.parquet(\"D:/GitHub/spark-intro/data/test\")\n",
    "test.as[Tuple2[Long, Int]].map(_._2).collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using dataframes the input size is much lower since we only read the second column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.select($\"_2\").collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `PushedFilter` optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the following equivalent dataset and dataframe programs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ds: Dataset[(Long, Int)] = \n",
    "    test.as[(Long, Int)]\n",
    "        .filter(_._1 >= 999995)\n",
    "\n",
    "val df: DataFrame = \n",
    "    test\n",
    "        .filter($\"_1\" >= 999995)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functionally, they are equivalent, but their performance differ significantly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.collect\n",
    "ds.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explanation of this difference lies in another optimization applied by the Spark SQL compiler: the so-called push-down filter optimization. In the previous `ReadSchema` optimization, we skipped certain columns of the dataset; now, we skip rows and read only the ones we are interested in (those that satisfy the predicate). We can check if the push-down filter optimization is actually applied by inspecting the query plan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.explain\n",
    "ds.explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `PartitionFilters` optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a test file with an additional column: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(0, 1000000)\n",
    "    .select($\"id\" as \"_1\", lit(1) as \"_2\", round(rand() * 10) mod lit(10) as \"_3\")\n",
    "    .write.mode(\"overwrite\").parquet(\"D:/GitHub/spark-intro/data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val test: DataFrame = spark.read.parquet(\"D:/GitHub/spark-intro/data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's suppose that we want to read data with value `_3` equal to `9.0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.filter($\"_3\" === lit(9.0)).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pushed filter optimization is created, but it would be better if we could just read directly those rows with the exact value for the thrid column. We can achieve that as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.write.mode(\"overwrite\").partitionBy(\"_3\").parquet(\"D:/GitHub/spark-intro/data/test/testP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the parquet file is splitted into ten partitions. Now, if we just want to process data with a particular key, Spark will generate an optimun query: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val testP: DataFrame = spark.read.parquet(\"D:/GitHub/spark-intro/data/test/testP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testP.filter($\"_3\" === lit(9.0)).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspet the Spark UI to check that we read less data in the last action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

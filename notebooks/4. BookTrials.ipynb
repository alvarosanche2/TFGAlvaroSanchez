{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3b5f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.5` \n",
    "import $ivy.`sh.almond::almond-spark:0.4.0`\n",
    "\n",
    "import org.apache.spark.sql.{NotebookSparkSession, SparkSession}\n",
    "import org.apache.spark.sql.{functions => func, _}\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val spark = NotebookSparkSession\n",
    "      .builder()\n",
    "      .config(\"spark.sql.join.preferSortMergeJoin\", false)\n",
    "      .config(\"spark.sql.shuffle.partitions\", 64)\n",
    "      .master(\"local[*]\")\n",
    "      .getOrCreate()\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "import org.slf4j.LoggerFactory\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "\n",
    "Logger.getRootLogger().setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf06962",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"json\").load(\"D:/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\").schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0920ad8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\n",
    "import org.apache.spark.sql.types.Metadata\n",
    "\n",
    "val myManualSchema = StructType(Array(\n",
    "  StructField(\"DEST_COUNTRY_NAME\", StringType, true),\n",
    "  StructField(\"ORIGIN_COUNTRY_NAME\", StringType, true),\n",
    "  StructField(\"count\", LongType, false,\n",
    "    Metadata.fromJson(\"{\\\"hello\\\":\\\"world\\\"}\"))\n",
    "))\n",
    "\n",
    "val df = spark.read.format(\"json\").schema(myManualSchema)\n",
    "  .load(\"D:/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3d5ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.orderBy($\"count\".asc).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e18606",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.col(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9589ec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.expr\n",
    "expr(\"(((someCol + 5) * 200) - 6) < otherCol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32af3a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41084da",
   "metadata": {},
   "outputs": [],
   "source": [
    "//import org.apache.spark.sql.Row\n",
    "val myRow = Row(\"Hello\", null, 1, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053b3075",
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.read.format(\"json\")\n",
    "  .load(\"D:/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b2e492",
   "metadata": {},
   "outputs": [],
   "source": [
    "val myManualSchema = new StructType(Array(\n",
    "  new StructField(\"some\", StringType, true),\n",
    "  new StructField(\"col\", StringType, true),\n",
    "  new StructField(\"names\", LongType, false)))\n",
    "val myRows = Seq(Row(\"Hello\", null, 1L))\n",
    "val myRDD = spark.sparkContext.parallelize(myRows)\n",
    "val myDf = spark.createDataFrame(myRDD, myManualSchema)\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c23d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"DEST_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d452d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(expr(\"DEST_COUNTRY_NAME as destination\").alias(\"DEST_COUNTRY_NAME\"))\n",
    "  .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337073f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.selectExpr(\n",
    "    \"*\", // include all original columns\n",
    "    \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\")\n",
    "  .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053b8e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.lit\n",
    "df.select(expr(\"*\"), lit(1).as(\"One\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110ff453",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"numberOne\", lit(1)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01135c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\"))\n",
    "  .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7686d754",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"count2\", $\"count\".cast(\"long\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329b3384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{col, column}\n",
    "//df.filter(col(\"count\") < 2).show(2)\n",
    "df.filter($\"count\" < 2).show(2)\n",
    "//df.where(\"count < 2\").show(2)\n",
    "df.where($\"count\" < 2).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813ce38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb736ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "val seed = 5\n",
    "val withReplacement = false\n",
    "val fraction = 0.5\n",
    "df.sample(withReplacement, fraction, seed).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f598f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val schema = df.schema\n",
    "val newRows = Seq(\n",
    "  Row(\"New Country\", \"Other Country\", 5L),\n",
    "  Row(\"New Country 2\", \"Other Country 3\", 1L)\n",
    ")\n",
    "val parallelizedRows = spark.sparkContext.parallelize(newRows)\n",
    "val newDF = spark.createDataFrame(parallelizedRows, schema)\n",
    "newDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c8b8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.union(newDF)\n",
    "  .where(\"count = 1\")\n",
    "  .where($\"ORIGIN_COUNTRY_NAME\" =!= \"United States\")\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761f1c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6e0ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9264a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val collectDF = df.limit(10)\n",
    "collectDF.take(5) // take works with an Integer count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6c0205",
   "metadata": {},
   "outputs": [],
   "source": [
    "collectDF.show() // this prints it out nicely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed23f0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "collectDF.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8725f89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "collectDF.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03360fda",
   "metadata": {},
   "source": [
    "## Capitulo 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa15f192",
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load(\"D:/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2010-12-01.csv\")\n",
    "df.printSchema()\n",
    "df.createOrReplaceTempView(\"dfTable\")    //crearÃ¡ una vista temporal de la tabla en la memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8e8945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.lit\n",
    "df.select(lit(5), lit(\"five\"), lit(5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011e25c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(col(\"InvoiceNo\").equalTo(536365))\n",
    "  .select(\"InvoiceNo\", \"Description\")\n",
    "  .show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fd5d3d",
   "metadata": {},
   "source": [
    "Lo mismo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f637cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.where(\"InvoiceNo = 536365\")\n",
    "  .show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a7f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val priceFilter = col(\"UnitPrice\") > 600\n",
    "val descripFilter = col(\"Description\").contains(\"POSTAGE\")\n",
    "df.where(col(\"StockCode\").isin(\"DOT\")).where(priceFilter.or(descripFilter))\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d705715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val DOTCodeFilter = col(\"StockCode\") === \"DOT\"\n",
    "val priceFilter = col(\"UnitPrice\") > 600\n",
    "val descripFilter = col(\"Description\").contains(\"POSTAGE\")\n",
    "df.withColumn(\"isExpensive\", DOTCodeFilter.and(priceFilter.or(descripFilter)))\n",
    "  .where(\"isExpensive\")\n",
    "  //.filter(\"isExpensive\")\n",
    "  .select(\"unitPrice\", \"isExpensive\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2010ade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{expr, not, col}\n",
    "df.withColumn(\"isExpensive\", not(col(\"UnitPrice\").leq(250)))\n",
    "  .filter(\"isExpensive\")\n",
    "  .select(\"Description\", \"UnitPrice\").show(5)\n",
    "df.withColumn(\"isExpensive\", expr(\"NOT UnitPrice <= 250\"))\n",
    "  .filter(\"isExpensive\")\n",
    "  .select(\"Description\", \"UnitPrice\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e71b0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(col(\"Description\").eqNullSafe(\"CustumerID\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371dc60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val fabricatedQuantity = func.pow(col(\"Quantity\") * col(\"UnitPrice\"), 2) + 5\n",
    "df.select(expr(\"CustomerId\"), fabricatedQuantity.alias(\"realQuantity\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952ce00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "//Como SQL\n",
    "df.selectExpr(\n",
    "  \"CustomerId\",\n",
    "  \"(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f801748",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(func.round(col(\"UnitPrice\"), 1).alias(\"rounded\"), col(\"UnitPrice\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0c61c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(func.round(lit(\"2.5\")), func.bround(lit(\"2.5\"))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fd50eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.stat.corr(\"Quantity\", \"UnitPrice\")\n",
    "df.select(func.corr(\"Quantity\", \"UnitPrice\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc8849c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afdd37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(func.monotonically_increasing_id()).show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bff2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{initcap}\n",
    "df.select(initcap(col(\"Description\"))).show(2, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcff2f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{lit, ltrim, rtrim, rpad, lpad, trim}\n",
    "df.select(\n",
    "    ltrim(lit(\"    HELLO    \")).as(\"ltrim\"),          //Elimina los espacios en blanco de la izquierda\n",
    "    rtrim(lit(\"    HELLO    \")).as(\"rtrim\"),          //Elimina los espacios en blanco de la derecha\n",
    "    trim(lit(\"    HELLO    \")).as(\"trim\"),            //Elimina los espacios en blanco tanto de derecha como izquierda\n",
    "    lpad(lit(\"HELLO\"), 7, \" \").as(\"lp\"),              //Se queda con el numero de posiciones indicadas aÃ±adiendo espacios por izq\n",
    "    rpad(lit(\"HELLO\"), 10, \" \").as(\"rp\")).show(2)     //Se queda con el numero de posiciones indicadas aÃ±adiendo espacios por der"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe65f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.regexp_replace\n",
    "val simpleColors = Seq(\"black\", \"white\", \"red\", \"green\", \"blue\")\n",
    "val regexString = simpleColors.map(_.toUpperCase).mkString(\"|\")\n",
    "// the | signifies `OR` in regular expression syntax\n",
    "df.select(\n",
    "  regexp_replace(col(\"Description\"), regexString, \"COLOR\").alias(\"color_clean\"),\n",
    "  col(\"Description\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3e3733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.translate\n",
    "df.select(translate(col(\"Description\"), \"LEET\", \"1337\"), col(\"Description\"))\n",
    "  .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bbb134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.regexp_extract\n",
    "val regexString = simpleColors.map(_.toUpperCase).mkString(\"(\", \"|\", \")\")\n",
    "// the | signifies OR in regular expression syntax\n",
    "df.select(\n",
    "     regexp_extract(col(\"Description\"), regexString, 1).alias(\"color_clean\"),\n",
    "     col(\"Description\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3abf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "val simpleColors = Seq(\"black\", \"white\", \"red\", \"green\", \"blue\")\n",
    "val selectedColumns = simpleColors.map(color => {\n",
    "   col(\"Description\").contains(color.toUpperCase).alias(s\"is_$color\")\n",
    "}):+expr(\"*\") // could also append this value\n",
    "df.select(selectedColumns:_*).where(col(\"is_white\").or(col(\"is_red\")))\n",
    "  .select(\"Description\").show(3, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f22bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{current_date, current_timestamp}\n",
    "val dateDF = spark.range(10)\n",
    "  .withColumn(\"today\", current_date())\n",
    "  .withColumn(\"now\", current_timestamp())\n",
    "dateDF.createOrReplaceTempView(\"dateTable\")\n",
    "dateDF.show()\n",
    "dateDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2b83ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{date_add, date_sub}\n",
    "dateDF.select(date_sub(col(\"today\"), 5), date_add(col(\"today\"), 5)).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58d096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{datediff, months_between, to_date}\n",
    "dateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\n",
    "  .select(datediff(col(\"week_ago\"), col(\"today\"))).show(1)\n",
    "dateDF.select(\n",
    "    to_date(lit(\"2016-01-01\")).alias(\"start\"),\n",
    "    to_date(lit(\"2017-05-22\")).alias(\"end\"))\n",
    "  .select(months_between(col(\"start\"), col(\"end\"))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69bb441",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(5).withColumn(\"date\", lit(\"2017-01-01\"))\n",
    "  .select(to_date(col(\"date\"))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc9451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.to_date\n",
    "val dateFormat = \"yyyy-dd-MM\"\n",
    "val cleanDateDF = spark.range(1).select(\n",
    "    to_date(lit(\"2017-12-11\"), dateFormat).alias(\"date\"),\n",
    "    to_date(lit(\"2017-20-12\"), dateFormat).alias(\"date2\"))\n",
    "cleanDateDF.createOrReplaceTempView(\"dateTable2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68364444",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanDateDF.filter(col(\"date2\") > \"'2017-12-12'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af796f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.coalesce\n",
    "df.select(coalesce(col(\"Description\"), col(\"CustomerId\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c66a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.fill(\"All Null values become this string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ac8fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a7b1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.fill(5, Seq(\"StockCode\", \"InvoiceNo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f426be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val fillColValues = Map(\"StockCode\" -> 5, \"Description\" -> \"No Value\")\n",
    "df.na.fill(fillColValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9144b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.selectExpr(\"(Description, InvoiceNo) as complex\", \"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5def6fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.struct\n",
    "val complexDF = df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))\n",
    "complexDF.createOrReplaceTempView(\"complexDF\")\n",
    "complexDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70642137",
   "metadata": {},
   "outputs": [],
   "source": [
    "complexDF.select(\"complex.Description\").show()\n",
    "complexDF.select(col(\"complex\").getField(\"InvoiceNo\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27800dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.split\n",
    "df.select(split(col(\"Description\"), \" \").alias(\"array_col\"))\n",
    "  .selectExpr(\"array_col[0]\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5013652b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.array_contains\n",
    "df.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d30151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.map\n",
    "df.select(map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2f05de",
   "metadata": {},
   "outputs": [],
   "source": [
    "val jsonDF = spark.range(1).selectExpr(\"\"\"\n",
    "  '{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}' as jsonString\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29d62b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca44439a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{get_json_object, json_tuple}\n",
    "jsonDF.select(\n",
    "    get_json_object(col(\"jsonString\"), \"$.myJSONKey.myJSONValue[1]\") as \"column\",\n",
    "    json_tuple(col(\"jsonString\"), \"myJSONKey\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7944f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.to_json\n",
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\")\n",
    "  .select(to_json(col(\"myStruct\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88f960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val udfExampleDF = spark.range(5).toDF(\"num\")\n",
    "def power3(number:Double):Double = number * number * number\n",
    "power3(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957a6d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.udf\n",
    "val power3udf = udf(power3(_:Double):Double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab57fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "udfExampleDF.select(power3udf(col(\"num\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e09a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.udf.register(\"power3\", power3(_:Double):Double)\n",
    "udfExampleDF.selectExpr(\"power3(num)\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f766d53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val df = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load(\"D:/Spark-The-Definitive-Guide-master/data/retail-data/all/*.csv\")\n",
    "  .coalesce(5)\n",
    "df.cache()\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a75508",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bec902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1046bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.countDistinct\n",
    "df.select(countDistinct(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919d0ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{sum, count, avg, expr}\n",
    "\n",
    "df.select(\n",
    "    count(\"Quantity\").alias(\"total_transactions\"),\n",
    "    sum(\"Quantity\").alias(\"total_purchases\"),\n",
    "    avg(\"Quantity\").alias(\"avg_purchases\"),\n",
    "    expr(\"mean(Quantity)\").alias(\"mean_purchases\"))\n",
    "  .selectExpr(\n",
    "    \"total_purchases/total_transactions\",\n",
    "    \"avg_purchases\",\n",
    "    \"mean_purchases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47408188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{var_pop, stddev_pop}\n",
    "import org.apache.spark.sql.functions.{var_samp, stddev_samp}\n",
    "df.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"),\n",
    "  stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f383e0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{skewness, kurtosis}\n",
    "df.select(skewness(\"Quantity\"), kurtosis(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab43c8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{collect_set, collect_list}\n",
    "df.agg(collect_set(\"Country\"), collect_list(\"Country\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec426689",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fba8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.count\n",
    "\n",
    "df.groupBy(\"InvoiceNo\").agg(\n",
    "  count(\"Quantity\").alias(\"quan\"),\n",
    "  expr(\"count(Quantity)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b05eb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "// in Scala\n",
    "df.groupBy(\"InvoiceNo\").agg(\"Quantity\"->\"avg\", \"Quantity\"->\"stddev_pop\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dbaea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{col, to_date}\n",
    "val dfWithDate = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"),\n",
    "  \"MM/d/yyyy H:mm\"))\n",
    "dfWithDate.createOrReplaceTempView(\"dfWithDate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f508b16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWithDate.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50790d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.functions.col\n",
    "val windowSpec = Window\n",
    "  .partitionBy(\"CustomerId\", \"date\")\n",
    "  .orderBy(col(\"Quantity\").desc)\n",
    "  .rowsBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1d5ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.max\n",
    "val maxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86192d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{dense_rank, rank}\n",
    "val purchaseDenseRank = dense_rank().over(windowSpec)\n",
    "val purchaseRank = rank().over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52145ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "dfWithDate.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\")\n",
    "  .select(\n",
    "    col(\"CustomerId\"),\n",
    "    col(\"date\"),\n",
    "    col(\"Quantity\"),\n",
    "    purchaseRank.alias(\"quantityRank\"),\n",
    "    purchaseDenseRank.alias(\"quantityDenseRank\"),\n",
    "    maxPurchaseQuantity.alias(\"maxPurchaseQuantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc917665",
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfNoNull = dfWithDate.drop()\n",
    "dfNoNull.createOrReplaceTempView(\"dfNoNull\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0023cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "val rolledUpDF = dfNoNull.rollup(\"Date\", \"Country\").agg(sum(\"Quantity\"))\n",
    "  .selectExpr(\"Date\", \"Country\", \"`sum(Quantity)` as total_quantity\")\n",
    "  .orderBy(\"Date\")\n",
    "rolledUpDF.show()\n",
    "\n",
    "//donde vea los valores null es donde encontrarÃ¡ los totales generales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94902063",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolledUpDF.where(\"Country IS NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618a46fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNoNull.cube(\"Date\", \"Country\").agg(sum($\"Quantity\"))\n",
    "  .select(\"Date\", \"Country\", \"sum(Quantity)\").orderBy(\"Date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbb393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{grouping_id, sum, expr}\n",
    "\n",
    "dfNoNull.cube(\"customerId\", \"stockCode\").agg(grouping_id(), sum(\"Quantity\"))\n",
    ".orderBy(expr(\"grouping_id()\").desc)\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acd4f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "val pivoted = dfWithDate.groupBy(\"date\").pivot(\"Country\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f395de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.expressions.MutableAggregationBuffer\n",
    "import org.apache.spark.sql.expressions.UserDefinedAggregateFunction\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types._\n",
    "class BoolAnd extends UserDefinedAggregateFunction {\n",
    "  def inputSchema: org.apache.spark.sql.types.StructType =\n",
    "    StructType(StructField(\"value\", BooleanType) :: Nil)\n",
    "  def bufferSchema: StructType = StructType(\n",
    "    StructField(\"result\", BooleanType) :: Nil\n",
    "  )\n",
    "  def dataType: DataType = BooleanType\n",
    "  def deterministic: Boolean = true\n",
    "  def initialize(buffer: MutableAggregationBuffer): Unit = {\n",
    "    buffer(0) = true\n",
    "  }\n",
    "  def update(buffer: MutableAggregationBuffer, input: Row): Unit = {\n",
    "    buffer(0) = buffer.getAs[Boolean](0) && input.getAs[Boolean](0)\n",
    "  }\n",
    "  def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {\n",
    "    buffer1(0) = buffer1.getAs[Boolean](0) && buffer2.getAs[Boolean](0)\n",
    "  }\n",
    "  def evaluate(buffer: Row): Any = {\n",
    "    buffer(0)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28f457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val ba = new BoolAnd\n",
    "spark.udf.register(\"booland\", ba)\n",
    "import org.apache.spark.sql.functions._\n",
    "spark.range(1)\n",
    "  .selectExpr(\"explode(array(TRUE, TRUE, TRUE)) as t\")\n",
    "  .selectExpr(\"explode(array(TRUE, FALSE, TRUE)) as f\", \"t\")\n",
    "  .select(ba(col(\"t\")), expr(\"booland(f)\"))\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d991e7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "val person = Seq(\n",
    "    (0, \"Bill Chambers\", 0, Seq(100)),\n",
    "    (1, \"Matei Zaharia\", 1, Seq(500, 250, 100)),\n",
    "    (2, \"Michael Armbrust\", 1, Seq(250, 100)))\n",
    "  .toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")\n",
    "val graduateProgram = Seq(\n",
    "    (0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n",
    "    (2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n",
    "    (1, \"Ph.D.\", \"EECS\", \"UC Berkeley\"))\n",
    "  .toDF(\"id\", \"degree\", \"department\", \"school\")\n",
    "val sparkStatus = Seq(\n",
    "    (500, \"Vice President\"),\n",
    "    (250, \"PMC Member\"),\n",
    "    (100, \"Contributor\"))\n",
    "  .toDF(\"id\", \"status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e576b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "person.createOrReplaceTempView(\"person\")\n",
    "graduateProgram.createOrReplaceTempView(\"graduateProgram\")\n",
    "sparkStatus.createOrReplaceTempView(\"sparkStatus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ccff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "val joinExpression = person.col(\"graduate_program\") === graduateProgram.col(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850e2215",
   "metadata": {},
   "outputs": [],
   "source": [
    "val wrongJoinExpression = person.col(\"name\") === graduateProgram.col(\"school\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f870edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "person.join(graduateProgram, joinExpression).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c836eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "val joinType = \"outer\"\n",
    "person.join(graduateProgram, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a2ed27",
   "metadata": {},
   "outputs": [],
   "source": [
    "val joinType = \"left_outer\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06295d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val joinType = \"right_outer\"\n",
    "person.join(graduateProgram, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c7d65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val joinType = \"left_semi\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1095d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val joinType = \"cross\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d56b98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "person.crossJoin(graduateProgram).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e5caf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.expr\n",
    "\n",
    "person.withColumnRenamed(\"id\", \"personId\")\n",
    "  .join(sparkStatus, expr(\"array_contains(spark_status, id)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98838f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "val gradProgramDupe = graduateProgram.withColumnRenamed(\"id\", \"graduate_program\")\n",
    "val joinExpr = gradProgramDupe.col(\"graduate_program\") === person.col(\n",
    "  \"graduate_program\")\n",
    "gradProgramDupe.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9d1359",
   "metadata": {},
   "outputs": [],
   "source": [
    "person.join(gradProgramDupe, joinExpr).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430c69b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "person.join(gradProgramDupe,\"graduate_program\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43cc24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "person.join(gradProgramDupe, joinExpr).drop(person.col(\"graduate_program\"))\n",
    "  .select(\"graduate_program\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cdc518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.broadcast\n",
    "val joinExpr = person.col(\"graduate_program\") === graduateProgram.col(\"id\")\n",
    "person.join(broadcast(graduateProgram), joinExpr).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f90a08e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\n",
    "val myManualSchema = new StructType(Array(\n",
    "  new StructField(\"DEST_COUNTRY_NAME\", StringType, true),\n",
    "  new StructField(\"ORIGIN_COUNTRY_NAME\", StringType, true),\n",
    "  new StructField(\"count\", LongType, false)\n",
    "))\n",
    "spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"mode\", \"FAILFAST\")\n",
    "  .schema(myManualSchema)\n",
    "  .load(\"D:/Spark-The-Definitive-Guide-master/data/flight-data/csv/2010-summary.csv\")\n",
    "  .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eec26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val csvFile = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\").option(\"mode\", \"FAILFAST\").schema(myManualSchema)\n",
    "  .load(\"D:/Spark-The-Definitive-Guide-master/data/flight-data/csv/2010-summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4ce449",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \"\\t\")\n",
    "  .save(\"D:/Spark-The-Definitive-Guide-master/tmp/my-tsv-file.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcad77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.write.format(\"json\").mode(\"overwrite\").save(\"D:/Spark-The-Definitive-Guide-master/tmp/my-json-file.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393f5cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"parquet\")\n",
    "  .load(\"D:/Spark-The-Definitive-Guide-master/data/flight-data/parquet/2010-summary.parquet\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65a998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.write.format(\"parquet\").mode(\"overwrite\")\n",
    "  .save(\"D:/Spark-The-Definitive-Guide-master/tmp/my-parquet-file.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c80c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.textFile(\"D:/Spark-The-Definitive-Guide-master/data/flight-data/csv/2010-summary.csv\")\n",
    "  .selectExpr(\"split(value, ',') as rows\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0628676",
   "metadata": {},
   "outputs": [],
   "source": [
    "//csvFile.select(\"DEST_COUNTRY_NAME\").write.text(\"D:/Spark-The-Definitive-Guide-master/tmp/simple-text-file.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521d9a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "//csvFile.repartition(5).write.format(\"csv\").save(\"D:/Spark-The-Definitive-Guide-master/tmp/multiple.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c5227b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csvFile.limit(10).write.mode(\"overwrite\").partitionBy(\"DEST_COUNTRY_NAME\")\n",
    "  .save(\"D:/Spark-The-Definitive-Guide-master/tmp/partitioned-files.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804a3ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val numberBuckets = 10\n",
    "val columnToBucketBy = \"count\"\n",
    "\n",
    "//csvFile.write.format(\"parquet\").mode(\"overwrite\")\n",
    "  //.bucketBy(numberBuckets, columnToBucketBy).saveAsTable(\"bucketedFiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfec363",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT 1 + 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e79a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.json(\"D:/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")\n",
    "  .createOrReplaceTempView(\"some_sql_view\") // DF => SQL\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count)\n",
    "FROM some_sql_view GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\")\n",
    "  .where(\"DEST_COUNTRY_NAME like 'S%'\").where(\"`sum(count)` > 10\")\n",
    "  .count() // SQL => DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bac0f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618a0e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(500).rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe14bdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(10).toDF.rdd.map(rowObject => rowObject.getLong(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89834993",
   "metadata": {},
   "outputs": [],
   "source": [
    "val myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\"\n",
    "  .split(\" \")\n",
    "val words = spark.sparkContext.parallelize(myCollection, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb245749",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.setName(\"myWords\")\n",
    "words.name // myWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5a624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.textFile(\"D:/Spark-The-Definitive-Guide-master/some/path/withTextFiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2c6236",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.wholeTextFiles(\"D:/Spark-The-Definitive-Guide-master/some/path/withTextFiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdaa684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def startsWithS(individual:String) = {\n",
    "  individual.startsWith(\"S\")\n",
    "}\n",
    "words.filter(word => startsWithS(word)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a827db7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val words2 = words.map(word => (word, word(0), word.startsWith(\"S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2feaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "words2.filter(record => record._3).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a002d2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.sortBy(word => word.length() * -1).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560ac81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val fiftyFiftySplit = words.randomSplit(Array[Double](0.5, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcb6c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.parallelize(1 to 20).reduce(_ + _) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde8261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordLengthReducer(leftWord:String, rightWord:String): String = {\n",
    "  if (leftWord.length > rightWord.length)\n",
    "    return leftWord\n",
    "  else\n",
    "    return rightWord\n",
    "}\n",
    "\n",
    "words.reduce(wordLengthReducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161e18a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val confidence = 0.95\n",
    "val timeoutMilliseconds = 400\n",
    "words.countApprox(timeoutMilliseconds, confidence)\n",
    "\n",
    "words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dfc1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.countApproxDistinct(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f0ba1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b807bc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2bda65",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.take(5)\n",
    "words.takeOrdered(5)\n",
    "words.top(5)\n",
    "val withReplacement = true\n",
    "val numberToTake = 6\n",
    "val randomSeed = 100L\n",
    "words.takeSample(withReplacement, numberToTake, randomSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684add61",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.cache()\n",
    "words.getStorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809e1f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexedFunc(partitionIndex:Int, withinPartIterator: Iterator[String]) = {\n",
    "  withinPartIterator.toList.map(\n",
    "    value => s\"Partition: $partitionIndex => $value\").iterator\n",
    "}\n",
    "words.mapPartitionsWithIndex(indexedFunc).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc80ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\"\n",
    "  .split(\" \")\n",
    "val words = spark.sparkContext.parallelize(myCollection, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ab2ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.map(word => (word.toLowerCase, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4771f170",
   "metadata": {},
   "outputs": [],
   "source": [
    "val keyword = words.keyBy(word => word.toLowerCase.toSeq(0).toString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e655acc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword.mapValues(word => word.toUpperCase).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2cb0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword.flatMapValues(word => word.toUpperCase).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59384d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword.keys.collect()\n",
    "keyword.values.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e55dafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword.lookup(\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af0ee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "val distinctChars = words.flatMap(word => word.toLowerCase.toSeq).distinct\n",
    "  .collect()\n",
    "import scala.util.Random\n",
    "val sampleMap = distinctChars.map(c => (c, new Random().nextDouble())).toMap\n",
    "words.map(word => (word.toLowerCase.toSeq(0), word))\n",
    "  .sampleByKey(true, sampleMap, 6L)\n",
    "  .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf72078",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.map(word => (word.toLowerCase.toSeq(0), word))\n",
    "  .sampleByKeyExact(true, sampleMap, 6L).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab439df",
   "metadata": {},
   "outputs": [],
   "source": [
    "val chars = words.flatMap(word => word.toLowerCase.toSeq)\n",
    "val KVcharacters = chars.map(letter => (letter, 1))\n",
    "def maxFunc(left:Int, right:Int) = math.max(left, right)\n",
    "def addFunc(left:Int, right:Int) = left + right\n",
    "//val nums = sc.parallelize(1 to 30, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b86b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "// in Scala\n",
    "val timeout = 1000L //milliseconds\n",
    "val confidence = 0.95\n",
    "KVcharacters.countByKey()\n",
    "KVcharacters.countByKeyApprox(timeout, confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79f061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "KVcharacters.groupByKey().map(row => (row._1, row._2.reduce(addFunc))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8856f0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "KVcharacters.reduceByKey(addFunc).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55604d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "KVcharacters.aggregateByKey(0)(addFunc, maxFunc).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a096ddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val valToCombiner = (value:Int) => List(value)\n",
    "val mergeValuesFunc = (vals:List[Int], valToAppend:Int) => valToAppend :: vals\n",
    "val mergeCombinerFunc = (vals1:List[Int], vals2:List[Int]) => vals1 ::: vals2\n",
    "// now we define these as function variables\n",
    "val outputPartitions = 6\n",
    "KVcharacters\n",
    "  .combineByKey(\n",
    "    valToCombiner,\n",
    "    mergeValuesFunc,\n",
    "    mergeCombinerFunc,\n",
    "    outputPartitions)\n",
    "  .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729d7470",
   "metadata": {},
   "outputs": [],
   "source": [
    "KVcharacters.foldByKey(0)(addFunc).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccf5be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.util.Random\n",
    "val distinctChars = words.flatMap(word => word.toLowerCase.toSeq).distinct\n",
    "val charRDD = distinctChars.map(c => (c, new Random().nextDouble()))\n",
    "val charRDD2 = distinctChars.map(c => (c, new Random().nextDouble()))\n",
    "val charRDD3 = distinctChars.map(c => (c, new Random().nextDouble()))\n",
    "charRDD.cogroup(charRDD2, charRDD3).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a11af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val keyedChars = distinctChars.map(c => (c, new Random().nextDouble()))\n",
    "val outputPartitions = 10\n",
    "KVcharacters.join(keyedChars).count()\n",
    "KVcharacters.join(keyedChars, outputPartitions).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb5dfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d51e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "val myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\"\n",
    "  .split(\" \")\n",
    "val words = spark.sparkContext.parallelize(myCollection, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a44353",
   "metadata": {},
   "outputs": [],
   "source": [
    "val supplementalData = Map(\"Spark\" -> 1000, \"Definitive\" -> 200,\n",
    "                           \"Big\" -> -300, \"Simple\" -> 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7c795c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val suppBroadcast = spark.sparkContext.broadcast(supplementalData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a7add2",
   "metadata": {},
   "outputs": [],
   "source": [
    "suppBroadcast.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a74e8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.map(word => (word, suppBroadcast.value.getOrElse(word, 0)))\n",
    "  .sortBy(wordPair => wordPair._2)\n",
    "  .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4851da",
   "metadata": {},
   "outputs": [],
   "source": [
    "case class Flight(DEST_COUNTRY_NAME: String,\n",
    "                  ORIGIN_COUNTRY_NAME: String, count: BigInt)\n",
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "val flights = spark.read\n",
    "  .parquet(\"D:/Spark-The-Definitive-Guide-master/data/flight-data/parquet/2010-summary.parquet\")\n",
    "  .as[Flight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7fc807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.util.LongAccumulator\n",
    "val accUnnamed = new LongAccumulator\n",
    "val acc = spark.sparkContext.register(accUnnamed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27126ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val accChina = new LongAccumulator\n",
    "val accChina2 = spark.sparkContext.longAccumulator(\"China\")\n",
    "spark.sparkContext.register(accChina, \"China\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc68d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accChinaFunc(flight_row: Flight) = {\n",
    "  val destination = flight_row.DEST_COUNTRY_NAME\n",
    "  val origin = flight_row.ORIGIN_COUNTRY_NAME\n",
    "  if (destination == \"China\") {\n",
    "    accChina.add(flight_row.count.toLong)\n",
    "  }\n",
    "  if (origin == \"China\") {\n",
    "    accChina.add(flight_row.count.toLong)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b81769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.foreach(flight_row => accChinaFunc(flight_row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9049fec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "accChina.value // 953"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009d9885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.collection.mutable.ArrayBuffer\n",
    "import org.apache.spark.util.AccumulatorV2\n",
    "\n",
    "val arr = ArrayBuffer[BigInt]()\n",
    "\n",
    "class EvenAccumulator extends AccumulatorV2[BigInt, BigInt] {\n",
    "  private var num:BigInt = 0\n",
    "  def reset(): Unit = {\n",
    "    this.num = 0\n",
    "  }\n",
    "  def add(intValue: BigInt): Unit = {\n",
    "    if (intValue % 2 == 0) {\n",
    "        this.num += intValue\n",
    "    }\n",
    "  }\n",
    "  def merge(other: AccumulatorV2[BigInt,BigInt]): Unit = {\n",
    "    this.num += other.value\n",
    "  }\n",
    "  def value():BigInt = {\n",
    "    this.num\n",
    "  }\n",
    "  def copy(): AccumulatorV2[BigInt,BigInt] = {\n",
    "    new EvenAccumulator\n",
    "  }\n",
    "  def isZero():Boolean = {\n",
    "    this.num == 0\n",
    "  }\n",
    "}\n",
    "val acc = new EvenAccumulator\n",
    "//val newAcc = sc.register(acc, \"evenAcc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ef8761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().appName(\"Databricks Spark Example\")\n",
    "  .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\")\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5489fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.SparkContext\n",
    "val sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d126488",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f810f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read\n",
    "   .option(\"header\", \"true\")\n",
    "   .csv(\"D:/Spark-The-Definitive-Guide-master/data/retail-data/all/online-retail-dataset.csv\")\n",
    "   .repartition(2)\n",
    "   .selectExpr(\"instr(Description, 'GLASS') >= 1 as is_glass\")\n",
    "   .groupBy(\"is_glass\")\n",
    "   .count()\n",
    "  .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8720e0c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695283dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

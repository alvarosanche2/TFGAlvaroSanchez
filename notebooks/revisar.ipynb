{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab25de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.5` \n",
    "import $ivy.`sh.almond::almond-spark:0.4.0`\n",
    "\n",
    "import org.apache.spark.sql.{NotebookSparkSession, SparkSession}\n",
    "import org.apache.spark.sql.{functions => func, _}\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val spark = NotebookSparkSession\n",
    "      .builder()\n",
    "      .config(\"spark.sql.join.preferSortMergeJoin\", false)\n",
    "      .config(\"spark.sql.shuffle.partitions\", 64)\n",
    "      .master(\"local[*]\")\n",
    "      .getOrCreate()\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "import org.slf4j.LoggerFactory\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "\n",
    "Logger.getRootLogger().setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5081269e",
   "metadata": {},
   "source": [
    "Esta lectura me esta funcionando a veces si y hay veces que no, he llegado a pensar que podria ser por la memoria de los ejecutores o que el mismo driver despues de realizar varias operaciones se quede sin memoria "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1cb268",
   "metadata": {},
   "outputs": [],
   "source": [
    "val allData = spark.read.json(\"D:/TFGAlvaroSanchez/dataJSONLine/*.json\").select(\n",
    "            $\"fecha\".cast(DateType), \n",
    "            $\"indicativo\", \n",
    "            $\"p_max\",\n",
    "            $\"glo\".cast(DoubleType), \n",
    "            $\"hr\".cast(DoubleType), \n",
    "            $\"nw_55\".cast(IntegerType), \n",
    "            $\"tm_min\".cast(DoubleType), \n",
    "            $\"ta_max\", \n",
    "            $\"ts_min\".cast(DoubleType), \n",
    "            $\"nt_30\".cast(IntegerType), \n",
    "            $\"n_des\".cast(IntegerType), \n",
    "            $\"w_racha\", \n",
    "            $\"np_100\".cast(IntegerType), \n",
    "            $\"nw_91\".cast(IntegerType), \n",
    "            $\"np_001\".cast(IntegerType), \n",
    "            $\"ta_min\", \n",
    "            $\"w_rec\".cast(IntegerType), \n",
    "            $\"e\".cast(DoubleType), \n",
    "            $\"np_300\".cast(IntegerType), \n",
    "            $\"p_mes\".cast(DoubleType), \n",
    "            $\"w_med\".cast(DoubleType), \n",
    "            $\"nt_00\".cast(IntegerType), \n",
    "            $\"ti_max\".cast(DoubleType), \n",
    "            $\"tm_mes\".cast(DoubleType), \n",
    "            $\"tm_max\".cast(DoubleType), \n",
    "            $\"np_010\".cast(IntegerType))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da10825f",
   "metadata": {},
   "source": [
    "Para tratar de leerlo de una manera mas optima, habia pensado lo siguiente; el problema es que todos los datos que lee, los pone con valor a null y hay en ocasiones que me pasa como en el ejemplo de arriba, que no llega a completarse la barra de progreso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a00fc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "val schema = StructType(\n",
    "    Array(\n",
    "        StructField(\"fecha\", DateType, true), \n",
    "        StructField(\"indicativo\", StringType, true),\n",
    "        StructField(\"p_max\", StringType, true),\n",
    "        StructField(\"glo\", DoubleType, true),\n",
    "        StructField(\"hr\", DoubleType, true), \n",
    "        StructField(\"nw_55\", IntegerType, true), \n",
    "        StructField(\"tm_min\", DoubleType, true), \n",
    "        StructField(\"ta_max\", StringType, true), \n",
    "        StructField(\"ts_min\", DoubleType, true),\n",
    "        StructField(\"nt_30\", IntegerType, true), \n",
    "        StructField(\"n_des\", IntegerType, true),\n",
    "        StructField(\"w_racha\", StringType, true),\n",
    "        StructField(\"np_100\", IntegerType, true),\n",
    "        StructField(\"nw_91\", IntegerType, true),\n",
    "        StructField(\"np_001\", IntegerType, true),\n",
    "        StructField(\"ta_min\", StringType, true),\n",
    "        StructField(\"w_rec\", IntegerType, true),\n",
    "        StructField(\"e\", DoubleType, true),\n",
    "        StructField(\"np_300\", IntegerType, true),\n",
    "        StructField(\"p_mes\", DoubleType, true), \n",
    "        StructField(\"w_med\", DoubleType, true),\n",
    "        StructField(\"nt_00\", IntegerType, true),\n",
    "        StructField(\"ti_max\", DoubleType, true),\n",
    "        StructField(\"tm_mes\", DoubleType, true),\n",
    "        StructField(\"tm_max\", DoubleType, true),\n",
    "        StructField(\"np_010\", IntegerType, true)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1321c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val allData2 = spark.read.schema(schema).json(\"D:/TFGAlvaroSanchez/dataJSONLine/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c74b713",
   "metadata": {},
   "outputs": [],
   "source": [
    "allData2.na.drop().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57394ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
